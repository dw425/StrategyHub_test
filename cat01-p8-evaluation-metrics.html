<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluation Metrics | AI/ML Foundations | Strategy Hub</title>
    <style>
        :root {
            --brand-orange: #FF9900;
            --cat-primary: #3B82F6;
            --cat-light: #60A5FA;
            --cat-dark: #2563EB;
            --cat-glow: rgba(59, 130, 246, 0.15);
            --page-primary: #8B5CF6;
            --page-light: #A78BFA;
            --page-glow: rgba(139, 92, 246, 0.15);
            --bg-dark: #0a0a0f;
            --bg-card: #12121a;
            --bg-card-alt: #1a1a2e;
            --bg-hover: #252538;
            --border-color: #2a2a3e;
            --text-primary: #ffffff;
            --text-secondary: #a0a0b0;
            --text-muted: #6a6a7a;
            --green: #10B981;
            --yellow: #F59E0B;
            --red: #EF4444;
            --blue: #3B82F6;
            --cyan: #06B6D4;
            --pink: #EC4899;
            --teal: #14B8A6;
            --violet: #8B5CF6;
            --sidebar-width: 260px;
            --header-height: 60px;
        }
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background: var(--bg-dark); color: var(--text-primary); line-height: 1.6; }
        
        header { background: var(--bg-card); border-bottom: 1px solid var(--border-color); padding: 16px 32px; position: fixed; top: 0; left: 0; right: 0; z-index: 1000; height: var(--header-height); display: flex; justify-content: space-between; align-items: center; }
        .logo { font-size: 20px; font-weight: 700; }
        .logo span { color: var(--brand-orange); }
        .breadcrumb { display: flex; align-items: center; gap: 8px; font-size: 13px; }
        .breadcrumb a { color: var(--text-secondary); text-decoration: none; }
        .breadcrumb a:hover { color: var(--brand-orange); }
        .breadcrumb .sep { color: var(--text-muted); }
        .breadcrumb .current { color: var(--text-primary); }
        
        .sidebar { position: fixed; top: var(--header-height); left: 0; width: var(--sidebar-width); height: calc(100vh - var(--header-height)); background: var(--bg-card); border-right: 1px solid var(--border-color); overflow-y: auto; z-index: 100; padding: 24px 0; }
        .sidebar-section { margin-bottom: 24px; }
        .sidebar-title { font-size: 11px; text-transform: uppercase; letter-spacing: 1.5px; color: var(--page-light); padding: 0 24px; margin-bottom: 8px; }
        .sidebar-nav { list-style: none; }
        .sidebar-nav a { display: flex; align-items: center; gap: 8px; padding: 10px 24px; color: var(--text-secondary); text-decoration: none; font-size: 12px; border-left: 3px solid transparent; transition: all 0.2s; }
        .sidebar-nav a:hover { background: var(--bg-hover); color: var(--text-primary); }
        .sidebar-nav a.active { background: var(--page-glow); color: var(--page-light); border-left-color: var(--page-light); }
        .nav-icon { font-size: 14px; width: 24px; text-align: center; }
        
        .main-wrapper { margin-left: var(--sidebar-width); margin-top: var(--header-height); }
        .main-content { max-width: 1100px; padding: 32px; }
        
        .hero-compact { display: grid; grid-template-columns: 1fr 1fr; gap: 32px; margin-bottom: 32px; padding: 28px; background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 16px; position: relative; }
        .hero-compact::before { content: ''; position: absolute; top: 0; left: 0; right: 0; height: 3px; background: linear-gradient(90deg, var(--page-primary), var(--page-light), var(--pink)); }
        .hero-tag { display: inline-flex; background: var(--page-glow); border: 1px solid var(--page-primary); padding: 5px 14px; border-radius: 9999px; font-size: 12px; color: var(--page-light); margin-bottom: 12px; width: fit-content; }
        .hero-left h1 { font-size: 28px; margin-bottom: 8px; }
        .hero-left p { font-size: 13px; color: var(--text-secondary); line-height: 1.6; }
        .hero-metrics { display: grid; grid-template-columns: 1fr 1fr; gap: 12px; }
        .hero-metric { background: var(--bg-card-alt); border: 1px solid var(--border-color); border-radius: 12px; padding: 16px; text-align: center; position: relative; }
        .hero-metric::before { content: ''; position: absolute; bottom: 0; left: 0; right: 0; height: 3px; background: var(--page-primary); border-radius: 0 0 12px 12px; }
        .hero-metric-value { font-size: 24px; font-weight: 700; color: var(--page-light); }
        .hero-metric-label { font-size: 10px; color: var(--text-muted); margin-top: 4px; }
        
        .module { margin-bottom: 32px; padding-top: 24px; border-top: 1px solid var(--border-color); }
        .module:first-of-type { border-top: none; padding-top: 0; }
        .module-header { display: flex; align-items: center; gap: 16px; margin-bottom: 20px; }
        .module-icon { width: 52px; height: 52px; background: var(--page-glow); border: 2px solid var(--page-primary); border-radius: 14px; display: flex; align-items: center; justify-content: center; font-size: 26px; }
        .module-info h2 { font-size: 20px; margin-bottom: 4px; }
        .module-info p { font-size: 12px; color: var(--text-muted); }
        
        .overview-content { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 24px; margin-bottom: 20px; }
        .overview-content h3 { font-size: 16px; margin-bottom: 12px; color: var(--page-light); }
        .overview-content p { font-size: 13px; color: var(--text-secondary); line-height: 1.7; margin-bottom: 12px; }
        .overview-content p:last-child { margin-bottom: 0; }
        
        .viz-container { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 16px; overflow: hidden; margin-bottom: 20px; }
        .viz-header { padding: 12px 20px; background: var(--bg-card-alt); border-bottom: 1px solid var(--border-color); display: flex; justify-content: space-between; align-items: center; }
        .viz-title { font-size: 13px; font-weight: 600; display: flex; align-items: center; gap: 8px; }
        .viz-badge { font-size: 11px; padding: 4px 12px; border-radius: 9999px; background: var(--page-glow); color: var(--page-light); }
        .viz-canvas { padding: 20px; background: var(--bg-dark); }
        
        .metric-grid { display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px; margin-bottom: 20px; }
        .metric-card { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; }
        .metric-card-header { display: flex; align-items: center; gap: 12px; margin-bottom: 12px; padding-bottom: 12px; border-bottom: 1px solid var(--border-color); }
        .metric-card-icon { width: 44px; height: 44px; background: var(--page-glow); border-radius: 10px; display: flex; align-items: center; justify-content: center; font-size: 22px; }
        .metric-card-title { font-size: 16px; font-weight: 600; }
        .metric-card-subtitle { font-size: 10px; color: var(--text-muted); }
        .metric-formula { background: var(--bg-dark); border-radius: 8px; padding: 12px 16px; font-family: 'Monaco', monospace; font-size: 12px; color: var(--page-light); margin-bottom: 12px; text-align: center; }
        .metric-desc { font-size: 11px; color: var(--text-secondary); line-height: 1.6; margin-bottom: 12px; }
        .metric-when { background: var(--bg-card-alt); border-radius: 8px; padding: 12px; }
        .metric-when-label { font-size: 9px; color: var(--text-muted); margin-bottom: 4px; }
        .metric-when-text { font-size: 11px; color: var(--text-secondary); }
        .metric-range { display: flex; justify-content: space-between; margin-top: 12px; padding-top: 12px; border-top: 1px solid var(--border-color); }
        .metric-range-item { text-align: center; }
        .metric-range-value { font-size: 12px; font-weight: 600; }
        .metric-range-label { font-size: 9px; color: var(--text-muted); }
        .metric-range-value.bad { color: var(--red); }
        .metric-range-value.ok { color: var(--yellow); }
        .metric-range-value.good { color: var(--green); }
        
        .confusion-matrix { display: grid; grid-template-columns: auto 1fr 1fr; grid-template-rows: auto 1fr 1fr; gap: 4px; max-width: 400px; margin: 0 auto; }
        .cm-cell { padding: 16px; text-align: center; border-radius: 8px; font-size: 11px; }
        .cm-header { background: transparent; color: var(--text-muted); font-weight: 600; }
        .cm-corner { background: transparent; }
        .cm-tp { background: rgba(16, 185, 129, 0.3); border: 2px solid var(--green); }
        .cm-tn { background: rgba(16, 185, 129, 0.2); border: 2px solid var(--green); }
        .cm-fp { background: rgba(239, 68, 68, 0.3); border: 2px solid var(--red); }
        .cm-fn { background: rgba(239, 68, 68, 0.2); border: 2px solid var(--red); }
        .cm-label { font-weight: 700; font-size: 13px; }
        .cm-desc { font-size: 9px; color: var(--text-muted); margin-top: 4px; }
        
        .metrics-table { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; overflow: hidden; margin-bottom: 20px; }
        .metrics-table-header { background: var(--bg-card-alt); padding: 12px 16px; border-bottom: 1px solid var(--border-color); }
        .metrics-table-title { font-size: 13px; font-weight: 600; }
        .metrics-table table { width: 100%; border-collapse: collapse; }
        .metrics-table th { background: var(--bg-card-alt); padding: 10px 12px; text-align: left; font-size: 10px; font-weight: 600; color: var(--text-muted); border-bottom: 1px solid var(--border-color); }
        .metrics-table td { padding: 12px; font-size: 11px; border-bottom: 1px solid var(--border-color); vertical-align: middle; }
        .metrics-table tr:hover td { background: var(--bg-hover); }
        .metric-name { font-weight: 600; display: flex; align-items: center; gap: 8px; }
        .metric-tag { font-size: 9px; padding: 2px 8px; border-radius: 4px; background: var(--page-glow); color: var(--page-light); }
        
        .scenario-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; margin-bottom: 20px; }
        .scenario-card { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; }
        .scenario-icon { font-size: 28px; margin-bottom: 12px; }
        .scenario-title { font-size: 14px; font-weight: 600; margin-bottom: 8px; }
        .scenario-desc { font-size: 11px; color: var(--text-secondary); line-height: 1.5; margin-bottom: 12px; }
        .scenario-metric { background: var(--bg-card-alt); border-radius: 8px; padding: 10px 12px; }
        .scenario-metric-label { font-size: 9px; color: var(--text-muted); }
        .scenario-metric-value { font-size: 12px; font-weight: 600; color: var(--page-light); }
        
        .best-practices { display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px; margin-bottom: 20px; }
        .practice-card { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; }
        .practice-card.do { border-left: 3px solid var(--green); }
        .practice-card.dont { border-left: 3px solid var(--red); }
        .practice-title { font-size: 14px; font-weight: 600; margin-bottom: 12px; display: flex; align-items: center; gap: 8px; }
        .practice-title.do { color: var(--green); }
        .practice-title.dont { color: var(--red); }
        .practice-list { list-style: none; }
        .practice-list li { font-size: 11px; color: var(--text-secondary); padding: 6px 0; border-bottom: 1px solid var(--border-color); display: flex; align-items: flex-start; gap: 8px; }
        .practice-list li:last-child { border-bottom: none; }
        .practice-list li::before { content: '‚úì'; color: var(--green); flex-shrink: 0; }
        .practice-card.dont .practice-list li::before { content: '‚úó'; color: var(--red); }
        
        .agent-grid { display: grid; grid-template-columns: 1fr 1.5fr; gap: 20px; }
        .agent-info { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; }
        .agent-avatar { width: 56px; height: 56px; background: linear-gradient(135deg, var(--page-primary), var(--page-light)); border-radius: 14px; display: flex; align-items: center; justify-content: center; font-size: 28px; margin-bottom: 16px; }
        .agent-name { font-size: 18px; font-weight: 600; margin-bottom: 4px; }
        .agent-role { font-size: 11px; color: var(--page-light); margin-bottom: 16px; }
        .agent-desc { font-size: 11px; color: var(--text-secondary); line-height: 1.7; margin-bottom: 16px; }
        .agent-capabilities { list-style: none; }
        .agent-capabilities li { font-size: 11px; color: var(--text-secondary); padding: 6px 0; display: flex; align-items: center; gap: 8px; }
        .agent-capabilities li::before { content: '‚ú¶'; color: var(--page-light); }
        
        .code-panel { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; overflow: hidden; }
        .code-header { display: flex; align-items: center; background: var(--bg-card-alt); border-bottom: 1px solid var(--border-color); }
        .code-tab { padding: 12px 20px; font-size: 12px; color: var(--text-muted); cursor: pointer; border-bottom: 2px solid transparent; }
        .code-tab.active { color: var(--page-light); border-bottom-color: var(--page-light); background: rgba(139,92,246,0.1); }
        .code-filename { margin-left: auto; padding: 12px 20px; font-size: 11px; color: var(--text-muted); font-family: monospace; }
        .code-content { padding: 16px; font-family: 'Monaco', 'Consolas', monospace; font-size: 11px; line-height: 1.8; background: var(--bg-dark); overflow-x: auto; }
        .code-comment { color: #6A737D; }
        .code-keyword { color: #FF7B72; }
        .code-string { color: #A5D6FF; }
        .code-function { color: #D2A8FF; }
        
        .related-pages { display: grid; grid-template-columns: repeat(3, 1fr); gap: 12px; }
        .related-card { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 10px; padding: 16px; text-decoration: none; color: inherit; transition: all 0.2s; }
        .related-card:hover { border-color: var(--page-primary); transform: translateY(-2px); }
        .related-num { font-size: 10px; color: var(--page-light); margin-bottom: 4px; }
        .related-title { font-size: 12px; font-weight: 600; margin-bottom: 4px; }
        .related-desc { font-size: 10px; color: var(--text-muted); }
        
        footer { background: var(--bg-card); border-top: 1px solid var(--border-color); padding: 20px 32px; margin-left: var(--sidebar-width); }
        .footer-nav { display: flex; justify-content: space-between; align-items: center; }
        .footer-link { display: flex; align-items: center; gap: 12px; text-decoration: none; color: var(--text-secondary); padding: 12px 20px; background: var(--bg-card-alt); border: 1px solid var(--border-color); border-radius: 10px; transition: all 0.2s; }
        .footer-link:hover { border-color: var(--page-primary); color: var(--text-primary); }
        .footer-link-label { font-size: 10px; color: var(--text-muted); }
        .footer-link-title { font-size: 13px; font-weight: 600; }
        .footer-brand { font-size: 12px; color: var(--text-muted); }
        .footer-brand strong { color: var(--brand-orange); }
        
        @media (max-width: 1024px) {
            .sidebar { display: none; }
            .main-wrapper { margin-left: 0; }
            footer { margin-left: 0; }
            .hero-compact, .best-practices, .agent-grid, .metric-grid { grid-template-columns: 1fr; }
            .scenario-grid, .related-pages { grid-template-columns: repeat(2, 1fr); }
        }
    </style>
</head>
<body>

<header>
    <div class="logo"><span>STRATEGY</span>HUB</div>
    <div class="breadcrumb">
        <a href="#">Strategy Hub</a>
        <span class="sep">/</span>
        <a href="cat01-aiml-foundations-overview.html">AI/ML Foundations</a>
        <span class="sep">/</span>
        <span class="current">Evaluation Metrics</span>
    </div>
</header>

<aside class="sidebar">
    <div class="sidebar-section">
        <div class="sidebar-title">üß† Category 01</div>
        <ul class="sidebar-nav">
            <li><a href="cat01-aiml-foundations-overview.html"><span class="nav-icon">üè†</span> Overview</a></li>
        </ul>
    </div>
    <div class="sidebar-section">
        <div class="sidebar-title">üìÑ This Page</div>
        <ul class="sidebar-nav">
            <li><a href="#overview" class="active"><span class="nav-icon">üìñ</span> Overview</a></li>
            <li><a href="#confusion"><span class="nav-icon">üéØ</span> Confusion Matrix</a></li>
            <li><a href="#classification"><span class="nav-icon">üìä</span> Classification</a></li>
            <li><a href="#regression"><span class="nav-icon">üìà</span> Regression</a></li>
            <li><a href="#other"><span class="nav-icon">üîç</span> Other Metrics</a></li>
            <li><a href="#scenarios"><span class="nav-icon">üí°</span> When to Use</a></li>
            <li><a href="#practices"><span class="nav-icon">‚úÖ</span> Best Practices</a></li>
            <li><a href="#agent"><span class="nav-icon">ü§ñ</span> Agent This</a></li>
        </ul>
    </div>
    <div class="sidebar-section">
        <div class="sidebar-title">üìÑ Other Pages</div>
        <ul class="sidebar-nav">
            <li><a href="cat01-p1-supervised-learning.html"><span class="nav-icon">üè∑Ô∏è</span> Supervised Learning</a></li>
            <li><a href="cat01-p2-unsupervised-learning.html"><span class="nav-icon">üîç</span> Unsupervised Learning</a></li>
            <li><a href="cat01-p3-reinforcement-learning.html"><span class="nav-icon">üéÆ</span> Reinforcement Learning</a></li>
            <li><a href="cat01-p4-neural-networks.html"><span class="nav-icon">üîÆ</span> Neural Networks</a></li>
            <li><a href="cat01-p5-ml-pipeline.html"><span class="nav-icon">‚öôÔ∏è</span> ML Pipeline</a></li>
            <li><a href="cat01-p6-algorithm-selection.html"><span class="nav-icon">üéØ</span> Algorithm Selection</a></li>
            <li><a href="cat01-p7-ml-frameworks.html"><span class="nav-icon">üõ†Ô∏è</span> ML Frameworks</a></li>
            <li><a href="cat01-p8-evaluation-metrics.html" class="active"><span class="nav-icon">üìà</span> Evaluation Metrics</a></li>
        </ul>
    </div>
</aside>

<div class="main-wrapper">
    <div class="main-content">
        
        <!-- HERO -->
        <section class="hero-compact" id="overview">
            <div class="hero-left">
                <span class="hero-tag">üìà Page 1.8</span>
                <h1>Evaluation Metrics</h1>
                <p>Master the art of measuring model performance. Learn when to use accuracy vs F1-score, why RMSE isn't always the answer, and how to choose metrics that align with business objectives. The right metric can mean the difference between success and failure.</p>
            </div>
            <div class="hero-metrics">
                <div class="hero-metric"><div class="hero-metric-value">20+</div><div class="hero-metric-label">Common ML Metrics</div></div>
                <div class="hero-metric"><div class="hero-metric-value">‚â†</div><div class="hero-metric-label">Accuracy ‚â† Success</div></div>
                <div class="hero-metric"><div class="hero-metric-value">F1</div><div class="hero-metric-label">Balances Precision/Recall</div></div>
                <div class="hero-metric"><div class="hero-metric-value">AUC</div><div class="hero-metric-label">Threshold-Independent</div></div>
            </div>
        </section>

        <!-- OVERVIEW -->
        <section class="module">
            <div class="module-header">
                <div class="module-icon">üìñ</div>
                <div class="module-info">
                    <h2>Why Metrics Matter</h2>
                    <p>Choosing the right measure of success</p>
                </div>
            </div>
            
            <div class="overview-content">
                <h3>The Metric Selection Problem</h3>
                <p>Your choice of evaluation metric fundamentally shapes what your model learns to optimize. A spam filter optimized for accuracy might let spam through; optimized for recall, it might block legitimate emails. The "best" metric depends entirely on your business context and the relative costs of different errors.</p>
                
                <h3>The Accuracy Trap</h3>
                <p>Accuracy is the most intuitive metric but often the most misleading. In a fraud detection system where 99.9% of transactions are legitimate, a model that always predicts "not fraud" achieves 99.9% accuracy while being completely useless. This is why understanding class imbalance and alternative metrics is crucial.</p>
                
                <h3>Business Alignment</h3>
                <p>Technical metrics (F1, AUC-ROC) must ultimately connect to business outcomes (revenue, user satisfaction, cost savings). A 2% improvement in precision might save millions in fraud losses or mean nothing at all‚Äîcontext determines value. Always translate ML metrics into business impact.</p>
            </div>
        </section>

        <!-- CONFUSION MATRIX -->
        <section class="module" id="confusion">
            <div class="module-header">
                <div class="module-icon">üéØ</div>
                <div class="module-info">
                    <h2>The Confusion Matrix</h2>
                    <p>Foundation of classification metrics</p>
                </div>
            </div>
            
            <div class="viz-container">
                <div class="viz-header">
                    <div class="viz-title">üìä Understanding Predictions vs Reality</div>
                    <div class="viz-badge">Binary Classification</div>
                </div>
                <div class="viz-canvas">
                    <svg viewBox="0 0 900 380" style="width: 100%; height: auto;">
                        <!-- Labels -->
                        <text x="450" y="30" text-anchor="middle" fill="#A78BFA" font-size="14" font-weight="600">PREDICTED</text>
                        <text x="90" y="200" text-anchor="middle" fill="#A78BFA" font-size="14" font-weight="600" transform="rotate(-90, 90, 200)">ACTUAL</text>
                        
                        <!-- Column Headers -->
                        <text x="280" y="70" text-anchor="middle" fill="#fff" font-size="12">Positive</text>
                        <text x="450" y="70" text-anchor="middle" fill="#fff" font-size="12">Negative</text>
                        
                        <!-- Row Headers -->
                        <text x="140" y="160" text-anchor="middle" fill="#fff" font-size="12">Positive</text>
                        <text x="140" y="280" text-anchor="middle" fill="#fff" font-size="12">Negative</text>
                        
                        <!-- TP Box -->
                        <g transform="translate(200, 90)">
                            <rect x="0" y="0" width="140" height="100" rx="10" fill="#10B98130" stroke="#10B981" stroke-width="3"/>
                            <text x="70" y="35" text-anchor="middle" fill="#34D399" font-size="18" font-weight="700">TP</text>
                            <text x="70" y="55" text-anchor="middle" fill="#fff" font-size="11">True Positive</text>
                            <text x="70" y="75" text-anchor="middle" fill="#6a6a7a" font-size="9">Correctly predicted</text>
                            <text x="70" y="88" text-anchor="middle" fill="#6a6a7a" font-size="9">positive ‚úì</text>
                        </g>
                        
                        <!-- FP Box -->
                        <g transform="translate(370, 90)">
                            <rect x="0" y="0" width="140" height="100" rx="10" fill="#EF444430" stroke="#EF4444" stroke-width="3"/>
                            <text x="70" y="35" text-anchor="middle" fill="#F87171" font-size="18" font-weight="700">FP</text>
                            <text x="70" y="55" text-anchor="middle" fill="#fff" font-size="11">False Positive</text>
                            <text x="70" y="75" text-anchor="middle" fill="#6a6a7a" font-size="9">Type I Error</text>
                            <text x="70" y="88" text-anchor="middle" fill="#6a6a7a" font-size="9">"False Alarm"</text>
                        </g>
                        
                        <!-- FN Box -->
                        <g transform="translate(200, 210)">
                            <rect x="0" y="0" width="140" height="100" rx="10" fill="#EF444420" stroke="#EF4444" stroke-width="2"/>
                            <text x="70" y="35" text-anchor="middle" fill="#F87171" font-size="18" font-weight="700">FN</text>
                            <text x="70" y="55" text-anchor="middle" fill="#fff" font-size="11">False Negative</text>
                            <text x="70" y="75" text-anchor="middle" fill="#6a6a7a" font-size="9">Type II Error</text>
                            <text x="70" y="88" text-anchor="middle" fill="#6a6a7a" font-size="9">"Missed Detection"</text>
                        </g>
                        
                        <!-- TN Box -->
                        <g transform="translate(370, 210)">
                            <rect x="0" y="0" width="140" height="100" rx="10" fill="#10B98120" stroke="#10B981" stroke-width="2"/>
                            <text x="70" y="35" text-anchor="middle" fill="#34D399" font-size="18" font-weight="700">TN</text>
                            <text x="70" y="55" text-anchor="middle" fill="#fff" font-size="11">True Negative</text>
                            <text x="70" y="75" text-anchor="middle" fill="#6a6a7a" font-size="9">Correctly predicted</text>
                            <text x="70" y="88" text-anchor="middle" fill="#6a6a7a" font-size="9">negative ‚úì</text>
                        </g>
                        
                        <!-- Formulas on right side -->
                        <g transform="translate(560, 100)">
                            <rect x="0" y="0" width="300" height="220" rx="10" fill="#1a1a2e" stroke="#2a2a3e"/>
                            <text x="150" y="30" text-anchor="middle" fill="#A78BFA" font-size="12" font-weight="600">Key Formulas</text>
                            
                            <text x="20" y="60" fill="#34D399" font-size="11" font-weight="600">Accuracy</text>
                            <text x="20" y="78" fill="#fff" font-size="10" font-family="monospace">(TP + TN) / Total</text>
                            
                            <text x="20" y="105" fill="#3B82F6" font-size="11" font-weight="600">Precision</text>
                            <text x="20" y="123" fill="#fff" font-size="10" font-family="monospace">TP / (TP + FP)</text>
                            
                            <text x="20" y="150" fill="#F59E0B" font-size="11" font-weight="600">Recall (Sensitivity)</text>
                            <text x="20" y="168" fill="#fff" font-size="10" font-family="monospace">TP / (TP + FN)</text>
                            
                            <text x="20" y="195" fill="#EC4899" font-size="11" font-weight="600">F1-Score</text>
                            <text x="20" y="213" fill="#fff" font-size="10" font-family="monospace">2 √ó (P √ó R) / (P + R)</text>
                        </g>
                    </svg>
                </div>
            </div>
        </section>

        <!-- CLASSIFICATION METRICS -->
        <section class="module" id="classification">
            <div class="module-header">
                <div class="module-icon">üìä</div>
                <div class="module-info">
                    <h2>Classification Metrics</h2>
                    <p>Measuring categorical prediction performance</p>
                </div>
            </div>
            
            <div class="metric-grid">
                <div class="metric-card">
                    <div class="metric-card-header">
                        <div class="metric-card-icon">üéØ</div>
                        <div>
                            <div class="metric-card-title">Accuracy</div>
                            <div class="metric-card-subtitle">Overall correctness</div>
                        </div>
                    </div>
                    <div class="metric-formula">(TP + TN) / (TP + TN + FP + FN)</div>
                    <div class="metric-desc">Percentage of correct predictions. Simple and intuitive but misleading for imbalanced datasets. A 99% accurate fraud detector might catch zero fraud.</div>
                    <div class="metric-when">
                        <div class="metric-when-label">USE WHEN</div>
                        <div class="metric-when-text">Classes are balanced and all errors have equal cost</div>
                    </div>
                    <div class="metric-range">
                        <div class="metric-range-item"><div class="metric-range-value bad">< 70%</div><div class="metric-range-label">Poor</div></div>
                        <div class="metric-range-item"><div class="metric-range-value ok">70-90%</div><div class="metric-range-label">Good</div></div>
                        <div class="metric-range-item"><div class="metric-range-value good">> 90%</div><div class="metric-range-label">Excellent</div></div>
                    </div>
                </div>
                
                <div class="metric-card">
                    <div class="metric-card-header">
                        <div class="metric-card-icon">üîµ</div>
                        <div>
                            <div class="metric-card-title">Precision</div>
                            <div class="metric-card-subtitle">Positive predictive value</div>
                        </div>
                    </div>
                    <div class="metric-formula">TP / (TP + FP)</div>
                    <div class="metric-desc">Of all positive predictions, how many were correct? High precision means few false alarms. Critical when false positives are costly (spam filter blocking important emails).</div>
                    <div class="metric-when">
                        <div class="metric-when-label">USE WHEN</div>
                        <div class="metric-when-text">False positives are expensive or annoying</div>
                    </div>
                    <div class="metric-range">
                        <div class="metric-range-item"><div class="metric-range-value bad">< 60%</div><div class="metric-range-label">Poor</div></div>
                        <div class="metric-range-item"><div class="metric-range-value ok">60-85%</div><div class="metric-range-label">Good</div></div>
                        <div class="metric-range-item"><div class="metric-range-value good">> 85%</div><div class="metric-range-label">Excellent</div></div>
                    </div>
                </div>
                
                <div class="metric-card">
                    <div class="metric-card-header">
                        <div class="metric-card-icon">üü†</div>
                        <div>
                            <div class="metric-card-title">Recall (Sensitivity)</div>
                            <div class="metric-card-subtitle">True positive rate</div>
                        </div>
                    </div>
                    <div class="metric-formula">TP / (TP + FN)</div>
                    <div class="metric-desc">Of all actual positives, how many did we catch? High recall means few missed cases. Critical when false negatives are dangerous (disease screening, fraud detection).</div>
                    <div class="metric-when">
                        <div class="metric-when-label">USE WHEN</div>
                        <div class="metric-when-text">Missing positive cases is costly or dangerous</div>
                    </div>
                    <div class="metric-range">
                        <div class="metric-range-item"><div class="metric-range-value bad">< 60%</div><div class="metric-range-label">Poor</div></div>
                        <div class="metric-range-item"><div class="metric-range-value ok">60-85%</div><div class="metric-range-label">Good</div></div>
                        <div class="metric-range-item"><div class="metric-range-value good">> 85%</div><div class="metric-range-label">Excellent</div></div>
                    </div>
                </div>
                
                <div class="metric-card">
                    <div class="metric-card-header">
                        <div class="metric-card-icon">‚öñÔ∏è</div>
                        <div>
                            <div class="metric-card-title">F1-Score</div>
                            <div class="metric-card-subtitle">Harmonic mean of P & R</div>
                        </div>
                    </div>
                    <div class="metric-formula">2 √ó (Precision √ó Recall) / (Precision + Recall)</div>
                    <div class="metric-desc">Balances precision and recall into a single metric. Penalizes extreme values‚Äîyou can't get a high F1 by sacrificing one for the other. The go-to metric for imbalanced datasets.</div>
                    <div class="metric-when">
                        <div class="metric-when-label">USE WHEN</div>
                        <div class="metric-when-text">You need balance between precision and recall</div>
                    </div>
                    <div class="metric-range">
                        <div class="metric-range-item"><div class="metric-range-value bad">< 50%</div><div class="metric-range-label">Poor</div></div>
                        <div class="metric-range-item"><div class="metric-range-value ok">50-80%</div><div class="metric-range-label">Good</div></div>
                        <div class="metric-range-item"><div class="metric-range-value good">> 80%</div><div class="metric-range-label">Excellent</div></div>
                    </div>
                </div>
                
                <div class="metric-card">
                    <div class="metric-card-header">
                        <div class="metric-card-icon">üìà</div>
                        <div>
                            <div class="metric-card-title">AUC-ROC</div>
                            <div class="metric-card-subtitle">Area Under ROC Curve</div>
                        </div>
                    </div>
                    <div class="metric-formula">Area under TPR vs FPR curve</div>
                    <div class="metric-desc">Measures ranking ability across all thresholds. AUC = 0.5 is random guessing, AUC = 1.0 is perfect. Threshold-independent‚Äîgreat for comparing models before choosing a threshold.</div>
                    <div class="metric-when">
                        <div class="metric-when-label">USE WHEN</div>
                        <div class="metric-when-text">Comparing models or threshold is adjustable</div>
                    </div>
                    <div class="metric-range">
                        <div class="metric-range-item"><div class="metric-range-value bad">< 0.7</div><div class="metric-range-label">Poor</div></div>
                        <div class="metric-range-item"><div class="metric-range-value ok">0.7-0.9</div><div class="metric-range-label">Good</div></div>
                        <div class="metric-range-item"><div class="metric-range-value good">> 0.9</div><div class="metric-range-label">Excellent</div></div>
                    </div>
                </div>
                
                <div class="metric-card">
                    <div class="metric-card-header">
                        <div class="metric-card-icon">üìâ</div>
                        <div>
                            <div class="metric-card-title">Log Loss</div>
                            <div class="metric-card-subtitle">Cross-entropy loss</div>
                        </div>
                    </div>
                    <div class="metric-formula">-Œ£ y¬∑log(p) + (1-y)¬∑log(1-p)</div>
                    <div class="metric-desc">Penalizes confident wrong predictions heavily. Unlike accuracy, considers probability calibration. Lower is better. Perfect predictions = 0, random = 0.693 for binary.</div>
                    <div class="metric-when">
                        <div class="metric-when-label">USE WHEN</div>
                        <div class="metric-when-text">Probability calibration matters, not just ranking</div>
                    </div>
                    <div class="metric-range">
                        <div class="metric-range-item"><div class="metric-range-value bad">> 0.5</div><div class="metric-range-label">Poor</div></div>
                        <div class="metric-range-item"><div class="metric-range-value ok">0.2-0.5</div><div class="metric-range-label">Good</div></div>
                        <div class="metric-range-item"><div class="metric-range-value good">< 0.2</div><div class="metric-range-label">Excellent</div></div>
                    </div>
                </div>
            </div>
        </section>

        <!-- REGRESSION METRICS -->
        <section class="module" id="regression">
            <div class="module-header">
                <div class="module-icon">üìà</div>
                <div class="module-info">
                    <h2>Regression Metrics</h2>
                    <p>Measuring continuous prediction performance</p>
                </div>
            </div>
            
            <div class="metrics-table">
                <div class="metrics-table-header">
                    <div class="metrics-table-title">Regression Metric Comparison</div>
                </div>
                <table>
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Formula</th>
                            <th>Range</th>
                            <th>Outlier Sensitivity</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><div class="metric-name">üìè MAE</div></td>
                            <td>Mean |y - ≈∑|</td>
                            <td>[0, ‚àû) ‚Üì</td>
                            <td><span class="metric-tag">Robust</span></td>
                            <td>Interpretable error in original units</td>
                        </tr>
                        <tr>
                            <td><div class="metric-name">üìê MSE</div></td>
                            <td>Mean (y - ≈∑)¬≤</td>
                            <td>[0, ‚àû) ‚Üì</td>
                            <td><span class="metric-tag" style="background:rgba(239,68,68,0.2);color:#EF4444">Sensitive</span></td>
                            <td>When large errors are especially bad</td>
                        </tr>
                        <tr>
                            <td><div class="metric-name">üìä RMSE</div></td>
                            <td>‚àöMSE</td>
                            <td>[0, ‚àû) ‚Üì</td>
                            <td><span class="metric-tag" style="background:rgba(239,68,68,0.2);color:#EF4444">Sensitive</span></td>
                            <td>Same units as target, penalizes large errors</td>
                        </tr>
                        <tr>
                            <td><div class="metric-name">üìà R¬≤ Score</div></td>
                            <td>1 - SS_res/SS_tot</td>
                            <td>(-‚àû, 1] ‚Üë</td>
                            <td><span class="metric-tag">Moderate</span></td>
                            <td>Variance explained, model comparison</td>
                        </tr>
                        <tr>
                            <td><div class="metric-name">üìâ MAPE</div></td>
                            <td>Mean |y-≈∑|/|y| √ó 100</td>
                            <td>[0, ‚àû) ‚Üì</td>
                            <td><span class="metric-tag">Robust</span></td>
                            <td>Percentage error, scale-independent</td>
                        </tr>
                        <tr>
                            <td><div class="metric-name">üéØ Huber Loss</div></td>
                            <td>Hybrid MAE/MSE</td>
                            <td>[0, ‚àû) ‚Üì</td>
                            <td><span class="metric-tag">Balanced</span></td>
                            <td>Robust to outliers while penalizing large errors</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <div class="overview-content">
                <h3>Choosing Between MAE, MSE, and RMSE</h3>
                <p><strong>MAE (Mean Absolute Error)</strong> treats all errors equally and is robust to outliers. Use when you want interpretable errors in original units and outliers shouldn't dominate.</p>
                <p><strong>MSE (Mean Squared Error)</strong> heavily penalizes large errors due to squaring. Use when large errors are disproportionately bad (e.g., predicting structural loads).</p>
                <p><strong>RMSE (Root MSE)</strong> is MSE in original units. Most commonly used because it balances interpretability with outlier sensitivity. Good default choice.</p>
                <p><strong>R¬≤ Score</strong> indicates proportion of variance explained (0-1, higher is better). Useful for comparing models but doesn't tell you the actual error magnitude.</p>
            </div>
        </section>

        <!-- OTHER METRICS -->
        <section class="module" id="other">
            <div class="module-header">
                <div class="module-icon">üîç</div>
                <div class="module-info">
                    <h2>Other Important Metrics</h2>
                    <p>Clustering, ranking, and specialized metrics</p>
                </div>
            </div>
            
            <div class="metrics-table">
                <div class="metrics-table-header">
                    <div class="metrics-table-title">Specialized Metrics by Task</div>
                </div>
                <table>
                    <thead>
                        <tr>
                            <th>Task</th>
                            <th>Metric</th>
                            <th>What It Measures</th>
                            <th>Range</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><span class="metric-tag">Clustering</span></td>
                            <td>Silhouette Score</td>
                            <td>Cluster cohesion vs separation</td>
                            <td>[-1, 1] ‚Üë</td>
                        </tr>
                        <tr>
                            <td><span class="metric-tag">Clustering</span></td>
                            <td>Davies-Bouldin Index</td>
                            <td>Cluster similarity (lower = better separation)</td>
                            <td>[0, ‚àû) ‚Üì</td>
                        </tr>
                        <tr>
                            <td><span class="metric-tag">Clustering</span></td>
                            <td>Adjusted Rand Index</td>
                            <td>Agreement with ground truth</td>
                            <td>[-1, 1] ‚Üë</td>
                        </tr>
                        <tr>
                            <td><span class="metric-tag">Ranking</span></td>
                            <td>NDCG</td>
                            <td>Ranking quality with position weighting</td>
                            <td>[0, 1] ‚Üë</td>
                        </tr>
                        <tr>
                            <td><span class="metric-tag">Ranking</span></td>
                            <td>MAP (Mean Avg Precision)</td>
                            <td>Average precision across queries</td>
                            <td>[0, 1] ‚Üë</td>
                        </tr>
                        <tr>
                            <td><span class="metric-tag">NLP</span></td>
                            <td>BLEU</td>
                            <td>Translation/generation quality</td>
                            <td>[0, 1] ‚Üë</td>
                        </tr>
                        <tr>
                            <td><span class="metric-tag">NLP</span></td>
                            <td>Perplexity</td>
                            <td>Language model uncertainty</td>
                            <td>[1, ‚àû) ‚Üì</td>
                        </tr>
                        <tr>
                            <td><span class="metric-tag">Object Detection</span></td>
                            <td>mAP (mean Avg Precision)</td>
                            <td>Detection accuracy across IoU thresholds</td>
                            <td>[0, 1] ‚Üë</td>
                        </tr>
                        <tr>
                            <td><span class="metric-tag">Segmentation</span></td>
                            <td>IoU (Jaccard)</td>
                            <td>Overlap between predicted and true regions</td>
                            <td>[0, 1] ‚Üë</td>
                        </tr>
                        <tr>
                            <td><span class="metric-tag">Time Series</span></td>
                            <td>MASE</td>
                            <td>Scale-free forecast accuracy</td>
                            <td>[0, ‚àû) ‚Üì</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <!-- WHEN TO USE WHAT -->
        <section class="module" id="scenarios">
            <div class="module-header">
                <div class="module-icon">üí°</div>
                <div class="module-info">
                    <h2>When to Use What</h2>
                    <p>Metric selection by business scenario</p>
                </div>
            </div>
            
            <div class="scenario-grid">
                <div class="scenario-card">
                    <div class="scenario-icon">üè•</div>
                    <div class="scenario-title">Medical Diagnosis</div>
                    <div class="scenario-desc">Missing a disease is worse than a false alarm. Patients can get follow-up tests, but missed diagnoses can be fatal.</div>
                    <div class="scenario-metric">
                        <div class="scenario-metric-label">PRIMARY METRIC</div>
                        <div class="scenario-metric-value">Recall (Sensitivity)</div>
                    </div>
                </div>
                
                <div class="scenario-card">
                    <div class="scenario-icon">üìß</div>
                    <div class="scenario-title">Spam Filtering</div>
                    <div class="scenario-desc">Blocking important emails is worse than letting some spam through. Users hate missing legitimate messages.</div>
                    <div class="scenario-metric">
                        <div class="scenario-metric-label">PRIMARY METRIC</div>
                        <div class="scenario-metric-value">Precision</div>
                    </div>
                </div>
                
                <div class="scenario-card">
                    <div class="scenario-icon">üí≥</div>
                    <div class="scenario-title">Fraud Detection</div>
                    <div class="scenario-desc">Need to catch fraud (recall) without blocking too many legitimate transactions (precision). Balance matters.</div>
                    <div class="scenario-metric">
                        <div class="scenario-metric-label">PRIMARY METRIC</div>
                        <div class="scenario-metric-value">F1-Score or PR-AUC</div>
                    </div>
                </div>
                
                <div class="scenario-card">
                    <div class="scenario-icon">üîç</div>
                    <div class="scenario-title">Search Ranking</div>
                    <div class="scenario-desc">Top results matter most. Users rarely look past page one, so ranking quality at top positions is crucial.</div>
                    <div class="scenario-metric">
                        <div class="scenario-metric-label">PRIMARY METRIC</div>
                        <div class="scenario-metric-value">NDCG@k</div>
                    </div>
                </div>
                
                <div class="scenario-card">
                    <div class="scenario-icon">üè†</div>
                    <div class="scenario-title">House Price Prediction</div>
                    <div class="scenario-desc">Errors in either direction are bad. Large errors matter more than small ones. Need interpretable units.</div>
                    <div class="scenario-metric">
                        <div class="scenario-metric-label">PRIMARY METRIC</div>
                        <div class="scenario-metric-value">RMSE or MAE</div>
                    </div>
                </div>
                
                <div class="scenario-card">
                    <div class="scenario-icon">üìä</div>
                    <div class="scenario-title">Model Comparison</div>
                    <div class="scenario-desc">Comparing classifiers before choosing a threshold. Need threshold-independent evaluation.</div>
                    <div class="scenario-metric">
                        <div class="scenario-metric-label">PRIMARY METRIC</div>
                        <div class="scenario-metric-value">AUC-ROC</div>
                    </div>
                </div>
            </div>
        </section>

        <!-- BEST PRACTICES -->
        <section class="module" id="practices">
            <div class="module-header">
                <div class="module-icon">‚úÖ</div>
                <div class="module-info">
                    <h2>Best Practices</h2>
                    <p>Guidelines for effective model evaluation</p>
                </div>
            </div>
            
            <div class="best-practices">
                <div class="practice-card do">
                    <div class="practice-title do">‚úì Do's</div>
                    <ul class="practice-list">
                        <li>Choose metrics that align with business objectives</li>
                        <li>Use multiple metrics‚Äîno single metric tells the whole story</li>
                        <li>Always establish a baseline (random, majority class, simple model)</li>
                        <li>Report confidence intervals, not just point estimates</li>
                        <li>Use stratified splits for imbalanced datasets</li>
                        <li>Consider the cost matrix‚Äîweight errors by business impact</li>
                        <li>Validate on truly held-out data (time-based for time series)</li>
                        <li>Document your choice of metrics and thresholds</li>
                    </ul>
                </div>
                <div class="practice-card dont">
                    <div class="practice-title dont">‚úó Don'ts</div>
                    <ul class="practice-list">
                        <li>Don't use accuracy for imbalanced datasets</li>
                        <li>Never evaluate on training data</li>
                        <li>Avoid optimizing for a metric that doesn't match business goals</li>
                        <li>Don't ignore calibration when probabilities matter</li>
                        <li>Never compare models using different train/test splits</li>
                        <li>Don't report metrics without context (baseline, variance)</li>
                        <li>Avoid "metric hacking"‚Äîgaming the metric without real improvement</li>
                        <li>Never deploy without testing on realistic, recent data</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- AGENT THIS -->
        <section class="module" id="agent">
            <div class="module-header">
                <div class="module-icon">ü§ñ</div>
                <div class="module-info">
                    <h2>Agent This</h2>
                    <p>AI-powered assistant for model evaluation</p>
                </div>
            </div>
            
            <div class="agent-grid">
                <div class="agent-info">
                    <div class="agent-avatar">üìà</div>
                    <div class="agent-name">EvaluationAgent</div>
                    <div class="agent-role">Model Performance Specialist</div>
                    <div class="agent-desc">Expert in model evaluation, metric selection, and performance analysis. Helps choose the right metrics for your business context, interprets results, and identifies model weaknesses.</div>
                    <ul class="agent-capabilities">
                        <li>Metric selection based on business context</li>
                        <li>Comprehensive evaluation report generation</li>
                        <li>Error analysis and failure mode identification</li>
                        <li>Threshold optimization for classification</li>
                        <li>Cross-validation strategy design</li>
                        <li>Statistical significance testing</li>
                    </ul>
                </div>
                
                <div class="code-panel">
                    <div class="code-header">
                        <div class="code-tab active">Agent Definition</div>
                        <div class="code-tab">Evaluation Task</div>
                        <div class="code-filename">eval_agent.py</div>
                    </div>
                    <div class="code-content">
                        <pre><span class="code-comment"># eval_agent.py - Model Evaluation Agent</span>
<span class="code-keyword">from</span> crewai <span class="code-keyword">import</span> Agent, Task, Crew
<span class="code-keyword">from</span> sklearn.metrics <span class="code-keyword">import</span> classification_report
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

<span class="code-function">evaluation_agent</span> = Agent(
    role=<span class="code-string">"Model Evaluation Specialist"</span>,
    goal=<span class="code-string">"Provide comprehensive model performance analysis"</span>,
    backstory=<span class="code-string">"""Expert in ML metrics, statistical 
    testing, and translating model performance into 
    business impact. Deep knowledge of evaluation 
    pitfalls and best practices."""</span>,
    tools=[
        MetricCalculator(),
        ConfusionMatrixAnalyzer(),
        ThresholdOptimizer(),
        ErrorAnalyzer(),
        StatisticalTester(),
    ]
)

<span class="code-function">evaluation_task</span> = Task(
    description=<span class="code-string">"""
    1. Calculate all relevant metrics for the problem type
    2. Generate confusion matrix and error analysis
    3. Identify optimal threshold (if classification)
    4. Analyze errors by segment/feature
    5. Compare to baseline and previous models
    6. Test for statistical significance
    7. Translate metrics to business impact
    8. Generate actionable recommendations
    """</span>,
    agent=evaluation_agent,
    expected_output=<span class="code-string">"Comprehensive evaluation report"</span>
)

<span class="code-comment"># Execute model evaluation</span>
crew = Crew(agents=[evaluation_agent], tasks=[evaluation_task])
result = crew.kickoff()</pre>
                    </div>
                </div>
            </div>
        </section>

        <!-- RELATED PAGES -->
        <section class="module" id="related">
            <div class="module-header">
                <div class="module-icon">üîó</div>
                <div class="module-info">
                    <h2>Related Pages</h2>
                    <p>Continue learning with these related topics</p>
                </div>
            </div>
            
            <div class="related-pages">
                <a href="cat01-p1-supervised-learning.html" class="related-card">
                    <div class="related-num">Page 1.1</div>
                    <div class="related-title">Supervised Learning</div>
                    <div class="related-desc">Where most of these metrics apply</div>
                </a>
                <a href="cat01-p6-algorithm-selection.html" class="related-card">
                    <div class="related-num">Page 1.6</div>
                    <div class="related-title">Algorithm Selection</div>
                    <div class="related-desc">Use metrics to compare algorithms</div>
                </a>
                <a href="cat01-p5-ml-pipeline.html" class="related-card">
                    <div class="related-num">Page 1.5</div>
                    <div class="related-title">ML Pipeline</div>
                    <div class="related-desc">Where evaluation fits in the workflow</div>
                </a>
            </div>
        </section>

        <!-- CATEGORY COMPLETE BANNER -->
        <section class="module">
            <div class="overview-content" style="border: 2px solid var(--green); background: rgba(16,185,129,0.1);">
                <h3 style="color: var(--green);">üéâ Category 01 Complete!</h3>
                <p>Congratulations! You've completed all 8 pages of AI/ML Foundations. You now have a solid understanding of machine learning paradigms, neural networks, pipelines, algorithm selection, frameworks, and evaluation metrics.</p>
                <p style="margin-top: 12px;"><strong>Next up:</strong> Explore more categories in the Strategy Hub to continue building your AI/ML expertise.</p>
            </div>
        </section>

    </div>
</div>

<footer>
    <div class="footer-nav">
        <a href="cat01-p7-ml-frameworks.html" class="footer-link">
            <span>‚Üê</span>
            <div>
                <div class="footer-link-label">Previous Page</div>
                <div class="footer-link-title">1.7: ML Frameworks</div>
            </div>
        </a>
        <div class="footer-brand"><strong>STRATEGY</strong>HUB ‚Ä¢ Page 1.8 of 8 ‚úì</div>
        <a href="cat01-aiml-foundations-overview.html" class="footer-link">
            <div>
                <div class="footer-link-label">Back to</div>
                <div class="footer-link-title">Category Overview</div>
            </div>
            <span>‚Üí</span>
        </a>
    </div>
</footer>

</body>
</html>
