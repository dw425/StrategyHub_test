<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Model Evaluation | Generative AI | Strategy Hub</title>
    <style>
        :root {
            --brand-orange: #FF9900;
            --cat-primary: #EC4899;
            --cat-light: #F472B6;
            --cat-glow: rgba(236, 72, 153, 0.15);
            --page-primary: #EF4444;
            --page-light: #F87171;
            --page-glow: rgba(239, 68, 68, 0.15);
            --bg-dark: #0a0a0f;
            --bg-card: #12121a;
            --bg-card-alt: #1a1a2e;
            --bg-hover: #252538;
            --border-color: #2a2a3e;
            --text-primary: #ffffff;
            --text-secondary: #a0a0b0;
            --text-muted: #6a6a7a;
            --green: #10B981;
            --yellow: #F59E0B;
            --red: #EF4444;
            --blue: #3B82F6;
            --cyan: #06B6D4;
            --pink: #EC4899;
            --violet: #8B5CF6;
            --sidebar-width: 260px;
            --header-height: 60px;
        }
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background: var(--bg-dark); color: var(--text-primary); line-height: 1.6; }
        
        header { background: var(--bg-card); border-bottom: 1px solid var(--border-color); padding: 16px 32px; position: fixed; top: 0; left: 0; right: 0; z-index: 1000; height: var(--header-height); display: flex; justify-content: space-between; align-items: center; }
        .logo { font-size: 20px; font-weight: 700; text-decoration: none; color: inherit; }
        .logo span { color: var(--brand-orange); }
        .breadcrumb { display: flex; align-items: center; gap: 8px; font-size: 13px; }
        .breadcrumb a { color: var(--text-secondary); text-decoration: none; }
        .breadcrumb a:hover { color: var(--brand-orange); }
        .breadcrumb .sep { color: var(--text-muted); }
        .breadcrumb .current { color: var(--text-primary); }
        
        .sidebar { position: fixed; top: var(--header-height); left: 0; width: var(--sidebar-width); height: calc(100vh - var(--header-height)); background: var(--bg-card); border-right: 1px solid var(--border-color); overflow-y: auto; z-index: 100; padding: 24px 0; }
        .sidebar-section { margin-bottom: 24px; }
        .sidebar-title { font-size: 11px; text-transform: uppercase; letter-spacing: 1.5px; color: var(--cat-light); padding: 0 24px; margin-bottom: 8px; }
        .sidebar-nav { list-style: none; }
        .sidebar-nav a { display: flex; align-items: center; gap: 8px; padding: 10px 24px; color: var(--text-secondary); text-decoration: none; font-size: 12px; border-left: 3px solid transparent; transition: all 0.2s; }
        .sidebar-nav a:hover { background: var(--bg-hover); color: var(--text-primary); }
        .sidebar-nav a.active { background: var(--page-glow); color: var(--page-light); border-left-color: var(--page-light); }
        .nav-icon { font-size: 14px; width: 24px; text-align: center; }
        
        .main-wrapper { margin-left: var(--sidebar-width); margin-top: var(--header-height); }
        .main-content { max-width: 1100px; padding: 32px; }
        
        .hero-compact { display: grid; grid-template-columns: 1fr 1fr; gap: 32px; margin-bottom: 32px; padding: 28px; background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 16px; position: relative; }
        .hero-compact::before { content: ''; position: absolute; top: 0; left: 0; right: 0; height: 3px; background: linear-gradient(90deg, var(--page-primary), var(--page-light), var(--cat-primary)); }
        .hero-tag { display: inline-flex; background: var(--page-glow); border: 1px solid var(--page-primary); padding: 5px 14px; border-radius: 9999px; font-size: 12px; color: var(--page-light); margin-bottom: 12px; width: fit-content; }
        .hero-left h1 { font-size: 28px; margin-bottom: 8px; }
        .hero-left p { font-size: 13px; color: var(--text-secondary); line-height: 1.6; }
        .hero-metrics { display: grid; grid-template-columns: 1fr 1fr; gap: 12px; }
        .hero-metric { background: var(--bg-card-alt); border: 1px solid var(--border-color); border-radius: 12px; padding: 16px; text-align: center; position: relative; }
        .hero-metric::before { content: ''; position: absolute; bottom: 0; left: 0; right: 0; height: 3px; background: var(--page-primary); border-radius: 0 0 12px 12px; }
        .hero-metric-value { font-size: 24px; font-weight: 700; color: var(--page-light); }
        .hero-metric-label { font-size: 10px; color: var(--text-muted); margin-top: 4px; }
        
        .module { margin-bottom: 32px; padding-top: 24px; border-top: 1px solid var(--border-color); }
        .module:first-of-type { border-top: none; padding-top: 0; }
        .module-header { display: flex; align-items: center; gap: 16px; margin-bottom: 20px; }
        .module-icon { width: 52px; height: 52px; background: var(--page-glow); border: 2px solid var(--page-primary); border-radius: 14px; display: flex; align-items: center; justify-content: center; font-size: 26px; }
        .module-info h2 { font-size: 20px; margin-bottom: 4px; }
        .module-info p { font-size: 12px; color: var(--text-muted); }
        
        .overview-content { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 24px; margin-bottom: 20px; }
        .overview-content h3 { font-size: 16px; margin-bottom: 12px; color: var(--page-light); }
        .overview-content p { font-size: 13px; color: var(--text-secondary); line-height: 1.7; margin-bottom: 12px; }
        .overview-content p:last-child { margin-bottom: 0; }
        
        /* BENCHMARK CHART */
        .benchmark-chart { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 16px; overflow: hidden; margin-bottom: 20px; }
        .benchmark-header { padding: 12px 20px; background: var(--bg-card-alt); border-bottom: 1px solid var(--border-color); display: flex; justify-content: space-between; align-items: center; }
        .benchmark-title { font-size: 13px; font-weight: 600; }
        .benchmark-badge { font-size: 11px; padding: 4px 12px; border-radius: 9999px; background: var(--page-glow); color: var(--page-light); }
        .benchmark-body { padding: 24px; }
        .benchmark-row { display: flex; align-items: center; gap: 16px; margin-bottom: 16px; }
        .benchmark-row:last-child { margin-bottom: 0; }
        .benchmark-label { width: 140px; font-size: 12px; font-weight: 600; flex-shrink: 0; }
        .benchmark-bar-wrap { flex: 1; height: 28px; background: var(--bg-dark); border-radius: 6px; overflow: hidden; position: relative; }
        .benchmark-bar { height: 100%; border-radius: 6px; display: flex; align-items: center; justify-content: flex-end; padding-right: 10px; font-size: 11px; font-weight: 600; color: white; transition: width 0.5s ease; }
        .benchmark-bar.gpt4 { background: linear-gradient(90deg, var(--green), #34D399); }
        .benchmark-bar.claude { background: linear-gradient(90deg, var(--violet), #A78BFA); }
        .benchmark-bar.gemini { background: linear-gradient(90deg, var(--blue), #60A5FA); }
        .benchmark-bar.llama { background: linear-gradient(90deg, var(--yellow), #FBBF24); }
        .benchmark-legend { display: flex; gap: 20px; justify-content: center; margin-top: 20px; padding-top: 16px; border-top: 1px solid var(--border-color); }
        .legend-item { display: flex; align-items: center; gap: 6px; font-size: 11px; color: var(--text-muted); }
        .legend-dot { width: 12px; height: 12px; border-radius: 3px; }
        
        /* EVALUATION FRAMEWORK */
        .eval-framework { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 24px; margin-bottom: 20px; }
        .eval-title { font-size: 14px; font-weight: 600; margin-bottom: 16px; }
        .eval-layers { display: flex; flex-direction: column; gap: 12px; }
        .eval-layer { display: flex; align-items: center; gap: 16px; padding: 16px; background: var(--bg-card-alt); border-radius: 10px; border-left: 4px solid var(--border-color); }
        .eval-layer.auto { border-left-color: var(--blue); }
        .eval-layer.human { border-left-color: var(--green); }
        .eval-layer.llm { border-left-color: var(--violet); }
        .eval-layer-num { width: 32px; height: 32px; background: var(--bg-dark); border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 14px; font-weight: 700; color: var(--page-light); }
        .eval-layer-content { flex: 1; }
        .eval-layer-title { font-size: 13px; font-weight: 600; margin-bottom: 4px; }
        .eval-layer-desc { font-size: 11px; color: var(--text-muted); }
        .eval-layer-examples { display: flex; gap: 8px; margin-top: 8px; flex-wrap: wrap; }
        .eval-example { font-size: 9px; padding: 3px 8px; background: var(--bg-dark); border-radius: 4px; color: var(--text-secondary); }
        
        /* METRICS GRID */
        .metrics-grid { display: grid; grid-template-columns: repeat(4, 1fr); gap: 16px; margin-bottom: 20px; }
        .metric-card { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; text-align: center; }
        .metric-icon { font-size: 28px; margin-bottom: 10px; }
        .metric-name { font-size: 13px; font-weight: 600; margin-bottom: 4px; }
        .metric-desc { font-size: 10px; color: var(--text-muted); line-height: 1.5; margin-bottom: 10px; }
        .metric-formula { font-family: monospace; font-size: 10px; color: var(--page-light); background: var(--bg-dark); padding: 6px 10px; border-radius: 6px; }
        
        /* BENCHMARK TABLE */
        .comparison-table { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; overflow: hidden; margin-bottom: 20px; }
        .comparison-header { background: var(--bg-card-alt); padding: 12px 16px; border-bottom: 1px solid var(--border-color); }
        .comparison-title { font-size: 13px; font-weight: 600; }
        .comparison-table table { width: 100%; border-collapse: collapse; }
        .comparison-table th { background: var(--bg-card-alt); padding: 10px 12px; text-align: left; font-size: 10px; font-weight: 600; color: var(--text-muted); border-bottom: 1px solid var(--border-color); }
        .comparison-table td { padding: 12px; font-size: 11px; border-bottom: 1px solid var(--border-color); }
        .comparison-table tr:hover td { background: var(--bg-hover); }
        .score-bar { height: 6px; background: var(--bg-dark); border-radius: 3px; overflow: hidden; }
        .score-fill { height: 100%; border-radius: 3px; }
        .score-fill.high { background: var(--green); }
        .score-fill.mid { background: var(--yellow); }
        .score-fill.low { background: var(--red); }
        
        /* EVALUATION TOOLS */
        .tools-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; margin-bottom: 20px; }
        .tool-card { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; }
        .tool-header { display: flex; align-items: center; gap: 12px; margin-bottom: 12px; }
        .tool-icon { font-size: 28px; }
        .tool-name { font-size: 14px; font-weight: 600; }
        .tool-type { font-size: 10px; color: var(--page-light); }
        .tool-desc { font-size: 11px; color: var(--text-secondary); line-height: 1.6; margin-bottom: 12px; }
        .tool-features { list-style: none; }
        .tool-features li { font-size: 10px; color: var(--text-muted); padding: 3px 0; }
        .tool-features li::before { content: '‚úì '; color: var(--green); }
        
        /* RADAR CHART PLACEHOLDER */
        .radar-chart { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 24px; margin-bottom: 20px; }
        .radar-title { font-size: 14px; font-weight: 600; margin-bottom: 16px; text-align: center; }
        .radar-visual { display: flex; justify-content: center; align-items: center; height: 200px; position: relative; }
        .radar-hex { width: 180px; height: 180px; position: relative; }
        .radar-axis { position: absolute; width: 2px; background: var(--border-color); transform-origin: bottom center; left: 50%; }
        .radar-label { position: absolute; font-size: 10px; color: var(--text-muted); transform: translate(-50%, -50%); }
        .radar-area { position: absolute; inset: 20px; border: 2px solid var(--page-primary); border-radius: 50%; opacity: 0.3; background: var(--page-glow); }
        .radar-scores { display: grid; grid-template-columns: repeat(3, 1fr); gap: 12px; margin-top: 16px; }
        .radar-score { background: var(--bg-card-alt); border-radius: 8px; padding: 12px; text-align: center; }
        .radar-score-value { font-size: 18px; font-weight: 700; color: var(--page-light); }
        .radar-score-label { font-size: 9px; color: var(--text-muted); margin-top: 4px; }
        
        /* AGENT SECTION */
        .agent-grid { display: grid; grid-template-columns: 1fr 1.5fr; gap: 20px; }
        .agent-info { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; }
        .agent-avatar { width: 56px; height: 56px; background: linear-gradient(135deg, var(--page-primary), var(--page-light)); border-radius: 14px; display: flex; align-items: center; justify-content: center; font-size: 28px; margin-bottom: 16px; }
        .agent-name { font-size: 18px; font-weight: 600; margin-bottom: 4px; }
        .agent-role { font-size: 11px; color: var(--page-light); margin-bottom: 16px; }
        .agent-desc { font-size: 11px; color: var(--text-secondary); line-height: 1.7; margin-bottom: 16px; }
        .agent-capabilities { list-style: none; }
        .agent-capabilities li { font-size: 11px; color: var(--text-secondary); padding: 6px 0; display: flex; align-items: center; gap: 8px; }
        .agent-capabilities li::before { content: '‚ú¶'; color: var(--page-light); }
        
        .code-panel { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; overflow: hidden; }
        .code-header { display: flex; align-items: center; background: var(--bg-card-alt); border-bottom: 1px solid var(--border-color); }
        .code-tab { padding: 10px 16px; font-size: 11px; color: var(--text-muted); cursor: pointer; border-bottom: 2px solid transparent; }
        .code-tab.active { color: var(--page-light); border-bottom-color: var(--page-light); }
        .code-filename { margin-left: auto; padding: 10px 16px; font-size: 10px; color: var(--text-muted); font-family: monospace; }
        .code-content { padding: 16px; background: var(--bg-dark); overflow-x: auto; }
        .code-content pre { font-family: 'Monaco', 'Consolas', monospace; font-size: 11px; line-height: 1.6; color: var(--text-secondary); }
        .code-keyword { color: var(--pink); }
        .code-string { color: var(--green); }
        .code-function { color: var(--page-light); }
        .code-comment { color: var(--text-muted); }
        
        /* RELATED PAGES */
        .related-pages { display: grid; grid-template-columns: repeat(3, 1fr); gap: 12px; }
        .related-card { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 10px; padding: 16px; text-decoration: none; color: inherit; transition: all 0.2s; }
        .related-card:hover { border-color: var(--page-primary); transform: translateY(-2px); }
        .related-num { font-size: 10px; color: var(--page-light); margin-bottom: 4px; }
        .related-title { font-size: 13px; font-weight: 600; margin-bottom: 4px; }
        .related-desc { font-size: 10px; color: var(--text-muted); }
        
        footer { background: var(--bg-card); border-top: 1px solid var(--border-color); padding: 20px 32px; margin-left: var(--sidebar-width); }
        .footer-nav { display: flex; justify-content: space-between; align-items: center; }
        .footer-link { display: flex; align-items: center; gap: 12px; text-decoration: none; color: var(--text-secondary); padding: 12px 20px; background: var(--bg-card-alt); border: 1px solid var(--border-color); border-radius: 10px; transition: all 0.2s; }
        .footer-link:hover { border-color: var(--page-primary); color: var(--text-primary); }
        .footer-link-label { font-size: 10px; color: var(--text-muted); }
        .footer-link-title { font-size: 13px; font-weight: 600; }
        .footer-brand { font-size: 12px; color: var(--text-muted); }
        .footer-brand strong { color: var(--brand-orange); }
    </style>
</head>
<body>

<header>
    <a href="index.html" class="logo"><span>STRATEGY</span>HUB</a>
    <div class="header-tagline">Enterprise Technology Knowledge Base</div>
</header>

<aside class="sidebar">
    <div class="sidebar-section">
        <div class="sidebar-title">üé® Category 05</div>
        <ul class="sidebar-nav">
            <li><a href="cat05-generative-ai-overview.html"><span class="nav-icon">üè†</span> Overview</a></li>
        </ul>
    </div>
    <div class="sidebar-section">
        <div class="sidebar-title">üìÑ Pages</div>
        <ul class="sidebar-nav">
            <li><a href="cat05-p1-llm-fundamentals.html"><span class="nav-icon">üß†</span> 5.1 LLM Fundamentals</a></li>
            <li><a href="cat05-p2-prompt-engineering.html"><span class="nav-icon">‚úçÔ∏è</span> 5.2 Prompt Engineering</a></li>
            <li><a href="cat05-p3-rag-systems.html"><span class="nav-icon">üîç</span> 5.3 RAG Systems</a></li>
            <li><a href="cat05-p4-fine-tuning.html"><span class="nav-icon">üéØ</span> 5.4 Fine-Tuning</a></li>
            <li><a href="cat05-p5-embeddings.html"><span class="nav-icon">üìä</span> 5.5 Embeddings</a></li>
            <li><a href="cat05-p6-model-evaluation.html" class="active"><span class="nav-icon">üìà</span> 5.6 Evaluation</a></li>
            <li><a href="cat05-p7-multimodal.html"><span class="nav-icon">üñºÔ∏è</span> 5.7 Multimodal</a></li>
            <li><a href="cat05-p8-responsible-ai.html"><span class="nav-icon">‚öñÔ∏è</span> 5.8 Responsible AI</a></li>
            <li><a href="cat05-p9-deployment.html"><span class="nav-icon">üöÄ</span> 5.9 Deployment</a></li>
            <li><a href="cat05-p10-cost-optimization.html"><span class="nav-icon">üí∞</span> 5.10 Cost</a></li>
        </ul>
    </div>
</aside>

<div class="main-wrapper">
    <div class="main-content">
        
        <!-- HERO -->
        <section class="hero-compact">
            <div class="hero-left">
                <div class="hero-tag">üìà Page 5.6</div>
                <h1>Model Evaluation</h1>
                <p>Measure LLM performance objectively with benchmarks, metrics, and evaluation frameworks. Compare models, track quality, and make data-driven decisions.</p>
            </div>
            <div class="hero-metrics">
                <div class="hero-metric">
                    <div class="hero-metric-value">MMLU</div>
                    <div class="hero-metric-label">Knowledge Benchmark</div>
                </div>
                <div class="hero-metric">
                    <div class="hero-metric-value">HumanEval</div>
                    <div class="hero-metric-label">Coding Benchmark</div>
                </div>
                <div class="hero-metric">
                    <div class="hero-metric-value">LLM</div>
                    <div class="hero-metric-label">As-a-Judge</div>
                </div>
                <div class="hero-metric">
                    <div class="hero-metric-value">A/B</div>
                    <div class="hero-metric-label">Testing</div>
                </div>
            </div>
        </section>

        <!-- OVERVIEW -->
        <section class="module">
            <div class="module-header">
                <div class="module-icon">üìñ</div>
                <div class="module-info">
                    <h2>Why Evaluate LLMs?</h2>
                    <p>The importance of rigorous model evaluation</p>
                </div>
            </div>
            <div class="overview-content">
                <h3>Evaluation Fundamentals</h3>
                <p>LLM evaluation is challenging because outputs are open-ended text, not simple classifications. Unlike traditional ML where accuracy is straightforward, evaluating whether "The capital of France is Paris" is better than "Paris is the capital of France" requires nuanced judgment. This complexity demands multiple evaluation approaches.</p>
                <p>Proper evaluation serves three critical purposes: <strong>model selection</strong> (choosing between GPT-4, Claude, Gemini), <strong>quality assurance</strong> (ensuring your fine-tuned model works), and <strong>regression detection</strong> (catching when prompt changes degrade performance). Without evaluation, you're flying blind.</p>
                <p>The key insight is that no single metric captures everything. Combine automated benchmarks for efficiency, human evaluation for nuance, and LLM-as-judge for scalable quality assessment. Different use cases emphasize different metrics‚Äîcoding tasks need correctness, creative writing needs style, and QA needs factuality.</p>
            </div>
        </section>

        <!-- BENCHMARK CHART -->
        <section class="module">
            <div class="module-header">
                <div class="module-icon">üìä</div>
                <div class="module-info">
                    <h2>Benchmark Comparison</h2>
                    <p>How leading models perform on key benchmarks</p>
                </div>
            </div>
            <div class="benchmark-chart">
                <div class="benchmark-header">
                    <div class="benchmark-title">üèÜ Model Performance on Major Benchmarks (2024)</div>
                    <div class="benchmark-badge">Normalized Scores</div>
                </div>
                <div class="benchmark-body">
                    <div class="benchmark-row">
                        <div class="benchmark-label">MMLU (Knowledge)</div>
                        <div class="benchmark-bar-wrap">
                            <div class="benchmark-bar gpt4" style="width: 92%;">GPT-4o: 92%</div>
                        </div>
                    </div>
                    <div class="benchmark-row">
                        <div class="benchmark-label"></div>
                        <div class="benchmark-bar-wrap">
                            <div class="benchmark-bar claude" style="width: 90%;">Claude 3.5: 90%</div>
                        </div>
                    </div>
                    <div class="benchmark-row">
                        <div class="benchmark-label">HumanEval (Code)</div>
                        <div class="benchmark-bar-wrap">
                            <div class="benchmark-bar gpt4" style="width: 91%;">GPT-4o: 91%</div>
                        </div>
                    </div>
                    <div class="benchmark-row">
                        <div class="benchmark-label"></div>
                        <div class="benchmark-bar-wrap">
                            <div class="benchmark-bar claude" style="width: 92%;">Claude 3.5: 92%</div>
                        </div>
                    </div>
                    <div class="benchmark-row">
                        <div class="benchmark-label">GSM8K (Math)</div>
                        <div class="benchmark-bar-wrap">
                            <div class="benchmark-bar gpt4" style="width: 95%;">GPT-4o: 95%</div>
                        </div>
                    </div>
                    <div class="benchmark-row">
                        <div class="benchmark-label"></div>
                        <div class="benchmark-bar-wrap">
                            <div class="benchmark-bar gemini" style="width: 94%;">Gemini 1.5: 94%</div>
                        </div>
                    </div>
                    <div class="benchmark-row">
                        <div class="benchmark-label">MT-Bench (Chat)</div>
                        <div class="benchmark-bar-wrap">
                            <div class="benchmark-bar claude" style="width: 94%;">Claude 3.5: 9.4</div>
                        </div>
                    </div>
                    <div class="benchmark-row">
                        <div class="benchmark-label"></div>
                        <div class="benchmark-bar-wrap">
                            <div class="benchmark-bar llama" style="width: 88%;">Llama 3.1 405B: 8.8</div>
                        </div>
                    </div>
                </div>
                <div class="benchmark-legend">
                    <div class="legend-item"><div class="legend-dot" style="background: var(--green);"></div> GPT-4o</div>
                    <div class="legend-item"><div class="legend-dot" style="background: var(--violet);"></div> Claude 3.5</div>
                    <div class="legend-item"><div class="legend-dot" style="background: var(--blue);"></div> Gemini 1.5</div>
                    <div class="legend-item"><div class="legend-dot" style="background: var(--yellow);"></div> Llama 3.1</div>
                </div>
            </div>
        </section>

        <!-- EVALUATION FRAMEWORK -->
        <section class="module">
            <div class="module-header">
                <div class="module-icon">üèóÔ∏è</div>
                <div class="module-info">
                    <h2>Evaluation Framework</h2>
                    <p>Multi-layer approach to comprehensive evaluation</p>
                </div>
            </div>
            <div class="eval-framework">
                <div class="eval-title">üîÑ Three-Layer Evaluation Strategy</div>
                <div class="eval-layers">
                    <div class="eval-layer auto">
                        <div class="eval-layer-num">1</div>
                        <div class="eval-layer-content">
                            <div class="eval-layer-title">Automated Metrics</div>
                            <div class="eval-layer-desc">Fast, cheap, deterministic. Run on every PR/deployment. Good for catching regressions and comparing versions.</div>
                            <div class="eval-layer-examples">
                                <span class="eval-example">BLEU / ROUGE</span>
                                <span class="eval-example">Exact Match</span>
                                <span class="eval-example">Perplexity</span>
                                <span class="eval-example">Code Execution</span>
                            </div>
                        </div>
                    </div>
                    <div class="eval-layer llm">
                        <div class="eval-layer-num">2</div>
                        <div class="eval-layer-content">
                            <div class="eval-layer-title">LLM-as-Judge</div>
                            <div class="eval-layer-desc">Use GPT-4 or Claude to score outputs. Scales better than human eval, captures nuance better than metrics. 70-80% agreement with humans.</div>
                            <div class="eval-layer-examples">
                                <span class="eval-example">Pairwise Comparison</span>
                                <span class="eval-example">Rubric Scoring</span>
                                <span class="eval-example">Criteria Evaluation</span>
                                <span class="eval-example">G-Eval</span>
                            </div>
                        </div>
                    </div>
                    <div class="eval-layer human">
                        <div class="eval-layer-num">3</div>
                        <div class="eval-layer-content">
                            <div class="eval-layer-title">Human Evaluation</div>
                            <div class="eval-layer-desc">Gold standard for quality. Expensive and slow, but captures subtleties that automated methods miss. Use for high-stakes decisions.</div>
                            <div class="eval-layer-examples">
                                <span class="eval-example">Expert Review</span>
                                <span class="eval-example">A/B Testing</span>
                                <span class="eval-example">User Feedback</span>
                                <span class="eval-example">Preference Ranking</span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- METRICS GRID -->
        <section class="module">
            <div class="module-header">
                <div class="module-icon">üìê</div>
                <div class="module-info">
                    <h2>Key Evaluation Metrics</h2>
                    <p>Common metrics for different task types</p>
                </div>
            </div>
            <div class="metrics-grid">
                <div class="metric-card">
                    <div class="metric-icon">üéØ</div>
                    <div class="metric-name">Exact Match</div>
                    <div class="metric-desc">Does the output exactly match the expected answer? Best for factual QA with definitive answers.</div>
                    <div class="metric-formula">EM = correct / total</div>
                </div>
                <div class="metric-card">
                    <div class="metric-icon">üìù</div>
                    <div class="metric-name">BLEU Score</div>
                    <div class="metric-desc">N-gram overlap between generated and reference text. Standard for translation and summarization.</div>
                    <div class="metric-formula">BLEU = BP √ó exp(Œ£logp‚Çô)</div>
                </div>
                <div class="metric-card">
                    <div class="metric-icon">üî¥</div>
                    <div class="metric-name">ROUGE</div>
                    <div class="metric-desc">Recall-oriented overlap. ROUGE-L measures longest common subsequence. Good for summarization.</div>
                    <div class="metric-formula">ROUGE-L = LCS / ref_len</div>
                </div>
                <div class="metric-card">
                    <div class="metric-icon">‚úÖ</div>
                    <div class="metric-name">Pass@k</div>
                    <div class="metric-desc">Probability that at least one of k code samples passes all tests. Standard for code generation.</div>
                    <div class="metric-formula">Pass@k = 1 - C(n-c,k)/C(n,k)</div>
                </div>
            </div>
        </section>

        <!-- BENCHMARK TABLE -->
        <section class="module">
            <div class="module-header">
                <div class="module-icon">üìã</div>
                <div class="module-info">
                    <h2>Popular Benchmarks</h2>
                    <p>Standard benchmarks for LLM evaluation</p>
                </div>
            </div>
            <div class="comparison-table">
                <div class="comparison-header">
                    <div class="comparison-title">üìä LLM Evaluation Benchmarks</div>
                </div>
                <table>
                    <thead>
                        <tr>
                            <th>Benchmark</th>
                            <th>What It Measures</th>
                            <th>Tasks</th>
                            <th>Size</th>
                            <th>Difficulty</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>MMLU</strong></td>
                            <td>Broad knowledge across 57 subjects</td>
                            <td>Multiple choice QA</td>
                            <td>14K questions</td>
                            <td><div class="score-bar"><div class="score-fill high" style="width: 70%;"></div></div></td>
                        </tr>
                        <tr>
                            <td><strong>HumanEval</strong></td>
                            <td>Code generation accuracy</td>
                            <td>Function completion</td>
                            <td>164 problems</td>
                            <td><div class="score-bar"><div class="score-fill mid" style="width: 60%;"></div></div></td>
                        </tr>
                        <tr>
                            <td><strong>GSM8K</strong></td>
                            <td>Grade school math reasoning</td>
                            <td>Word problems</td>
                            <td>8.5K problems</td>
                            <td><div class="score-bar"><div class="score-fill mid" style="width: 50%;"></div></div></td>
                        </tr>
                        <tr>
                            <td><strong>MATH</strong></td>
                            <td>Competition-level math</td>
                            <td>Proof-style problems</td>
                            <td>12.5K problems</td>
                            <td><div class="score-bar"><div class="score-fill high" style="width: 90%;"></div></div></td>
                        </tr>
                        <tr>
                            <td><strong>MT-Bench</strong></td>
                            <td>Multi-turn conversation quality</td>
                            <td>Dialogue scoring</td>
                            <td>80 questions</td>
                            <td><div class="score-bar"><div class="score-fill mid" style="width: 55%;"></div></div></td>
                        </tr>
                        <tr>
                            <td><strong>TruthfulQA</strong></td>
                            <td>Factual accuracy, avoiding hallucination</td>
                            <td>QA with traps</td>
                            <td>817 questions</td>
                            <td><div class="score-bar"><div class="score-fill high" style="width: 75%;"></div></div></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <!-- EVALUATION TOOLS -->
        <section class="module">
            <div class="module-header">
                <div class="module-icon">üõ†Ô∏è</div>
                <div class="module-info">
                    <h2>Evaluation Tools</h2>
                    <p>Platforms and frameworks for LLM evaluation</p>
                </div>
            </div>
            <div class="tools-grid">
                <div class="tool-card">
                    <div class="tool-header">
                        <div class="tool-icon">üìê</div>
                        <div>
                            <div class="tool-name">LangSmith</div>
                            <div class="tool-type">LangChain ‚Ä¢ Managed</div>
                        </div>
                    </div>
                    <div class="tool-desc">End-to-end LLM observability and evaluation platform. Track prompts, runs, and quality metrics in production.</div>
                    <ul class="tool-features">
                        <li>Trace visualization</li>
                        <li>Dataset management</li>
                        <li>Custom evaluators</li>
                        <li>A/B testing support</li>
                    </ul>
                </div>
                <div class="tool-card">
                    <div class="tool-header">
                        <div class="tool-icon">üî¨</div>
                        <div>
                            <div class="tool-name">Weights & Biases</div>
                            <div class="tool-type">W&B ‚Ä¢ MLOps</div>
                        </div>
                    </div>
                    <div class="tool-desc">ML experiment tracking with LLM-specific features. Great for fine-tuning experiments and model comparison.</div>
                    <ul class="tool-features">
                        <li>Experiment tracking</li>
                        <li>Prompt versioning</li>
                        <li>Model registry</li>
                        <li>Team collaboration</li>
                    </ul>
                </div>
                <div class="tool-card">
                    <div class="tool-header">
                        <div class="tool-icon">üéØ</div>
                        <div>
                            <div class="tool-name">DeepEval</div>
                            <div class="tool-type">Open Source</div>
                        </div>
                    </div>
                    <div class="tool-desc">Open-source LLM evaluation framework. Pytest-like interface with 14+ built-in metrics including LLM-as-judge.</div>
                    <ul class="tool-features">
                        <li>14+ built-in metrics</li>
                        <li>LLM-as-judge support</li>
                        <li>CI/CD integration</li>
                        <li>RAG evaluation</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- MODEL COMPARISON RADAR -->
        <section class="module">
            <div class="module-header">
                <div class="module-icon">üéØ</div>
                <div class="module-info">
                    <h2>Multi-Dimensional Evaluation</h2>
                    <p>Evaluate models across multiple criteria</p>
                </div>
            </div>
            <div class="radar-chart">
                <div class="radar-title">GPT-4o Capability Profile</div>
                <div class="radar-visual">
                    <div class="radar-hex">
                        <div class="radar-area"></div>
                        <div class="radar-label" style="top: 5%; left: 50%;">Reasoning</div>
                        <div class="radar-label" style="top: 30%; left: 95%;">Coding</div>
                        <div class="radar-label" style="top: 75%; left: 90%;">Math</div>
                        <div class="radar-label" style="top: 95%; left: 50%;">Knowledge</div>
                        <div class="radar-label" style="top: 75%; left: 10%;">Creativity</div>
                        <div class="radar-label" style="top: 30%; left: 5%;">Safety</div>
                    </div>
                </div>
                <div class="radar-scores">
                    <div class="radar-score">
                        <div class="radar-score-value">95</div>
                        <div class="radar-score-label">Reasoning</div>
                    </div>
                    <div class="radar-score">
                        <div class="radar-score-value">91</div>
                        <div class="radar-score-label">Coding</div>
                    </div>
                    <div class="radar-score">
                        <div class="radar-score-value">95</div>
                        <div class="radar-score-label">Math</div>
                    </div>
                    <div class="radar-score">
                        <div class="radar-score-value">92</div>
                        <div class="radar-score-label">Knowledge</div>
                    </div>
                    <div class="radar-score">
                        <div class="radar-score-value">88</div>
                        <div class="radar-score-label">Creativity</div>
                    </div>
                    <div class="radar-score">
                        <div class="radar-score-value">90</div>
                        <div class="radar-score-label">Safety</div>
                    </div>
                </div>
            </div>
        </section>

        <!-- AGENT THIS -->
        <section class="module" id="agent">
            <div class="module-header">
                <div class="module-icon">ü§ñ</div>
                <div class="module-info">
                    <h2>Agent This</h2>
                    <p>AI-powered evaluation specialist</p>
                </div>
            </div>
            
            <div class="agent-grid">
                <div class="agent-info">
                    <div class="agent-avatar">üìà</div>
                    <div class="agent-name">EvalExpertAgent</div>
                    <div class="agent-role">LLM Evaluation Specialist</div>
                    <div class="agent-desc">Expert in LLM benchmarking, evaluation frameworks, and quality metrics. Helps design evaluation strategies, select appropriate benchmarks, implement LLM-as-judge systems, and interpret results for decision-making.</div>
                    <ul class="agent-capabilities">
                        <li>Design evaluation frameworks</li>
                        <li>Select appropriate benchmarks</li>
                        <li>Implement LLM-as-judge</li>
                        <li>Build custom evaluation datasets</li>
                        <li>Analyze and interpret results</li>
                        <li>Set up regression testing</li>
                        <li>Compare model performance</li>
                    </ul>
                </div>
                
                <div class="code-panel">
                    <div class="code-header">
                        <div class="code-tab active">Agent Definition</div>
                        <div class="code-tab">Evaluation Task</div>
                        <div class="code-filename">eval_expert_agent.py</div>
                    </div>
                    <div class="code-content">
                        <pre><span class="code-comment"># eval_expert_agent.py - Evaluation Expert Agent</span>
<span class="code-keyword">from</span> crewai <span class="code-keyword">import</span> Agent, Task, Crew

<span class="code-function">eval_agent</span> = Agent(
    role=<span class="code-string">"LLM Evaluation Expert"</span>,
    goal=<span class="code-string">"Ensure rigorous model quality assessment"</span>,
    backstory=<span class="code-string">"""Expert in LLM evaluation with deep 
    knowledge of benchmarks (MMLU, HumanEval, GSM8K),
    metrics (BLEU, ROUGE, Pass@k), and evaluation
    frameworks. Has designed evaluation systems for
    production LLM applications. Expert in LLM-as-judge
    and human evaluation methodologies."""</span>,
    tools=[
        BenchmarkRunner(),
        MetricsCalculator(),
        LLMJudge(),
        DatasetBuilder(),
        ResultsAnalyzer(),
    ]
)

<span class="code-function">eval_task</span> = Task(
    description=<span class="code-string">"""
    1. Understand evaluation requirements
    2. Select appropriate benchmarks and metrics
    3. Build or curate evaluation dataset
    4. Run automated evaluations
    5. Implement LLM-as-judge for nuanced tasks
    6. Analyze results and identify patterns
    7. Generate actionable recommendations
    """</span>,
    agent=eval_agent,
    expected_output=<span class="code-string">"Comprehensive evaluation report"</span>
)

<span class="code-comment"># Execute evaluation</span>
crew = Crew(agents=[eval_agent], tasks=[eval_task])
result = crew.kickoff()</pre>
                    </div>
                </div>
            </div>
        </section>

        <!-- RELATED PAGES -->
        <section class="module" id="related">
            <div class="module-header">
                <div class="module-icon">üîó</div>
                <div class="module-info">
                    <h2>Related Pages</h2>
                    <p>Continue learning</p>
                </div>
            </div>
            
            <div class="related-pages">
                <a href="cat05-p1-llm-fundamentals.html" class="related-card">
                    <div class="related-num">Page 5.1</div>
                    <div class="related-title">LLM Fundamentals</div>
                    <div class="related-desc">Understand what you're evaluating</div>
                </a>
                <a href="cat05-p4-fine-tuning.html" class="related-card">
                    <div class="related-num">Page 5.4</div>
                    <div class="related-title">Fine-Tuning</div>
                    <div class="related-desc">Evaluate your fine-tuned models</div>
                </a>
                <a href="cat05-p8-responsible-ai.html" class="related-card">
                    <div class="related-num">Page 5.8</div>
                    <div class="related-title">Responsible AI</div>
                    <div class="related-desc">Safety and bias evaluation</div>
                </a>
            </div>
        </section>

    </div>
</div>

<footer>
    <div class="footer-nav">
        <a href="cat05-p5-embeddings.html" class="footer-link">
            <span>‚Üê</span>
            <div>
                <div class="footer-link-label">Previous Page</div>
                <div class="footer-link-title">5.5: Embeddings & Vectors</div>
            </div>
        </a>
        <div class="footer-brand"><strong>STRATEGY</strong>HUB ‚Ä¢ Page 5.6 of 10</div>
        <a href="cat05-p7-multimodal.html" class="footer-link">
            <div>
                <div class="footer-link-label">Next Page</div>
                <div class="footer-link-title">5.7: Multimodal AI</div>
            </div>
            <span>‚Üí</span>
        </a>
    </div>
</footer>

</body>
</html>
