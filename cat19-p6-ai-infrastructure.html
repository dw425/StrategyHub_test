<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Infrastructure | Emerging Technologies | Strategy Hub</title>
    <style>
        :root {
            --brand-orange: #FF9900;
            --page-primary: #10B981;
            --page-light: #34D399;
            --page-glow: rgba(16, 185, 129, 0.15);
            --bg-dark: #0a0a0f;
            --bg-card: #12121a;
            --bg-card-alt: #1a1a2e;
            --bg-hover: #252538;
            --border-color: #2a2a3e;
            --text-primary: #ffffff;
            --text-secondary: #a0a0b0;
            --text-muted: #6a6a7a;
            --green: #10B981;
            --yellow: #F59E0B;
            --red: #EF4444;
            --blue: #3B82F6;
            --cyan: #06B6D4;
            --pink: #EC4899;
            --purple: #8B5CF6;
            --orange: #F97316;
            --sidebar-width: 280px;
            --header-height: 60px;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background: var(--bg-dark); color: var(--text-primary); line-height: 1.6; }
        header { background: var(--bg-card); border-bottom: 1px solid var(--border-color); padding: 0 32px; position: fixed; top: 0; left: 0; right: 0; z-index: 1000; height: var(--header-height); display: flex; justify-content: space-between; align-items: center; }
        .logo { font-size: 20px; font-weight: 700; text-decoration: none; color: inherit; }
        .logo span { color: var(--brand-orange); }
        .header-tagline { font-size: 13px; color: var(--text-muted); }
        .sidebar { position: fixed; top: var(--header-height); left: 0; width: var(--sidebar-width); height: calc(100vh - var(--header-height)); background: var(--bg-card); border-right: 1px solid var(--border-color); overflow-y: auto; z-index: 100; padding: 24px 0; }
        .sidebar-section { margin-bottom: 28px; }
        .sidebar-title { font-size: 11px; text-transform: uppercase; letter-spacing: 1.5px; color: var(--page-light); padding: 0 24px; margin-bottom: 12px; font-weight: 600; }
        .sidebar-nav { list-style: none; }
        .sidebar-nav a { display: flex; align-items: center; gap: 10px; padding: 10px 24px; color: var(--text-secondary); text-decoration: none; font-size: 13px; border-left: 3px solid transparent; transition: all 0.2s; }
        .sidebar-nav a:hover { background: var(--bg-hover); color: var(--text-primary); }
        .sidebar-nav a.active { background: var(--page-glow); color: var(--page-light); border-left-color: var(--page-light); }
        .nav-icon { width: 24px; text-align: center; }
        .main-wrapper { margin-left: var(--sidebar-width); margin-top: var(--header-height); }
        .main-content { max-width: 1200px; padding: 32px; }

        /* Hero */
        .hero-compact { display: grid; grid-template-columns: 1fr 1fr; gap: 32px; margin-bottom: 32px; align-items: center; }
        .hero-tag { display: inline-flex; align-items: center; gap: 8px; background: var(--page-glow); border: 1px solid var(--page-primary); padding: 6px 14px; border-radius: 9999px; font-size: 12px; color: var(--page-light); margin-bottom: 16px; }
        .hero-left h1 { font-size: 32px; font-weight: 700; margin-bottom: 12px; }
        .hero-left > p { font-size: 15px; color: var(--text-secondary); line-height: 1.7; }
        .hero-metrics { display: grid; grid-template-columns: repeat(2, 1fr); gap: 12px; }
        .hero-metric { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; text-align: center; }
        .hero-metric-value { font-size: 28px; font-weight: 700; color: var(--page-light); }
        .hero-metric-label { font-size: 11px; color: var(--text-muted); margin-top: 4px; }

        /* Modules */
        .module { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 16px; padding: 28px; margin-bottom: 24px; }
        .module-header { display: flex; align-items: flex-start; gap: 16px; margin-bottom: 24px; }
        .module-icon { width: 48px; height: 48px; background: var(--page-glow); border: 2px solid var(--page-primary); border-radius: 12px; display: flex; align-items: center; justify-content: center; font-size: 22px; flex-shrink: 0; }
        .module-info h2 { font-size: 20px; font-weight: 600; margin-bottom: 4px; }
        .module-info p { font-size: 13px; color: var(--text-secondary); }

        /* Overview Content */
        .overview-content { display: grid; grid-template-columns: 1fr 1fr; gap: 24px; }
        .overview-content h3 { font-size: 15px; font-weight: 600; margin-bottom: 12px; color: var(--page-light); }
        .overview-content p { font-size: 14px; color: var(--text-secondary); line-height: 1.7; margin-bottom: 12px; }
        .overview-content ul { list-style: none; margin-bottom: 12px; }
        .overview-content li { font-size: 14px; color: var(--text-secondary); padding: 4px 0; padding-left: 20px; position: relative; }
        .overview-content li::before { content: '‚Ä¢'; position: absolute; left: 0; color: var(--page-light); }
        .overview-content strong { color: var(--text-primary); }
        .overview-full { grid-column: 1 / -1; }

        /* Comparison Table */
        .comparison-table { width: 100%; border-collapse: collapse; margin-bottom: 24px; }
        .comparison-table th, .comparison-table td { padding: 14px 16px; text-align: left; border-bottom: 1px solid var(--border-color); font-size: 13px; }
        .comparison-table th { background: var(--bg-dark); color: var(--text-primary); font-weight: 600; }
        .comparison-table td { color: var(--text-secondary); }
        .comparison-table tr:hover td { background: var(--bg-hover); }
        .table-highlight { color: var(--page-light) !important; font-weight: 600; }

        /* Stats Row */
        .stats-row { display: grid; grid-template-columns: repeat(4, 1fr); gap: 16px; margin-bottom: 24px; }
        .stat-card { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; text-align: center; }
        .stat-value { font-size: 28px; font-weight: 700; color: var(--page-light); }
        .stat-label { font-size: 12px; color: var(--text-muted); margin-top: 4px; }

        /* Provider Cards */
        .provider-cards { display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; margin-bottom: 24px; }
        .provider-card { background: var(--bg-dark); border: 2px solid var(--border-color); border-radius: 12px; overflow: hidden; transition: all 0.3s; }
        .provider-card:hover { transform: translateY(-4px); }
        .provider-card.nvidia { border-color: rgba(118, 185, 0, 0.4); }
        .provider-card.nvidia:hover { border-color: #76B900; }
        .provider-card.aws { border-color: rgba(255, 153, 0, 0.4); }
        .provider-card.aws:hover { border-color: #FF9900; }
        .provider-card.azure { border-color: rgba(0, 120, 212, 0.4); }
        .provider-card.azure:hover { border-color: #0078D4; }
        .provider-card.gcp { border-color: rgba(66, 133, 244, 0.4); }
        .provider-card.gcp:hover { border-color: #4285F4; }
        .provider-card.coreweave { border-color: rgba(236, 72, 153, 0.4); }
        .provider-card.coreweave:hover { border-color: #EC4899; }
        .provider-card.lambda { border-color: rgba(139, 92, 246, 0.4); }
        .provider-card.lambda:hover { border-color: #8B5CF6; }
        .provider-header { padding: 20px; display: flex; align-items: center; gap: 16px; }
        .provider-logo { font-size: 32px; }
        .provider-name { font-size: 16px; font-weight: 700; }
        .provider-sub { font-size: 12px; color: var(--text-muted); }
        .provider-body { padding: 0 20px 20px; }
        .provider-features { list-style: none; }
        .provider-features li { font-size: 13px; color: var(--text-secondary); padding: 8px 0; border-bottom: 1px solid var(--border-color); display: flex; align-items: center; gap: 8px; }
        .provider-features li:last-child { border-bottom: none; }
        .provider-check { color: var(--green); }

        /* Deep Dive */
        .deep-dive { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; margin-bottom: 24px; overflow: hidden; }
        .deep-header { padding: 20px 24px; border-bottom: 1px solid var(--border-color); display: flex; align-items: center; gap: 16px; }
        .deep-icon { font-size: 28px; }
        .deep-title { font-size: 18px; font-weight: 600; }
        .deep-subtitle { font-size: 12px; color: var(--text-muted); }
        .deep-body { padding: 24px; }
        .deep-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 24px; }
        .deep-section h4 { font-size: 14px; font-weight: 600; margin-bottom: 12px; color: var(--page-light); }
        .deep-section ul { list-style: none; }
        .deep-section li { font-size: 13px; color: var(--text-secondary); padding: 6px 0; padding-left: 20px; position: relative; }
        .deep-section li::before { content: '‚Üí'; position: absolute; left: 0; color: var(--page-light); }

        /* SW Grid */
        .sw-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 16px; margin-bottom: 24px; }
        .sw-card { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; }
        .sw-title { font-size: 14px; font-weight: 600; margin-bottom: 12px; display: flex; align-items: center; gap: 8px; }
        .sw-title.green { color: var(--green); }
        .sw-title.yellow { color: var(--yellow); }
        .sw-list { list-style: none; }
        .sw-list li { font-size: 13px; color: var(--text-secondary); padding: 6px 0; padding-left: 16px; position: relative; }
        .sw-card.strengths .sw-list li::before { content: '‚úì'; position: absolute; left: 0; color: var(--green); }
        .sw-card.weaknesses .sw-list li::before { content: '‚ö†'; position: absolute; left: 0; color: var(--yellow); font-size: 11px; }

        /* GPU Cards */
        .gpu-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; margin-bottom: 24px; }
        .gpu-card { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; transition: all 0.3s; }
        .gpu-card:hover { border-color: var(--page-primary); }
        .gpu-header { display: flex; justify-content: space-between; align-items: flex-start; margin-bottom: 12px; }
        .gpu-name { font-size: 15px; font-weight: 600; }
        .gpu-gen { font-size: 11px; color: var(--text-muted); }
        .gpu-badge { padding: 4px 10px; background: var(--page-glow); border: 1px solid var(--page-primary); border-radius: 9999px; font-size: 11px; color: var(--page-light); font-weight: 600; }
        .gpu-desc { font-size: 12px; color: var(--text-secondary); margin-bottom: 12px; line-height: 1.5; }
        .gpu-specs { display: grid; grid-template-columns: 1fr 1fr; gap: 8px; font-size: 11px; }
        .gpu-spec-value { color: var(--text-primary); font-weight: 600; }
        .gpu-spec-label { color: var(--text-muted); }

        /* Platform Cards */
        .platform-grid { display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px; margin-bottom: 24px; }
        .platform-card { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; display: flex; gap: 16px; transition: all 0.3s; }
        .platform-card:hover { border-color: var(--page-primary); }
        .platform-icon { width: 48px; height: 48px; background: var(--page-glow); border: 1px solid var(--page-primary); border-radius: 12px; display: flex; align-items: center; justify-content: center; font-size: 24px; flex-shrink: 0; }
        .platform-title { font-size: 15px; font-weight: 600; margin-bottom: 6px; }
        .platform-desc { font-size: 13px; color: var(--text-secondary); line-height: 1.5; margin-bottom: 8px; }
        .platform-tools { font-size: 11px; color: var(--text-muted); }

        /* Cost Cards */
        .cost-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; margin-bottom: 24px; }
        .cost-card { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; text-align: center; }
        .cost-icon { font-size: 32px; margin-bottom: 12px; }
        .cost-title { font-size: 14px; font-weight: 600; margin-bottom: 8px; }
        .cost-desc { font-size: 12px; color: var(--text-secondary); line-height: 1.5; }
        .cost-savings { font-size: 20px; font-weight: 700; color: var(--green); margin-top: 12px; }

        /* Best Practices */
        .practices-grid { display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px; margin-bottom: 24px; }
        .practice-card { display: flex; gap: 16px; padding: 20px; background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; }
        .practice-num { width: 32px; height: 32px; background: var(--page-glow); border: 1px solid var(--page-primary); border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 14px; font-weight: 700; color: var(--page-light); flex-shrink: 0; }
        .practice-title { font-size: 14px; font-weight: 600; margin-bottom: 4px; }
        .practice-desc { font-size: 12px; color: var(--text-secondary); line-height: 1.5; }

        /* Architecture Layers */
        .arch-stack { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; padding: 24px; margin-bottom: 24px; }
        .arch-layer { display: flex; align-items: center; gap: 16px; padding: 16px; background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 8px; margin-bottom: 8px; }
        .arch-layer:last-child { margin-bottom: 0; }
        .arch-layer-num { width: 28px; height: 28px; background: var(--page-glow); border: 1px solid var(--page-primary); border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 12px; font-weight: 700; color: var(--page-light); flex-shrink: 0; }
        .arch-layer-name { font-size: 14px; font-weight: 600; min-width: 140px; }
        .arch-layer-desc { font-size: 13px; color: var(--text-secondary); flex: 1; }
        .arch-layer-tools { font-size: 11px; color: var(--text-muted); }

        /* Agent Grid */
        .agent-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 24px; }
        .agent-info h3 { font-size: 18px; font-weight: 600; margin-bottom: 12px; color: var(--page-light); }
        .agent-info > p { font-size: 14px; color: var(--text-secondary); line-height: 1.7; margin-bottom: 20px; }
        .agent-capabilities { display: flex; flex-direction: column; gap: 10px; }
        .agent-capability { display: flex; align-items: center; gap: 12px; padding: 12px 16px; background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 8px; font-size: 13px; color: var(--text-secondary); }
        .capability-icon { font-size: 18px; }
        .agent-code { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; overflow: hidden; }
        .code-header { background: var(--bg-card-alt); padding: 12px 16px; font-size: 12px; font-weight: 600; border-bottom: 1px solid var(--border-color); font-family: monospace; color: var(--text-muted); }
        .code-content { padding: 20px; font-family: 'Monaco', 'Menlo', monospace; font-size: 12px; line-height: 1.6; overflow-x: auto; color: var(--text-secondary); }
        .ck { color: #FF7B72; }
        .cs { color: #A5D6FF; }
        .cf { color: #79C0FF; }

        /* Related */
        .related-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; }
        .related-card { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; text-decoration: none; color: inherit; transition: all 0.2s; }
        .related-card:hover { border-color: var(--page-primary); transform: translateY(-2px); }
        .related-icon { font-size: 24px; margin-bottom: 12px; }
        .related-title { font-size: 14px; font-weight: 600; margin-bottom: 6px; }
        .related-desc { font-size: 12px; color: var(--text-secondary); }

        footer { background: var(--bg-card); border-top: 1px solid var(--border-color); padding: 24px 32px; margin-left: var(--sidebar-width); }
        .footer-nav { display: flex; justify-content: space-between; align-items: center; }
        .footer-link { display: flex; align-items: center; gap: 12px; text-decoration: none; color: var(--text-secondary); padding: 12px 20px; background: var(--bg-card-alt); border: 1px solid var(--border-color); border-radius: 12px; transition: all 0.2s; }
        .footer-link:hover { border-color: var(--page-primary); color: var(--text-primary); }
        .footer-link-label { font-size: 11px; color: var(--text-muted); }
        .footer-link-title { font-size: 13px; font-weight: 600; }
        .footer-brand { font-size: 13px; color: var(--text-muted); }
        .footer-brand span { color: var(--brand-orange); }

        @media (max-width: 1024px) {
            .sidebar { display: none; }
            .main-wrapper { margin-left: 0; }
            footer { margin-left: 0; }
            .hero-compact, .overview-content, .sw-grid, .deep-grid, .agent-grid { grid-template-columns: 1fr; }
            .provider-cards, .gpu-grid, .platform-grid, .cost-grid, .practices-grid, .related-grid { grid-template-columns: 1fr; }
            .stats-row { grid-template-columns: repeat(2, 1fr); }
        }
    </style>
</head>
<body>

<header>
    <a href="index.html" class="logo"><span>STRATEGY</span>HUB</a>
    <div class="header-tagline">Enterprise Technology Knowledge Base</div>
</header>

<aside class="sidebar">
    <div class="sidebar-section">
        <div class="sidebar-title">üöÄ Category 19</div>
        <ul class="sidebar-nav">
            <li><a href="cat19-emerging-technologies-overview.html"><span class="nav-icon">üè†</span> Overview</a></li>
            <li><a href="cat19-p1-foundation-models.html"><span class="nav-icon">üß†</span> 19.1 Foundation Models</a></li>
            <li><a href="cat19-p2-agentic-ai.html"><span class="nav-icon">ü§ñ</span> 19.2 Agentic AI</a></li>
            <li><a href="cat19-p3-multimodal-ai.html"><span class="nav-icon">üé®</span> 19.3 Multimodal AI</a></li>
            <li><a href="cat19-p4-ai-coding.html"><span class="nav-icon">üíª</span> 19.4 AI Coding Tools</a></li>
            <li><a href="cat19-p5-edge-ai.html"><span class="nav-icon">üì±</span> 19.5 Edge & On-Device</a></li>
            <li><a href="cat19-p6-ai-infrastructure.html" class="active"><span class="nav-icon">üñ•Ô∏è</span> 19.6 AI Infrastructure</a></li>
            <li><a href="cat19-p7-ai-safety.html"><span class="nav-icon">üõ°Ô∏è</span> 19.7 AI Safety & Governance</a></li>
            <li><a href="cat19-p8-rag-knowledge.html"><span class="nav-icon">üìö</span> 19.8 RAG & Knowledge</a></li>
            <li><a href="cat19-p9-observability.html"><span class="nav-icon">üìä</span> 19.9 Observability & Evals</a></li>
            <li><a href="cat19-p10-enterprise-adoption.html"><span class="nav-icon">üè¢</span> 19.10 Enterprise Adoption</a></li>
        </ul>
    </div>
    <div class="sidebar-section">
        <div class="sidebar-title">üìë On This Page</div>
        <ul class="sidebar-nav">
            <li><a href="#overview"><span class="nav-icon">üí°</span> Overview</a></li>
            <li><a href="#gpus"><span class="nav-icon">üéÆ</span> GPU Hardware</a></li>
            <li><a href="#providers"><span class="nav-icon">‚òÅÔ∏è</span> Cloud GPU Providers</a></li>
            <li><a href="#serving"><span class="nav-icon">üöÄ</span> Model Serving</a></li>
            <li><a href="#training"><span class="nav-icon">üèãÔ∏è</span> Distributed Training</a></li>
            <li><a href="#mlops"><span class="nav-icon">üîÑ</span> MLOps Platforms</a></li>
            <li><a href="#costs"><span class="nav-icon">üí∞</span> Cost Optimization</a></li>
            <li><a href="#architecture"><span class="nav-icon">üèóÔ∏è</span> Reference Architecture</a></li>
            <li><a href="#practices"><span class="nav-icon">‚úÖ</span> Best Practices</a></li>
            <li><a href="#agent"><span class="nav-icon">ü§ñ</span> Agent This</a></li>
        </ul>
    </div>
</aside>

<div class="main-wrapper">
    <div class="main-content">

        <!-- Hero -->
        <section class="hero-compact">
            <div class="hero-left">
                <span class="hero-tag">üñ•Ô∏è Page 19.6</span>
                <h1>AI Infrastructure</h1>
                <p>The compute, storage, and orchestration systems that power AI workloads. From GPU clusters and model serving to distributed training and MLOps‚Äîeverything needed to run AI at scale in production.</p>
            </div>
            <div class="hero-metrics">
                <div class="hero-metric"><div class="hero-metric-value">$2-4/hr</div><div class="hero-metric-label">H100 GPU Cloud Cost</div></div>
                <div class="hero-metric"><div class="hero-metric-value">10,000+</div><div class="hero-metric-label">GPUs for Frontier Training</div></div>
                <div class="hero-metric"><div class="hero-metric-value">~$100M</div><div class="hero-metric-label">Cost to Train GPT-4 Class</div></div>
                <div class="hero-metric"><div class="hero-metric-value">70%</div><div class="hero-metric-label">Inference vs Training Spend</div></div>
            </div>
        </section>

        <!-- Overview -->
        <section class="module" id="overview">
            <div class="module-header">
                <div class="module-icon">üí°</div>
                <div class="module-info">
                    <h2>Overview</h2>
                    <p>Understanding AI infrastructure landscape</p>
                </div>
            </div>

            <div class="overview-content">
                <div>
                    <h3>What is AI Infrastructure?</h3>
                    <p><strong>AI Infrastructure</strong> encompasses all the hardware, software, and services required to develop, train, and deploy machine learning models. This includes GPU clusters for compute, high-bandwidth networking for distributed training, storage systems for massive datasets, and orchestration platforms for managing the full ML lifecycle.</p>
                    <p>Unlike traditional computing, AI workloads are extremely compute-intensive and memory-bound. Training large models requires coordinating thousands of GPUs, while inference demands low-latency response times and efficient resource utilization.</p>
                </div>
                <div>
                    <h3>The AI Compute Crisis</h3>
                    <p>Demand for AI compute is outpacing supply:</p>
                    <ul>
                        <li><strong>Training compute</strong> doubles every 6-10 months for frontier models</li>
                        <li><strong>GPU availability</strong> remains constrained‚ÄîH100s are allocated 12+ months out</li>
                        <li><strong>Cost explosion</strong>‚Äîlarge training runs now cost $100M+</li>
                        <li><strong>Energy constraints</strong>‚Äîdata centers can't build power capacity fast enough</li>
                    </ul>
                    <p>This scarcity has spawned a new ecosystem of GPU clouds, inference optimization, and efficient training techniques.</p>
                </div>
            </div>

            <div class="sw-grid">
                <div class="sw-card strengths">
                    <div class="sw-title green">‚úì Training vs Inference</div>
                    <ul class="sw-list">
                        <li><strong>Training:</strong> Run once (or periodically), extremely compute-intensive</li>
                        <li><strong>Inference:</strong> Run continuously, latency-sensitive, cost per query</li>
                        <li>70% of AI infrastructure spend goes to inference</li>
                        <li>Training optimizes for throughput; inference for latency</li>
                        <li>Different hardware optimal for each (H100 vs L4/A10)</li>
                        <li>Inference costs compound with scale‚Äîoptimization critical</li>
                    </ul>
                </div>
                <div class="sw-card weaknesses">
                    <div class="sw-title yellow">‚ö† Key Challenges</div>
                    <ul class="sw-list">
                        <li>GPU scarcity and long lead times</li>
                        <li>High capital expenditure for on-prem</li>
                        <li>Complex distributed systems engineering</li>
                        <li>Rapidly evolving hardware generations</li>
                        <li>Power and cooling constraints at scale</li>
                        <li>Talent shortage for ML infrastructure</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- GPU Hardware -->
        <section class="module" id="gpus">
            <div class="module-header">
                <div class="module-icon">üéÆ</div>
                <div class="module-info">
                    <h2>GPU Hardware</h2>
                    <p>The silicon powering AI workloads</p>
                </div>
            </div>

            <div class="overview-content" style="margin-bottom: 24px;">
                <div class="overview-full">
                    <h3>Why GPUs for AI?</h3>
                    <p><strong>GPUs (Graphics Processing Units)</strong> excel at AI workloads because of their massively parallel architecture. While CPUs have 8-64 powerful cores for sequential work, GPUs have thousands of smaller cores optimized for matrix operations‚Äîthe core computation in neural networks. A single H100 GPU delivers ~2 petaFLOPs of AI compute, roughly equivalent to hundreds of high-end CPUs for ML workloads.</p>
                </div>
            </div>

            <div class="gpu-grid">
                <div class="gpu-card">
                    <div class="gpu-header">
                        <div><div class="gpu-name">NVIDIA H100</div><div class="gpu-gen">Hopper Architecture</div></div>
                        <span class="gpu-badge">Flagship</span>
                    </div>
                    <p class="gpu-desc">The gold standard for AI training and inference. Transformer Engine with FP8 support. NVLink 4.0 for multi-GPU scaling. Powers most frontier model training.</p>
                    <div class="gpu-specs">
                        <div><span class="gpu-spec-value">80GB</span> <span class="gpu-spec-label">HBM3</span></div>
                        <div><span class="gpu-spec-value">3.35TB/s</span> <span class="gpu-spec-label">bandwidth</span></div>
                        <div><span class="gpu-spec-value">~2 PFLOPS</span> <span class="gpu-spec-label">FP8</span></div>
                        <div><span class="gpu-spec-value">$25-40K</span> <span class="gpu-spec-label">MSRP</span></div>
                    </div>
                </div>

                <div class="gpu-card">
                    <div class="gpu-header">
                        <div><div class="gpu-name">NVIDIA H200</div><div class="gpu-gen">Hopper Architecture</div></div>
                        <span class="gpu-badge">2024</span>
                    </div>
                    <p class="gpu-desc">H100 successor with HBM3e memory. 76% more memory capacity and 43% more bandwidth. Better for large context inference workloads.</p>
                    <div class="gpu-specs">
                        <div><span class="gpu-spec-value">141GB</span> <span class="gpu-spec-label">HBM3e</span></div>
                        <div><span class="gpu-spec-value">4.8TB/s</span> <span class="gpu-spec-label">bandwidth</span></div>
                        <div><span class="gpu-spec-value">~2 PFLOPS</span> <span class="gpu-spec-label">FP8</span></div>
                        <div><span class="gpu-spec-value">$30-45K</span> <span class="gpu-spec-label">estimated</span></div>
                    </div>
                </div>

                <div class="gpu-card">
                    <div class="gpu-header">
                        <div><div class="gpu-name">NVIDIA B200</div><div class="gpu-gen">Blackwell Architecture</div></div>
                        <span class="gpu-badge">Next Gen</span>
                    </div>
                    <p class="gpu-desc">Next-generation flagship with 2.5x training performance and 5x inference over H100. Second-gen Transformer Engine. Shipping late 2024.</p>
                    <div class="gpu-specs">
                        <div><span class="gpu-spec-value">192GB</span> <span class="gpu-spec-label">HBM3e</span></div>
                        <div><span class="gpu-spec-value">8TB/s</span> <span class="gpu-spec-label">bandwidth</span></div>
                        <div><span class="gpu-spec-value">~5 PFLOPS</span> <span class="gpu-spec-label">FP8</span></div>
                        <div><span class="gpu-spec-value">$40-60K</span> <span class="gpu-spec-label">estimated</span></div>
                    </div>
                </div>

                <div class="gpu-card">
                    <div class="gpu-header">
                        <div><div class="gpu-name">NVIDIA A100</div><div class="gpu-gen">Ampere Architecture</div></div>
                        <span class="gpu-badge">Workhorse</span>
                    </div>
                    <p class="gpu-desc">Previous generation but widely available. Still excellent for inference and medium-scale training. Common in cloud instances.</p>
                    <div class="gpu-specs">
                        <div><span class="gpu-spec-value">40/80GB</span> <span class="gpu-spec-label">HBM2e</span></div>
                        <div><span class="gpu-spec-value">2TB/s</span> <span class="gpu-spec-label">bandwidth</span></div>
                        <div><span class="gpu-spec-value">~312 TFLOPS</span> <span class="gpu-spec-label">TF32</span></div>
                        <div><span class="gpu-spec-value">$10-15K</span> <span class="gpu-spec-label">current</span></div>
                    </div>
                </div>

                <div class="gpu-card">
                    <div class="gpu-header">
                        <div><div class="gpu-name">NVIDIA L4</div><div class="gpu-gen">Ada Lovelace</div></div>
                        <span class="gpu-badge">Inference</span>
                    </div>
                    <p class="gpu-desc">Optimized for inference workloads. Low power, small form factor. Cost-effective for serving at scale. Available in most clouds.</p>
                    <div class="gpu-specs">
                        <div><span class="gpu-spec-value">24GB</span> <span class="gpu-spec-label">GDDR6</span></div>
                        <div><span class="gpu-spec-value">300GB/s</span> <span class="gpu-spec-label">bandwidth</span></div>
                        <div><span class="gpu-spec-value">~121 TFLOPS</span> <span class="gpu-spec-label">INT8</span></div>
                        <div><span class="gpu-spec-value">72W</span> <span class="gpu-spec-label">TDP</span></div>
                    </div>
                </div>

                <div class="gpu-card">
                    <div class="gpu-header">
                        <div><div class="gpu-name">AMD MI300X</div><div class="gpu-gen">CDNA 3</div></div>
                        <span class="gpu-badge">Alternative</span>
                    </div>
                    <p class="gpu-desc">AMD's H100 competitor. More memory at better price point. Growing software ecosystem with ROCm. Used by Microsoft, Meta.</p>
                    <div class="gpu-specs">
                        <div><span class="gpu-spec-value">192GB</span> <span class="gpu-spec-label">HBM3</span></div>
                        <div><span class="gpu-spec-value">5.3TB/s</span> <span class="gpu-spec-label">bandwidth</span></div>
                        <div><span class="gpu-spec-value">~1.3 PFLOPS</span> <span class="gpu-spec-label">FP16</span></div>
                        <div><span class="gpu-spec-value">$15-20K</span> <span class="gpu-spec-label">estimated</span></div>
                    </div>
                </div>
            </div>

            <div class="overview-content">
                <div>
                    <h3>Memory is Often the Bottleneck</h3>
                    <p>Large language models require massive memory. A 70B parameter model at FP16 needs ~140GB just for weights‚Äîmore than a single H100. This drives multi-GPU setups and memory optimization techniques like quantization, KV-cache compression, and tensor parallelism. When evaluating GPUs, <strong>memory capacity and bandwidth often matter more than raw compute</strong> for inference workloads.</p>
                </div>
                <div>
                    <h3>NVIDIA's Dominance</h3>
                    <p>NVIDIA controls 80-90% of the AI accelerator market. Their CUDA ecosystem, mature software stack (cuDNN, TensorRT, Triton), and NVLink interconnect for multi-GPU create strong lock-in. AMD's ROCm and Intel's oneAPI are improving but lag in maturity. Custom silicon (Google TPU, AWS Trainium/Inferentia) offers alternatives but with narrower software support.</p>
                </div>
            </div>
        </section>

        <!-- Cloud GPU Providers -->
        <section class="module" id="providers">
            <div class="module-header">
                <div class="module-icon">‚òÅÔ∏è</div>
                <div class="module-info">
                    <h2>Cloud GPU Providers</h2>
                    <p>Where to rent AI compute</p>
                </div>
            </div>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Provider</th>
                        <th>H100 Pricing</th>
                        <th>A100 Pricing</th>
                        <th>Strengths</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>AWS (EC2)</strong></td>
                        <td>$3.50-4.50/hr</td>
                        <td>$3.00-4.00/hr</td>
                        <td>Ecosystem, SageMaker integration</td>
                        <td>Enterprise, MLOps integration</td>
                    </tr>
                    <tr>
                        <td><strong>Azure</strong></td>
                        <td>$3.40-4.30/hr</td>
                        <td>$3.20-3.80/hr</td>
                        <td>OpenAI partnership, enterprise</td>
                        <td>Microsoft shops, Azure ML</td>
                    </tr>
                    <tr>
                        <td><strong>GCP</strong></td>
                        <td>$3.30-4.20/hr</td>
                        <td>$2.90-3.50/hr</td>
                        <td>TPUs, Vertex AI, networking</td>
                        <td>Research, TPU workloads</td>
                    </tr>
                    <tr>
                        <td><strong>CoreWeave</strong></td>
                        <td class="table-highlight">$2.00-2.80/hr</td>
                        <td>$1.80-2.20/hr</td>
                        <td>GPU-native, availability, price</td>
                        <td>Training at scale, startups</td>
                    </tr>
                    <tr>
                        <td><strong>Lambda Labs</strong></td>
                        <td class="table-highlight">$2.00-2.50/hr</td>
                        <td>$1.50-2.00/hr</td>
                        <td>Developer-friendly, simple</td>
                        <td>Research, prototyping</td>
                    </tr>
                    <tr>
                        <td><strong>RunPod</strong></td>
                        <td>$2.50-3.00/hr</td>
                        <td>$1.50-2.00/hr</td>
                        <td>Spot instances, serverless</td>
                        <td>Hobbyists, variable workloads</td>
                    </tr>
                    <tr>
                        <td><strong>Together AI</strong></td>
                        <td>Per-token pricing</td>
                        <td>Per-token</td>
                        <td>Inference API, fine-tuning</td>
                        <td>Open source model inference</td>
                    </tr>
                </tbody>
            </table>

            <div class="deep-dive">
                <div class="deep-header">
                    <div class="deep-icon">üí°</div>
                    <div>
                        <div class="deep-title">Choosing a GPU Cloud</div>
                        <div class="deep-subtitle">Factors beyond hourly price</div>
                    </div>
                </div>
                <div class="deep-body">
                    <div class="deep-grid">
                        <div class="deep-section">
                            <h4>Hyperscalers (AWS/Azure/GCP)</h4>
                            <ul>
                                <li>Best for enterprise with existing cloud footprint</li>
                                <li>Integrated MLOps (SageMaker, Vertex, Azure ML)</li>
                                <li>Compliance certifications, enterprise support</li>
                                <li>Reserved capacity for production workloads</li>
                                <li>Higher prices but broader ecosystem</li>
                            </ul>
                        </div>
                        <div class="deep-section">
                            <h4>GPU-Native Clouds</h4>
                            <ul>
                                <li>30-50% lower pricing than hyperscalers</li>
                                <li>Better GPU availability (H100s actually in stock)</li>
                                <li>Optimized for ML workloads specifically</li>
                                <li>Simpler pricing, less ecosystem lock-in</li>
                                <li>May lack enterprise compliance/support</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <div class="overview-content">
                <div>
                    <h3>Spot/Preemptible Instances</h3>
                    <p>Most providers offer <strong>spot instances</strong> at 50-80% discount in exchange for potential interruption. Excellent for fault-tolerant training with checkpointing, batch inference, and experiments. Not suitable for latency-sensitive production inference. CoreWeave and RunPod specialize in spot capacity.</p>
                </div>
                <div>
                    <h3>Custom Silicon Alternatives</h3>
                    <ul>
                        <li><strong>Google TPU v5:</strong> Competitive with H100, best with JAX/TensorFlow</li>
                        <li><strong>AWS Trainium/Inferentia:</strong> Cost-effective for specific frameworks</li>
                        <li><strong>Cerebras, Groq, SambaNova:</strong> Specialized for specific workloads</li>
                        <li><strong>Groq LPU:</strong> Ultra-fast inference, limited model support</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- Model Serving -->
        <section class="module" id="serving">
            <div class="module-header">
                <div class="module-icon">üöÄ</div>
                <div class="module-info">
                    <h2>Model Serving</h2>
                    <p>Getting models into production</p>
                </div>
            </div>

            <div class="overview-content" style="margin-bottom: 24px;">
                <div class="overview-full">
                    <h3>The Inference Challenge</h3>
                    <p>Serving LLMs at scale is fundamentally different from serving traditional ML models. A single GPT-4 class model processes ~10,000 tokens per second per GPU, but users expect sub-second response times. <strong>Inference optimization</strong>‚Äîbatching, caching, quantization, and speculative decoding‚Äîis essential to make production serving economically viable.</p>
                </div>
            </div>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Framework</th>
                        <th>Type</th>
                        <th>Key Feature</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>vLLM</strong></td>
                        <td>LLM Serving</td>
                        <td class="table-highlight">PagedAttention, continuous batching</td>
                        <td>High-throughput LLM inference</td>
                    </tr>
                    <tr>
                        <td><strong>TensorRT-LLM</strong></td>
                        <td>LLM Serving</td>
                        <td>NVIDIA optimized, FP8 support</td>
                        <td>Maximum NVIDIA GPU performance</td>
                    </tr>
                    <tr>
                        <td><strong>llama.cpp</strong></td>
                        <td>LLM Serving</td>
                        <td>CPU inference, quantization</td>
                        <td>Edge deployment, local inference</td>
                    </tr>
                    <tr>
                        <td><strong>Triton Inference Server</strong></td>
                        <td>General ML</td>
                        <td>Multi-model, multi-framework</td>
                        <td>Enterprise multi-model serving</td>
                    </tr>
                    <tr>
                        <td><strong>Text Generation Inference</strong></td>
                        <td>LLM Serving</td>
                        <td>Hugging Face native, easy setup</td>
                        <td>HF model deployment</td>
                    </tr>
                    <tr>
                        <td><strong>Ray Serve</strong></td>
                        <td>General ML</td>
                        <td>Distributed, Python-native</td>
                        <td>Complex ML pipelines</td>
                    </tr>
                </tbody>
            </table>

            <div class="deep-dive">
                <div class="deep-header">
                    <div class="deep-icon">‚ö°</div>
                    <div>
                        <div class="deep-title">Inference Optimization Techniques</div>
                        <div class="deep-subtitle">Reduce cost and latency</div>
                    </div>
                </div>
                <div class="deep-body">
                    <div class="deep-grid">
                        <div class="deep-section">
                            <h4>Batching & Scheduling</h4>
                            <ul>
                                <li><strong>Continuous batching:</strong> Add new requests mid-generation</li>
                                <li><strong>Dynamic batching:</strong> Group requests to maximize GPU util</li>
                                <li><strong>PagedAttention:</strong> Virtual memory for KV-cache, 24x throughput</li>
                                <li><strong>Speculative decoding:</strong> Draft with small model, verify with large</li>
                            </ul>
                        </div>
                        <div class="deep-section">
                            <h4>Model Optimization</h4>
                            <ul>
                                <li><strong>Quantization:</strong> INT8/INT4 for 2-4x speedup, minimal quality loss</li>
                                <li><strong>KV-cache compression:</strong> Reduce memory for long contexts</li>
                                <li><strong>Tensor parallelism:</strong> Split model across GPUs</li>
                                <li><strong>Flash Attention:</strong> Memory-efficient attention computation</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Distributed Training -->
        <section class="module" id="training">
            <div class="module-header">
                <div class="module-icon">üèãÔ∏è</div>
                <div class="module-info">
                    <h2>Distributed Training</h2>
                    <p>Training at scale across many GPUs</p>
                </div>
            </div>

            <div class="overview-content" style="margin-bottom: 24px;">
                <div class="overview-full">
                    <h3>Why Distributed Training?</h3>
                    <p>Modern LLMs are too large to fit on a single GPU and would take years to train without parallelization. Training GPT-4 class models requires <strong>thousands of GPUs working together</strong>, coordinated by sophisticated distributed training frameworks. The challenge isn't just compute‚Äîit's efficient communication between GPUs, fault tolerance, and memory management.</p>
                </div>
            </div>

            <div class="deep-dive">
                <div class="deep-header">
                    <div class="deep-icon">üîÄ</div>
                    <div>
                        <div class="deep-title">Parallelism Strategies</div>
                        <div class="deep-subtitle">Different ways to distribute training</div>
                    </div>
                </div>
                <div class="deep-body">
                    <div class="deep-grid">
                        <div class="deep-section">
                            <h4>Data Parallelism</h4>
                            <ul>
                                <li>Same model on each GPU, different data batches</li>
                                <li>Gradients synchronized across GPUs each step</li>
                                <li>Simplest approach, works when model fits in GPU memory</li>
                                <li>Scales to thousands of GPUs with ZeRO optimization</li>
                            </ul>
                        </div>
                        <div class="deep-section">
                            <h4>Tensor Parallelism</h4>
                            <ul>
                                <li>Split individual layers across GPUs</li>
                                <li>High communication overhead, needs fast interconnect</li>
                                <li>Best within a single node (8 GPUs)</li>
                                <li>Essential for models too large for one GPU</li>
                            </ul>
                        </div>
                        <div class="deep-section">
                            <h4>Pipeline Parallelism</h4>
                            <ul>
                                <li>Different layers on different GPUs</li>
                                <li>Lower communication than tensor parallelism</li>
                                <li>Micro-batching to hide pipeline bubbles</li>
                                <li>Good for scaling across nodes</li>
                            </ul>
                        </div>
                        <div class="deep-section">
                            <h4>3D Parallelism (Combined)</h4>
                            <ul>
                                <li>Data + Tensor + Pipeline parallelism together</li>
                                <li>Required for frontier model training</li>
                                <li>Complex to tune and debug</li>
                                <li>Used by Megatron-LM, DeepSpeed, FSDP</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Framework</th>
                        <th>Provider</th>
                        <th>Key Features</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>DeepSpeed</strong></td>
                        <td>Microsoft</td>
                        <td>ZeRO optimizer, easy integration</td>
                        <td>Large model training with PyTorch</td>
                    </tr>
                    <tr>
                        <td><strong>FSDP</strong></td>
                        <td>PyTorch</td>
                        <td>Native PyTorch, simpler API</td>
                        <td>PyTorch-native distributed training</td>
                    </tr>
                    <tr>
                        <td><strong>Megatron-LM</strong></td>
                        <td>NVIDIA</td>
                        <td>Maximum efficiency, complex setup</td>
                        <td>Frontier model training</td>
                    </tr>
                    <tr>
                        <td><strong>Ray Train</strong></td>
                        <td>Anyscale</td>
                        <td>Framework-agnostic, fault tolerant</td>
                        <td>Heterogeneous clusters</td>
                    </tr>
                    <tr>
                        <td><strong>JAX + pjit</strong></td>
                        <td>Google</td>
                        <td>XLA compilation, TPU optimized</td>
                        <td>Google TPU training</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- MLOps Platforms -->
        <section class="module" id="mlops">
            <div class="module-header">
                <div class="module-icon">üîÑ</div>
                <div class="module-info">
                    <h2>MLOps Platforms</h2>
                    <p>Managing the ML lifecycle</p>
                </div>
            </div>

            <div class="overview-content" style="margin-bottom: 24px;">
                <div class="overview-full">
                    <h3>What is MLOps?</h3>
                    <p><strong>MLOps (Machine Learning Operations)</strong> applies DevOps principles to ML systems: version control for models and data, CI/CD for training pipelines, monitoring for model performance, and infrastructure-as-code for reproducibility. The goal is to move from ad-hoc experimentation to reliable, repeatable ML production systems.</p>
                </div>
            </div>

            <div class="platform-grid">
                <div class="platform-card">
                    <div class="platform-icon">üìä</div>
                    <div>
                        <div class="platform-title">Experiment Tracking</div>
                        <div class="platform-desc">Log parameters, metrics, and artifacts from training runs. Compare experiments, reproduce results, and collaborate with team.</div>
                        <div class="platform-tools"><strong>Tools:</strong> MLflow, Weights & Biases, Neptune, Comet</div>
                    </div>
                </div>

                <div class="platform-card">
                    <div class="platform-icon">üîÄ</div>
                    <div>
                        <div class="platform-title">Pipeline Orchestration</div>
                        <div class="platform-desc">Define and schedule multi-step ML workflows. Handle dependencies, retries, and parallelization. Version pipelines as code.</div>
                        <div class="platform-tools"><strong>Tools:</strong> Kubeflow, Airflow, Prefect, Dagster</div>
                    </div>
                </div>

                <div class="platform-card">
                    <div class="platform-icon">üì¶</div>
                    <div>
                        <div class="platform-title">Model Registry</div>
                        <div class="platform-desc">Central repository for trained models. Track lineage, manage versions, handle approvals and deployments.</div>
                        <div class="platform-tools"><strong>Tools:</strong> MLflow Registry, SageMaker Registry, Hugging Face Hub</div>
                    </div>
                </div>

                <div class="platform-card">
                    <div class="platform-icon">üéõÔ∏è</div>
                    <div>
                        <div class="platform-title">Feature Stores</div>
                        <div class="platform-desc">Centralized feature management for training and serving. Ensure consistency between training and production features.</div>
                        <div class="platform-tools"><strong>Tools:</strong> Feast, Tecton, Databricks Feature Store</div>
                    </div>
                </div>

                <div class="platform-card">
                    <div class="platform-icon">üì°</div>
                    <div>
                        <div class="platform-title">Model Monitoring</div>
                        <div class="platform-desc">Track model performance in production. Detect drift, latency issues, and data quality problems. Trigger retraining when needed.</div>
                        <div class="platform-tools"><strong>Tools:</strong> Evidently, WhyLabs, Arize, Fiddler</div>
                    </div>
                </div>

                <div class="platform-card">
                    <div class="platform-icon">üèóÔ∏è</div>
                    <div>
                        <div class="platform-title">End-to-End Platforms</div>
                        <div class="platform-desc">Integrated platforms combining multiple MLOps capabilities. Trade-off: convenience vs flexibility and vendor lock-in.</div>
                        <div class="platform-tools"><strong>Platforms:</strong> SageMaker, Vertex AI, Azure ML, Databricks</div>
                    </div>
                </div>
            </div>

            <div class="overview-content">
                <div>
                    <h3>LLMOps Differences</h3>
                    <p>LLMs introduce new MLOps challenges: <strong>prompt versioning</strong> (prompts are code now), <strong>evaluation</strong> (traditional metrics don't capture quality), <strong>fine-tuning pipelines</strong> (LoRA, RLHF), and <strong>cost tracking</strong> (token-based billing). Tools like LangSmith, Weights & Biases Prompts, and Humanloop address these LLM-specific needs.</p>
                </div>
                <div>
                    <h3>Platform Selection</h3>
                    <p>For teams starting out, <strong>MLflow</strong> + <strong>Weights & Biases</strong> covers 80% of needs and is vendor-agnostic. For enterprise in a specific cloud, <strong>SageMaker/Vertex/Azure ML</strong> offers integration benefits. For LLM applications, consider <strong>LangSmith</strong> or <strong>Helicone</strong> alongside traditional MLOps tools.</p>
                </div>
            </div>
        </section>

        <!-- Cost Optimization -->
        <section class="module" id="costs">
            <div class="module-header">
                <div class="module-icon">üí∞</div>
                <div class="module-info">
                    <h2>Cost Optimization</h2>
                    <p>Making AI economically viable</p>
                </div>
            </div>

            <div class="stats-row">
                <div class="stat-card">
                    <div class="stat-value">70%</div>
                    <div class="stat-label">Potential Savings with Optimization</div>
                </div>
                <div class="stat-card">
                    <div class="stat-value">50-80%</div>
                    <div class="stat-label">Spot Instance Discount</div>
                </div>
                <div class="stat-card">
                    <div class="stat-value">2-4x</div>
                    <div class="stat-label">Throughput with Batching</div>
                </div>
                <div class="stat-card">
                    <div class="stat-value">30-50%</div>
                    <div class="stat-label">GPU Cloud vs Hyperscaler</div>
                </div>
            </div>

            <div class="cost-grid">
                <div class="cost-card">
                    <div class="cost-icon">üìâ</div>
                    <div class="cost-title">Right-Size GPUs</div>
                    <div class="cost-desc">Use L4/A10 for inference instead of H100. Match GPU to workload‚Äîdon't use training GPUs for serving.</div>
                    <div class="cost-savings">~60% savings</div>
                </div>

                <div class="cost-card">
                    <div class="cost-icon">üì¶</div>
                    <div class="cost-title">Quantization</div>
                    <div class="cost-desc">INT8/INT4 models run faster on smaller GPUs. Often 2-4x throughput improvement with minimal quality loss.</div>
                    <div class="cost-savings">~50% savings</div>
                </div>

                <div class="cost-card">
                    <div class="cost-icon">‚ö°</div>
                    <div class="cost-title">Spot Instances</div>
                    <div class="cost-desc">Use preemptible instances for training with checkpointing. Not for production serving, but great for experiments.</div>
                    <div class="cost-savings">~70% savings</div>
                </div>

                <div class="cost-card">
                    <div class="cost-icon">üîÑ</div>
                    <div class="cost-title">Request Batching</div>
                    <div class="cost-desc">Continuous batching with vLLM can increase throughput 2-4x. GPU utilization often under 30% without batching.</div>
                    <div class="cost-savings">~60% savings</div>
                </div>

                <div class="cost-card">
                    <div class="cost-icon">üíæ</div>
                    <div class="cost-title">Caching</div>
                    <div class="cost-desc">Cache common prompts and completions. Semantic caching for similar queries. Reduce redundant computation.</div>
                    <div class="cost-savings">~40% savings</div>
                </div>

                <div class="cost-card">
                    <div class="cost-icon">üéØ</div>
                    <div class="cost-title">Model Routing</div>
                    <div class="cost-desc">Use smaller models for simple queries, route complex ones to larger models. 80% of queries may not need GPT-4.</div>
                    <div class="cost-savings">~50% savings</div>
                </div>
            </div>

            <div class="overview-content">
                <div class="overview-full">
                    <h3>The Economics of AI Inference</h3>
                    <p>A typical enterprise serving 1M requests/day at $0.01/request spends <strong>$3.6M/year on inference</strong>. At that scale, a 50% optimization saves $1.8M annually. This is why inference optimization has become a critical discipline. Key levers: model selection (often a 7B model suffices), batching efficiency, caching, and hardware selection. The difference between naive and optimized serving can be 10x in cost.</p>
                </div>
            </div>
        </section>

        <!-- Reference Architecture -->
        <section class="module" id="architecture">
            <div class="module-header">
                <div class="module-icon">üèóÔ∏è</div>
                <div class="module-info">
                    <h2>Reference Architecture</h2>
                    <p>Enterprise AI infrastructure stack</p>
                </div>
            </div>

            <div class="arch-stack">
                <div class="arch-layer">
                    <div class="arch-layer-num">1</div>
                    <div class="arch-layer-name">Applications</div>
                    <div class="arch-layer-desc">User-facing applications consuming AI capabilities</div>
                    <div class="arch-layer-tools">Web apps, mobile apps, internal tools, APIs</div>
                </div>
                <div class="arch-layer">
                    <div class="arch-layer-num">2</div>
                    <div class="arch-layer-name">AI Gateway</div>
                    <div class="arch-layer-desc">Central access layer for all AI services with routing, auth, rate limiting</div>
                    <div class="arch-layer-tools">Kong AI Gateway, Portkey, LiteLLM, custom proxy</div>
                </div>
                <div class="arch-layer">
                    <div class="arch-layer-num">3</div>
                    <div class="arch-layer-name">Model Serving</div>
                    <div class="arch-layer-desc">Inference endpoints for deployed models with load balancing</div>
                    <div class="arch-layer-tools">vLLM, TensorRT-LLM, Triton, Ray Serve</div>
                </div>
                <div class="arch-layer">
                    <div class="arch-layer-num">4</div>
                    <div class="arch-layer-name">Model Registry</div>
                    <div class="arch-layer-desc">Versioned model storage with metadata, lineage, approvals</div>
                    <div class="arch-layer-tools">MLflow Registry, HuggingFace Hub, S3/GCS</div>
                </div>
                <div class="arch-layer">
                    <div class="arch-layer-num">5</div>
                    <div class="arch-layer-name">Training Platform</div>
                    <div class="arch-layer-desc">Distributed training infrastructure with experiment tracking</div>
                    <div class="arch-layer-tools">DeepSpeed, Ray Train, W&B, custom pipelines</div>
                </div>
                <div class="arch-layer">
                    <div class="arch-layer-num">6</div>
                    <div class="arch-layer-name">Data Platform</div>
                    <div class="arch-layer-desc">Training data storage, processing, and feature management</div>
                    <div class="arch-layer-tools">Databricks, Snowflake, Delta Lake, Feast</div>
                </div>
                <div class="arch-layer">
                    <div class="arch-layer-num">7</div>
                    <div class="arch-layer-name">Compute Layer</div>
                    <div class="arch-layer-desc">GPU clusters, container orchestration, autoscaling</div>
                    <div class="arch-layer-tools">Kubernetes, SLURM, cloud GPU instances</div>
                </div>
                <div class="arch-layer">
                    <div class="arch-layer-num">8</div>
                    <div class="arch-layer-name">Observability</div>
                    <div class="arch-layer-desc">Monitoring, logging, tracing across entire stack</div>
                    <div class="arch-layer-tools">Prometheus, Grafana, Datadog, Arize</div>
                </div>
            </div>
        </section>

        <!-- Best Practices -->
        <section class="module" id="practices">
            <div class="module-header">
                <div class="module-icon">‚úÖ</div>
                <div class="module-info">
                    <h2>Best Practices</h2>
                    <p>Guidelines for AI infrastructure success</p>
                </div>
            </div>

            <div class="practices-grid">
                <div class="practice-card">
                    <div class="practice-num">1</div>
                    <div>
                        <div class="practice-title">Start with APIs, Then Self-Host</div>
                        <div class="practice-desc">Use OpenAI/Anthropic APIs to validate use cases before investing in infrastructure. Only self-host when scale, cost, or privacy requirements demand it.</div>
                    </div>
                </div>

                <div class="practice-card">
                    <div class="practice-num">2</div>
                    <div>
                        <div class="practice-title">Separate Training and Inference</div>
                        <div class="practice-desc">Use different GPU types for each. H100s for training, L4/A10 for inference. Different cost profiles, different optimization strategies.</div>
                    </div>
                </div>

                <div class="practice-card">
                    <div class="practice-num">3</div>
                    <div>
                        <div class="practice-title">Invest in Observability Early</div>
                        <div class="practice-desc">Track latency, throughput, GPU utilization, and costs from day one. Can't optimize what you can't measure. AI systems fail in subtle ways.</div>
                    </div>
                </div>

                <div class="practice-card">
                    <div class="practice-num">4</div>
                    <div>
                        <div class="practice-title">Implement Cost Controls</div>
                        <div class="practice-desc">Set budgets, alerts, and kill switches. AI costs can explode unexpectedly. Per-team quotas, per-request limits, and automatic scaling limits are essential.</div>
                    </div>
                </div>

                <div class="practice-card">
                    <div class="practice-num">5</div>
                    <div>
                        <div class="practice-title">Build for GPU Scarcity</div>
                        <div class="practice-desc">Design systems that gracefully handle GPU unavailability. Queue management, fallback to smaller models, multi-cloud capability for resilience.</div>
                    </div>
                </div>

                <div class="practice-card">
                    <div class="practice-num">6</div>
                    <div>
                        <div class="practice-title">Automate Everything</div>
                        <div class="practice-desc">Infrastructure-as-code for reproducibility. Automated training pipelines with checkpointing. Auto-scaling inference based on demand. GitOps for model deployment.</div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Agent This -->
        <section class="module" id="agent">
            <div class="module-header">
                <div class="module-icon">ü§ñ</div>
                <div class="module-info">
                    <h2>Agent This</h2>
                    <p>Infrastructure optimization agent</p>
                </div>
            </div>

            <div class="agent-grid">
                <div class="agent-info">
                    <h3>üñ•Ô∏è InfraOptimizer</h3>
                    <p>An agent that analyzes AI infrastructure costs and performance, recommends optimizations, generates infrastructure-as-code configurations, and monitors for cost anomalies and performance degradation.</p>
                    <div class="agent-capabilities">
                        <div class="agent-capability"><span class="capability-icon">üìä</span> Analyze GPU utilization and costs</div>
                        <div class="agent-capability"><span class="capability-icon">üéØ</span> Recommend right-sized instance types</div>
                        <div class="agent-capability"><span class="capability-icon">‚ö°</span> Identify batching opportunities</div>
                        <div class="agent-capability"><span class="capability-icon">üìù</span> Generate Terraform/Kubernetes configs</div>
                        <div class="agent-capability"><span class="capability-icon">üö®</span> Alert on cost anomalies</div>
                        <div class="agent-capability"><span class="capability-icon">üìà</span> Forecast capacity needs</div>
                    </div>
                </div>
                <div class="agent-code">
                    <div class="code-header">infra_optimizer.py</div>
                    <div class="code-content">
<pre><span class="ck">from</span> crewai <span class="ck">import</span> Agent, Task, Crew
<span class="ck">from</span> langchain_anthropic <span class="ck">import</span> ChatAnthropic

llm = ChatAnthropic(model=<span class="cs">"claude-sonnet-4-20250514"</span>)

infra_optimizer = Agent(
    role=<span class="cs">"AI Infrastructure Optimization Specialist"</span>,
    goal=<span class="cs">"Minimize costs while maximizing performance"</span>,
    backstory=<span class="cs">"""Expert in GPU infrastructure, Kubernetes,
    and cloud cost optimization. Deep knowledge of vLLM,
    TensorRT, and distributed training frameworks."""</span>,
    tools=[
        CloudCostAnalyzer(),
        GPUUtilizationMonitor(),
        TerraformGenerator(),
        CapacityForecaster()
    ],
    llm=llm
)

<span class="ck">async def</span> <span class="cf">optimize_infrastructure</span>(cluster_config, metrics):
    task = Task(
        description=<span class="cs">f"""Analyze infrastructure:
        Config: {cluster_config}
        Metrics: {metrics}
        Provide: Cost savings opportunities, 
        right-sizing recommendations, IaC changes"""</span>,
        agent=infra_optimizer,
        expected_output=<span class="cs">"Optimization report with IaC"</span>
    )
    crew = Crew(agents=[infra_optimizer], tasks=[task])
    <span class="ck">return await</span> crew.kickoff_async()</pre>
                    </div>
                </div>
            </div>
        </section>

        <!-- Related -->
        <section class="module">
            <div class="module-header">
                <div class="module-icon">üìö</div>
                <div class="module-info">
                    <h2>Related Topics</h2>
                    <p>Continue exploring</p>
                </div>
            </div>
            <div class="related-grid">
                <a href="cat19-p1-foundation-models.html" class="related-card">
                    <div class="related-icon">üß†</div>
                    <div class="related-title">Foundation Models</div>
                    <div class="related-desc">The models this infrastructure powers</div>
                </a>
                <a href="cat19-p5-edge-ai.html" class="related-card">
                    <div class="related-icon">üì±</div>
                    <div class="related-title">Edge & On-Device AI</div>
                    <div class="related-desc">Alternative to cloud infrastructure</div>
                </a>
                <a href="cat19-p9-observability.html" class="related-card">
                    <div class="related-icon">üìä</div>
                    <div class="related-title">Observability & Evals</div>
                    <div class="related-desc">Monitoring AI infrastructure</div>
                </a>
            </div>
        </section>

    </div>
</div>

<footer>
    <div class="footer-nav">
        <a href="cat19-p5-edge-ai.html" class="footer-link">
            <span>‚Üê</span>
            <div>
                <div class="footer-link-label">Previous</div>
                <div class="footer-link-title">19.5 Edge & On-Device AI</div>
            </div>
        </a>
        <div class="footer-brand"><span>STRATEGY</span>HUB ‚Ä¢ Page 19.6</div>
        <a href="cat19-p7-ai-safety.html" class="footer-link">
            <div>
                <div class="footer-link-label">Next</div>
                <div class="footer-link-title">19.7 AI Safety & Governance</div>
            </div>
            <span>‚Üí</span>
        </a>
    </div>
</footer>

</body>
</html>
