<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multimodal AI | Emerging Technologies | Strategy Hub</title>
    <style>
        :root {
            --brand-orange: #FF9900;
            --cat-primary: #EC4899;
            --cat-light: #F472B6;
            --cat-dark: #DB2777;
            --cat-glow: rgba(236, 72, 153, 0.15);
            --page-primary: #EC4899;
            --page-light: #F472B6;
            --page-glow: rgba(236, 72, 153, 0.15);
            --bg-dark: #0a0a0f;
            --bg-card: #12121a;
            --bg-card-alt: #1a1a2e;
            --bg-hover: #252538;
            --border-color: #2a2a3e;
            --text-primary: #ffffff;
            --text-secondary: #a0a0b0;
            --text-muted: #6a6a7a;
            --green: #10B981;
            --yellow: #F59E0B;
            --red: #EF4444;
            --blue: #3B82F6;
            --cyan: #06B6D4;
            --pink: #EC4899;
            --purple: #8B5CF6;
            --orange: #F97316;
            --teal: #14B8A6;
            --sidebar-width: 280px;
            --header-height: 60px;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background: var(--bg-dark); color: var(--text-primary); line-height: 1.6; }

        header { background: var(--bg-card); border-bottom: 1px solid var(--border-color); padding: 0 32px; position: fixed; top: 0; left: 0; right: 0; z-index: 1000; height: var(--header-height); display: flex; justify-content: space-between; align-items: center; }
        .logo { font-size: 20px; font-weight: 700; text-decoration: none; color: inherit; }
        .logo span { color: var(--brand-orange); }
        .breadcrumb { display: flex; align-items: center; gap: 8px; font-size: 13px; }
        .breadcrumb a { color: var(--text-secondary); text-decoration: none; }
        .breadcrumb a:hover { color: var(--brand-orange); }
        .breadcrumb .sep { color: var(--text-muted); }
        .breadcrumb .current { color: var(--text-primary); }

        .sidebar { position: fixed; top: var(--header-height); left: 0; width: var(--sidebar-width); height: calc(100vh - var(--header-height)); background: var(--bg-card); border-right: 1px solid var(--border-color); overflow-y: auto; z-index: 100; padding: 24px 0; }
        .sidebar-section { margin-bottom: 28px; }
        .sidebar-title { font-size: 11px; text-transform: uppercase; letter-spacing: 1.5px; color: var(--cat-light); padding: 0 24px; margin-bottom: 12px; font-weight: 600; }
        .sidebar-nav { list-style: none; }
        .sidebar-nav a { display: flex; align-items: center; gap: 10px; padding: 10px 24px; color: var(--text-secondary); text-decoration: none; font-size: 13px; border-left: 3px solid transparent; transition: all 0.2s; }
        .sidebar-nav a:hover { background: var(--bg-hover); color: var(--text-primary); }
        .sidebar-nav a.active { background: var(--cat-glow); color: var(--cat-light); border-left-color: var(--cat-light); }
        .nav-icon { font-size: 16px; width: 24px; text-align: center; }

        .main-wrapper { margin-left: var(--sidebar-width); margin-top: var(--header-height); }
        .main-content { max-width: 1200px; padding: 32px; }

        .hero-compact { display: grid; grid-template-columns: 1fr 1fr; gap: 32px; margin-bottom: 32px; align-items: center; }
        .hero-tag { display: inline-flex; align-items: center; gap: 8px; background: var(--page-glow); border: 1px solid var(--page-primary); padding: 6px 14px; border-radius: 9999px; font-size: 12px; color: var(--page-light); margin-bottom: 16px; }
        .hero-left h1 { font-size: 32px; font-weight: 700; margin-bottom: 12px; }
        .hero-left > p { font-size: 15px; color: var(--text-secondary); line-height: 1.7; }
        .hero-metrics { display: grid; grid-template-columns: repeat(2, 1fr); gap: 12px; }
        .hero-metric { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; text-align: center; }
        .hero-metric-value { font-size: 28px; font-weight: 700; color: var(--page-light); }
        .hero-metric-label { font-size: 11px; color: var(--text-muted); margin-top: 4px; }

        .module { background: var(--bg-card); border: 1px solid var(--border-color); border-radius: 16px; padding: 28px; margin-bottom: 24px; }
        .module-header { display: flex; align-items: flex-start; gap: 16px; margin-bottom: 24px; }
        .module-icon { width: 48px; height: 48px; background: var(--page-glow); border: 2px solid var(--page-primary); border-radius: 12px; display: flex; align-items: center; justify-content: center; font-size: 22px; flex-shrink: 0; }
        .module-info h2 { font-size: 20px; font-weight: 600; margin-bottom: 4px; }
        .module-info p { font-size: 13px; color: var(--text-secondary); }

        .overview-content { display: grid; grid-template-columns: 1fr 1fr; gap: 24px; }
        .overview-content h3 { font-size: 15px; font-weight: 600; margin-bottom: 12px; color: var(--page-light); }
        .overview-content p { font-size: 14px; color: var(--text-secondary); line-height: 1.7; margin-bottom: 12px; }
        .overview-full { grid-column: 1 / -1; }
        .overview-content ul { margin-left: 20px; margin-bottom: 12px; }
        .overview-content li { font-size: 14px; color: var(--text-secondary); margin-bottom: 6px; }
        .overview-content strong { color: var(--text-primary); }

        .callout { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; margin-bottom: 24px; display: flex; gap: 16px; }
        .callout.info { border-left: 4px solid var(--blue); }
        .callout.warning { border-left: 4px solid var(--yellow); }
        .callout.success { border-left: 4px solid var(--green); }
        .callout.pink { border-left: 4px solid var(--pink); }
        .callout-icon { font-size: 24px; }
        .callout-content h4 { font-size: 14px; font-weight: 600; margin-bottom: 6px; }
        .callout-content p { font-size: 13px; color: var(--text-secondary); line-height: 1.6; }

        /* MODEL CARDS */
        .models-grid { display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px; margin-bottom: 24px; }
        .model-card { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; transition: all 0.3s; }
        .model-card:hover { border-color: var(--page-primary); transform: translateY(-2px); }
        .model-header { display: flex; justify-content: space-between; align-items: flex-start; margin-bottom: 12px; }
        .model-name { font-size: 18px; font-weight: 600; }
        .model-company { font-size: 12px; color: var(--text-muted); margin-top: 2px; }
        .model-badge { padding: 4px 10px; border-radius: 9999px; font-size: 10px; font-weight: 600; }
        .model-badge.vision { background: rgba(59, 130, 246, 0.15); color: var(--blue); border: 1px solid rgba(59, 130, 246, 0.3); }
        .model-badge.audio { background: rgba(16, 185, 129, 0.15); color: var(--green); border: 1px solid rgba(16, 185, 129, 0.3); }
        .model-badge.video { background: rgba(139, 92, 246, 0.15); color: var(--purple); border: 1px solid rgba(139, 92, 246, 0.3); }
        .model-badge.native { background: rgba(236, 72, 153, 0.15); color: var(--pink); border: 1px solid rgba(236, 72, 153, 0.3); }
        .model-badge.generation { background: rgba(249, 115, 22, 0.15); color: var(--orange); border: 1px solid rgba(249, 115, 22, 0.3); }
        .model-desc { font-size: 13px; color: var(--text-secondary); line-height: 1.6; margin-bottom: 12px; }
        .model-specs { display: flex; flex-wrap: wrap; gap: 8px; margin-bottom: 12px; }
        .model-spec { font-size: 11px; padding: 4px 10px; background: var(--bg-card); border-radius: 6px; color: var(--text-muted); }
        .model-spec.highlight { background: var(--page-glow); color: var(--page-light); }
        .model-capabilities { display: flex; flex-wrap: wrap; gap: 6px; }
        .capability-tag { font-size: 10px; padding: 3px 8px; background: var(--bg-card); border-radius: 4px; color: var(--text-muted); }
        .capability-tag.strong { background: rgba(16, 185, 129, 0.15); color: var(--green); }

        /* MODALITY CARDS */
        .modality-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; margin-bottom: 24px; }
        .modality-card { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; text-align: center; transition: all 0.3s; }
        .modality-card:hover { border-color: var(--page-primary); }
        .modality-icon { font-size: 40px; margin-bottom: 12px; }
        .modality-title { font-size: 16px; font-weight: 600; margin-bottom: 8px; }
        .modality-desc { font-size: 12px; color: var(--text-muted); margin-bottom: 12px; }
        .modality-status { font-size: 11px; padding: 4px 10px; border-radius: 9999px; display: inline-block; }
        .modality-status.mature { background: rgba(16, 185, 129, 0.15); color: var(--green); }
        .modality-status.emerging { background: rgba(245, 158, 11, 0.15); color: var(--yellow); }
        .modality-status.early { background: rgba(139, 92, 246, 0.15); color: var(--purple); }

        /* COMPARISON TABLE */
        .compare-table { width: 100%; border-collapse: collapse; margin-bottom: 24px; font-size: 13px; }
        .compare-table th { text-align: left; padding: 12px 14px; font-size: 11px; font-weight: 600; text-transform: uppercase; letter-spacing: 0.5px; color: var(--text-muted); background: var(--bg-dark); border-bottom: 1px solid var(--border-color); }
        .compare-table td { padding: 12px 14px; border-bottom: 1px solid var(--border-color); color: var(--text-secondary); }
        .compare-table tr:hover td { background: var(--bg-hover); }
        .compare-table .model-name-cell { font-weight: 600; color: var(--text-primary); }
        .compare-table .check { color: var(--green); }
        .compare-table .partial { color: var(--yellow); }
        .compare-table .none { color: var(--text-muted); }
        .compare-table .best { background: rgba(16, 185, 129, 0.1); }

        /* USE CASE CARDS */
        .usecase-grid { display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px; margin-bottom: 24px; }
        .usecase-card { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; }
        .usecase-header { display: flex; align-items: center; gap: 12px; margin-bottom: 12px; }
        .usecase-icon { width: 44px; height: 44px; background: var(--page-glow); border: 1px solid var(--page-primary); border-radius: 10px; display: flex; align-items: center; justify-content: center; font-size: 22px; }
        .usecase-title { font-size: 15px; font-weight: 600; }
        .usecase-desc { font-size: 13px; color: var(--text-secondary); line-height: 1.6; margin-bottom: 12px; }
        .usecase-examples { font-size: 12px; color: var(--text-muted); }
        .usecase-examples strong { color: var(--text-secondary); }

        /* PRACTICES */
        .practices-grid { display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px; margin-bottom: 24px; }
        .practice-card { display: flex; gap: 14px; padding: 16px; background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 10px; }
        .practice-num { width: 32px; height: 32px; background: var(--page-glow); border: 1px solid var(--page-primary); border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 13px; font-weight: 700; color: var(--page-light); flex-shrink: 0; }
        .practice-content h4 { font-size: 14px; font-weight: 600; margin-bottom: 4px; }
        .practice-content p { font-size: 12px; color: var(--text-secondary); line-height: 1.5; }

        /* GENERATION TOOLS */
        .gen-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; margin-bottom: 24px; }
        .gen-card { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; padding: 20px; transition: all 0.3s; }
        .gen-card:hover { border-color: var(--page-primary); }
        .gen-header { display: flex; justify-content: space-between; align-items: flex-start; margin-bottom: 12px; }
        .gen-name { font-size: 16px; font-weight: 600; }
        .gen-company { font-size: 11px; color: var(--text-muted); }
        .gen-badge { padding: 4px 8px; border-radius: 6px; font-size: 10px; font-weight: 600; }
        .gen-badge.image { background: rgba(59, 130, 246, 0.15); color: var(--blue); }
        .gen-badge.video { background: rgba(139, 92, 246, 0.15); color: var(--purple); }
        .gen-badge.audio { background: rgba(16, 185, 129, 0.15); color: var(--green); }
        .gen-badge.music { background: rgba(236, 72, 153, 0.15); color: var(--pink); }
        .gen-badge.voice { background: rgba(249, 115, 22, 0.15); color: var(--orange); }
        .gen-desc { font-size: 12px; color: var(--text-secondary); line-height: 1.5; margin-bottom: 10px; }
        .gen-features { display: flex; flex-wrap: wrap; gap: 6px; }
        .gen-feature { font-size: 10px; padding: 3px 8px; background: var(--bg-card); border-radius: 4px; color: var(--text-muted); }

        /* TIMELINE */
        .timeline { position: relative; padding-left: 30px; margin-bottom: 24px; }
        .timeline::before { content: ''; position: absolute; left: 8px; top: 0; bottom: 0; width: 2px; background: var(--border-color); }
        .timeline-item { position: relative; padding-bottom: 24px; }
        .timeline-item:last-child { padding-bottom: 0; }
        .timeline-dot { position: absolute; left: -26px; top: 4px; width: 12px; height: 12px; border-radius: 50%; background: var(--page-primary); border: 2px solid var(--bg-card); }
        .timeline-date { font-size: 12px; color: var(--page-light); font-weight: 600; margin-bottom: 4px; }
        .timeline-title { font-size: 14px; font-weight: 600; margin-bottom: 4px; }
        .timeline-desc { font-size: 13px; color: var(--text-secondary); }

        /* AGENT SECTION */
        .agent-grid { display: grid; grid-template-columns: 1fr 1.2fr; gap: 24px; }
        .agent-info h3 { font-size: 18px; margin-bottom: 12px; color: var(--page-light); }
        .agent-info > p { font-size: 14px; color: var(--text-secondary); line-height: 1.7; margin-bottom: 16px; }
        .agent-capabilities { display: flex; flex-direction: column; gap: 8px; }
        .agent-capability { display: flex; align-items: center; gap: 10px; padding: 10px 14px; background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 8px; font-size: 13px; color: var(--text-secondary); }
        .capability-icon { font-size: 16px; }
        .agent-code { background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; overflow: hidden; }
        .code-header { padding: 12px 16px; background: var(--bg-card-alt); border-bottom: 1px solid var(--border-color); font-size: 12px; font-family: monospace; color: var(--text-muted); }
        .code-content { padding: 16px; overflow-x: auto; }
        .code-content pre { font-family: 'Monaco', 'Menlo', monospace; font-size: 12px; line-height: 1.6; color: var(--text-secondary); }
        .code-keyword { color: var(--pink); }
        .code-string { color: var(--green); }
        .code-comment { color: var(--text-muted); }
        .code-function { color: var(--cyan); }

        /* RELATED */
        .related-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; }
        .related-card { display: block; padding: 20px; background: var(--bg-dark); border: 1px solid var(--border-color); border-radius: 12px; text-decoration: none; transition: all 0.3s; }
        .related-card:hover { border-color: var(--page-primary); transform: translateY(-2px); }
        .related-icon { font-size: 24px; margin-bottom: 12px; }
        .related-title { font-size: 14px; font-weight: 600; color: var(--text-primary); margin-bottom: 6px; }
        .related-desc { font-size: 12px; color: var(--text-muted); }

        /* FOOTER */
        footer { background: var(--bg-card); border-top: 1px solid var(--border-color); padding: 16px 32px; margin-left: var(--sidebar-width); }
        .footer-nav { display: flex; justify-content: space-between; align-items: center; }
        .footer-link { display: flex; align-items: center; gap: 12px; text-decoration: none; color: var(--text-secondary); padding: 12px 20px; background: var(--bg-card-alt); border: 1px solid var(--border-color); border-radius: 10px; transition: all 0.2s; }
        .footer-link:hover { border-color: var(--page-primary); color: var(--text-primary); }
        .footer-link-label { font-size: 11px; color: var(--text-muted); }
        .footer-link-title { font-size: 14px; font-weight: 600; }
        .footer-brand { font-size: 13px; color: var(--text-muted); }
        .footer-brand span { color: var(--brand-orange); }

        @media (max-width: 1024px) {
            .sidebar { display: none; }
            .main-wrapper { margin-left: 0; }
            footer { margin-left: 0; }
            .hero-compact, .overview-content, .models-grid, .usecase-grid, .practices-grid, .agent-grid { grid-template-columns: 1fr; }
            .modality-grid, .gen-grid, .related-grid { grid-template-columns: 1fr; }
        }
    </style>
</head>
<body>

<header>
    <a href="index.html" class="logo"><span>STRATEGY</span>HUB</a>
    <div class="breadcrumb">
        <a href="index.html">Hub</a>
        <span class="sep">/</span>
        <a href="cat19-emerging-technologies-overview.html">Emerging Technologies</a>
        <span class="sep">/</span>
        <span class="current">Multimodal AI</span>
    </div>
</header>

<aside class="sidebar">
    <div class="sidebar-section">
        <div class="sidebar-title">ğŸš€ Category 19</div>
        <ul class="sidebar-nav">
            <li><a href="cat19-emerging-technologies-overview.html"><span class="nav-icon">ğŸ </span> Overview</a></li>
            <li><a href="cat19-p1-foundation-models.html"><span class="nav-icon">ğŸ§ </span> 19.1 Foundation Models</a></li>
            <li><a href="cat19-p2-agentic-ai.html"><span class="nav-icon">ğŸ¤–</span> 19.2 Agentic AI</a></li>
            <li><a href="cat19-p3-multimodal-ai.html" class="active"><span class="nav-icon">ğŸ¨</span> 19.3 Multimodal AI</a></li>
            <li><a href="cat19-p4-ai-coding.html"><span class="nav-icon">ğŸ’»</span> 19.4 AI Coding Tools</a></li>
            <li><a href="cat19-p5-edge-ai.html"><span class="nav-icon">ğŸ“±</span> 19.5 Edge & On-Device</a></li>
            <li><a href="cat19-p6-ai-infrastructure.html"><span class="nav-icon">ğŸ–¥ï¸</span> 19.6 AI Infrastructure</a></li>
            <li><a href="cat19-p7-ai-safety.html"><span class="nav-icon">ğŸ›¡ï¸</span> 19.7 AI Safety & Governance</a></li>
            <li><a href="cat19-p8-rag-knowledge.html"><span class="nav-icon">ğŸ“š</span> 19.8 RAG & Knowledge</a></li>
            <li><a href="cat19-p9-observability.html"><span class="nav-icon">ğŸ“Š</span> 19.9 Observability & Evals</a></li>
            <li><a href="cat19-p10-enterprise-adoption.html"><span class="nav-icon">ğŸ¢</span> 19.10 Enterprise Adoption</a></li>
        </ul>
    </div>
    <div class="sidebar-section">
        <div class="sidebar-title">ğŸ“‘ On This Page</div>
        <ul class="sidebar-nav">
            <li><a href="#overview"><span class="nav-icon">ğŸ’¡</span> Overview</a></li>
            <li><a href="#why-multimodal"><span class="nav-icon">âš¡</span> Why Multimodal</a></li>
            <li><a href="#modalities"><span class="nav-icon">ğŸ¯</span> Modalities</a></li>
            <li><a href="#architectures"><span class="nav-icon">ğŸ—ï¸</span> Architectures</a></li>
            <li><a href="#evolution"><span class="nav-icon">ğŸ“ˆ</span> Evolution</a></li>
            <li><a href="#understanding"><span class="nav-icon">ğŸ‘ï¸</span> Vision-Language</a></li>
            <li><a href="#vision"><span class="nav-icon">ğŸ“·</span> Vision Deep Dive</a></li>
            <li><a href="#audio"><span class="nav-icon">ğŸ¤</span> Audio & Speech</a></li>
            <li><a href="#video"><span class="nav-icon">ğŸ¬</span> Video Understanding</a></li>
            <li><a href="#image-gen"><span class="nav-icon">ğŸ–¼ï¸</span> Image Generation</a></li>
            <li><a href="#video-gen"><span class="nav-icon">ğŸ“¹</span> Video Generation</a></li>
            <li><a href="#audio-gen"><span class="nav-icon">ğŸ”Š</span> Audio Generation</a></li>
            <li><a href="#native"><span class="nav-icon">ğŸŒ</span> Native Multimodal</a></li>
            <li><a href="#realtime"><span class="nav-icon">âš¡</span> Realtime APIs</a></li>
            <li><a href="#comparison"><span class="nav-icon">ğŸ“Š</span> Model Comparison</a></li>
            <li><a href="#usecases"><span class="nav-icon">ğŸ’¼</span> Use Cases</a></li>
            <li><a href="#implementation"><span class="nav-icon">ğŸ› ï¸</span> Implementation</a></li>
            <li><a href="#practices"><span class="nav-icon">âœ…</span> Best Practices</a></li>
            <li><a href="#costs"><span class="nav-icon">ğŸ’°</span> Costs & Tokens</a></li>
            <li><a href="#opensource"><span class="nav-icon">ğŸ”“</span> Open Source</a></li>
            <li><a href="#prompting"><span class="nav-icon">âœï¸</span> Prompting</a></li>
            <li><a href="#limitations"><span class="nav-icon">âš ï¸</span> Limitations</a></li>
            <li><a href="#integration"><span class="nav-icon">ğŸ”—</span> Integration</a></li>
            <li><a href="#future"><span class="nav-icon">ğŸ”®</span> Future</a></li>
            <li><a href="#agent"><span class="nav-icon">ğŸ¤–</span> Agent This</a></li>
        </ul>
    </div>
</aside>

<div class="main-wrapper">
    <div class="main-content">
        
        <!-- HERO -->
        <section class="hero-compact">
            <div class="hero-left">
                <div class="hero-tag">ğŸ¨ Page 19.3</div>
                <h1>Multimodal AI</h1>
                <p>AI systems that understand and generate across text, images, audio, and video. From vision-language models to native multimodal systemsâ€”the convergence of all AI modalities.</p>
            </div>
            <div class="hero-metrics">
                <div class="hero-metric">
                    <div class="hero-metric-value">6+</div>
                    <div class="hero-metric-label">Modalities</div>
                </div>
                <div class="hero-metric">
                    <div class="hero-metric-value">2M</div>
                    <div class="hero-metric-label">Max Context</div>
                </div>
                <div class="hero-metric">
                    <div class="hero-metric-value">Native</div>
                    <div class="hero-metric-label">Integration</div>
                </div>
                <div class="hero-metric">
                    <div class="hero-metric-value">Realtime</div>
                    <div class="hero-metric-label">Voice/Video</div>
                </div>
            </div>
        </section>

        <!-- OVERVIEW -->
        <section class="module" id="overview">
            <div class="module-header">
                <div class="module-icon">ğŸ’¡</div>
                <div class="module-info">
                    <h2>Overview</h2>
                    <p>Understanding multimodal AI systems</p>
                </div>
            </div>

            <div class="overview-content">
                <div>
                    <h3>What is Multimodal AI?</h3>
                    <p><strong>Multimodal AI</strong> refers to systems that can process, understand, and generate content across multiple types of dataâ€”text, images, audio, video, and more. Unlike single-modality models that only work with text, multimodal models can see, hear, and create across formats.</p>
                    <p>The goal is AI that interacts with the world the way humans doâ€”through <strong>all senses simultaneously</strong>, understanding the connections between what we see, hear, and read.</p>
                </div>
                <div>
                    <h3>Key Capabilities</h3>
                    <ul>
                        <li><strong>Vision Understanding</strong> â€” Analyze images, charts, documents, screenshots</li>
                        <li><strong>Audio Processing</strong> â€” Transcribe speech, understand tone, process music</li>
                        <li><strong>Video Analysis</strong> â€” Understand scenes, actions, temporal relationships</li>
                        <li><strong>Image Generation</strong> â€” Create images from text descriptions</li>
                        <li><strong>Voice Synthesis</strong> â€” Generate natural-sounding speech</li>
                        <li><strong>Cross-modal Reasoning</strong> â€” Connect information across modalities</li>
                    </ul>
                </div>
                <div class="overview-full">
                    <h3>The Convergence</h3>
                    <p>We're witnessing a convergence: models that started as text-only (GPT, Claude) are gaining vision, audio, and video capabilities. Models that started as image generators (DALL-E, Midjourney) are gaining text understanding. The end state is <strong>unified multimodal models</strong> that handle all modalities nativelyâ€”understanding and generating any combination of text, image, audio, and video in a single coherent system.</p>
                </div>
            </div>
        </section>

        <!-- WHY MULTIMODAL -->
        <section class="module" id="why-multimodal">
            <div class="module-header">
                <div class="module-icon">âš¡</div>
                <div class="module-info">
                    <h2>Why Multimodal Matters</h2>
                    <p>The case for multi-sensory AI</p>
                </div>
            </div>

            <div class="callout pink">
                <div class="callout-icon">ğŸŒ</div>
                <div class="callout-content">
                    <h4>The World is Multimodal</h4>
                    <p>Humans don't experience the world in text alone. We see, hear, and feel simultaneously. Our documents have images. Our meetings have whiteboards. Our messages have photos and voice notes. <strong>AI that can only read text is fundamentally limited.</strong> True AI assistance requires understanding the full richness of human communication.</p>
                </div>
            </div>

            <div class="usecase-grid">
                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“„</div>
                        <div class="usecase-title">Document Understanding</div>
                    </div>
                    <p class="usecase-desc">Real documents aren't just text. They have tables, charts, diagrams, logos, signatures. Vision-enabled AI can understand documents the way humans doâ€”seeing the whole picture, not just extracted text.</p>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ—£ï¸</div>
                        <div class="usecase-title">Natural Interaction</div>
                    </div>
                    <p class="usecase-desc">Voice is the most natural interface. Show-and-tell is intuitive. Multimodal AI enables conversations where you can speak, share your screen, snap a photoâ€”interacting naturally instead of typing everything.</p>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ”</div>
                        <div class="usecase-title">Richer Analysis</div>
                    </div>
                    <p class="usecase-desc">Understanding a product review with images. Analyzing a video tutorial. Processing a podcast transcript with speaker identification. Multimodal enables analysis that captures the full context.</p>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ¨</div>
                        <div class="usecase-title">Creative Generation</div>
                    </div>
                    <p class="usecase-desc">Create illustrations for blog posts. Generate product mockups from descriptions. Produce podcast intros. Multimodal generation enables creation across all media types from a single prompt.</p>
                </div>
            </div>

            <div class="overview-content">
                <div>
                    <h3>Business Impact</h3>
                    <ul>
                        <li><strong>80%</strong> of enterprise data is unstructured (images, PDFs, videos)</li>
                        <li><strong>Document processing</strong> automation finally works reliably</li>
                        <li><strong>Customer support</strong> can handle image/video submissions</li>
                        <li><strong>Content creation</strong> workflows become 10x faster</li>
                        <li><strong>Accessibility</strong> improves with alt-text, transcription, description</li>
                    </ul>
                </div>
                <div>
                    <h3>The Unlock</h3>
                    <ul>
                        <li><strong>Computer use</strong> â€” AI that can see screens can operate any software</li>
                        <li><strong>Robotics</strong> â€” Vision-language enables embodied AI</li>
                        <li><strong>Autonomous vehicles</strong> â€” Scene understanding from cameras</li>
                        <li><strong>Healthcare</strong> â€” Medical image analysis at scale</li>
                        <li><strong>Retail</strong> â€” Visual search, virtual try-on, inventory management</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- MODALITIES -->
        <section class="module" id="modalities">
            <div class="module-header">
                <div class="module-icon">ğŸ¯</div>
                <div class="module-info">
                    <h2>The Modalities</h2>
                    <p>Types of data multimodal AI can handle</p>
                </div>
            </div>

            <div class="modality-grid">
                <div class="modality-card">
                    <div class="modality-icon">ğŸ“</div>
                    <div class="modality-title">Text</div>
                    <div class="modality-desc">Natural language, code, structured data. The foundation modality that all modern AI started with.</div>
                    <span class="modality-status mature">Mature</span>
                </div>

                <div class="modality-card">
                    <div class="modality-icon">ğŸ–¼ï¸</div>
                    <div class="modality-title">Images</div>
                    <div class="modality-desc">Photos, diagrams, charts, screenshots, documents. Both understanding and generation.</div>
                    <span class="modality-status mature">Mature</span>
                </div>

                <div class="modality-card">
                    <div class="modality-icon">ğŸ¤</div>
                    <div class="modality-title">Audio</div>
                    <div class="modality-desc">Speech, music, sound effects, ambient sounds. Transcription, understanding, and synthesis.</div>
                    <span class="modality-status mature">Mature</span>
                </div>

                <div class="modality-card">
                    <div class="modality-icon">ğŸ¬</div>
                    <div class="modality-title">Video</div>
                    <div class="modality-desc">Moving images with temporal understanding. Scene analysis, action recognition, video generation.</div>
                    <span class="modality-status emerging">Emerging</span>
                </div>

                <div class="modality-card">
                    <div class="modality-icon">ğŸµ</div>
                    <div class="modality-title">Music</div>
                    <div class="modality-desc">Compositions, instrumentals, vocals. Generation of full songs from descriptions or stems.</div>
                    <span class="modality-status emerging">Emerging</span>
                </div>

                <div class="modality-card">
                    <div class="modality-icon">ğŸŒ</div>
                    <div class="modality-title">3D</div>
                    <div class="modality-desc">3D objects, scenes, environments. Generation from images or text for games, AR/VR.</div>
                    <span class="modality-status early">Early</span>
                </div>
            </div>
        </section>

        <!-- MULTIMODAL ARCHITECTURES -->
        <section class="module" id="architectures">
            <div class="module-header">
                <div class="module-icon">ğŸ—ï¸</div>
                <div class="module-info">
                    <h2>Multimodal Architectures</h2>
                    <p>How multimodal systems are built</p>
                </div>
            </div>

            <div class="callout info">
                <div class="callout-icon">ğŸ§ </div>
                <div class="callout-content">
                    <h4>The Architecture Matters</h4>
                    <p>Not all multimodal models are created equal. <strong>Bolt-on</strong> approaches add vision to existing text models. <strong>Native</strong> architectures train on all modalities together from scratch. The difference affects quality, latency, and capability depth. Understanding architectures helps you choose the right tool.</p>
                </div>
            </div>

            <div class="overview-content">
                <div>
                    <h3>Bolt-On Multimodal</h3>
                    <p>Take a pre-trained LLM and add vision capabilities by:</p>
                    <ul>
                        <li><strong>Vision encoder</strong> â€” CLIP, SigLIP, or ViT processes images</li>
                        <li><strong>Projection layer</strong> â€” Maps visual features to LLM embedding space</li>
                        <li><strong>Fine-tuning</strong> â€” Train on image-text pairs</li>
                        <li><strong>Examples</strong> â€” LLaVA, InternVL, early GPT-4V</li>
                    </ul>
                    <p><strong>Pros:</strong> Faster to build, leverages existing LLM capabilities<br>
                    <strong>Cons:</strong> Vision is "second class citizen", less integrated understanding</p>
                </div>
                <div>
                    <h3>Native Multimodal</h3>
                    <p>Train from scratch on all modalities together:</p>
                    <ul>
                        <li><strong>Unified tokenization</strong> â€” Text, image, audio tokens in same sequence</li>
                        <li><strong>Joint training</strong> â€” All modalities trained simultaneously</li>
                        <li><strong>Cross-modal attention</strong> â€” Deep connections between modalities</li>
                        <li><strong>Examples</strong> â€” GPT-4o, Gemini 2.0, Chameleon</li>
                    </ul>
                    <p><strong>Pros:</strong> Deeper understanding, better cross-modal reasoning<br>
                    <strong>Cons:</strong> Much more expensive to train, requires massive multimodal datasets</p>
                </div>
            </div>

            <div class="usecase-grid" style="margin-top: 24px;">
                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ”—</div>
                        <div class="usecase-title">CLIP Architecture</div>
                    </div>
                    <p class="usecase-desc">Contrastive Language-Image Pre-training. Trains image and text encoders to map matching pairs close together. Foundation for most vision-language systems.</p>
                    <div class="usecase-examples"><strong>Used in:</strong> DALL-E, Stable Diffusion, many VLMs</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ¯</div>
                        <div class="usecase-title">Vision Transformer (ViT)</div>
                    </div>
                    <p class="usecase-desc">Applies transformer architecture to images by dividing into patches. Each patch becomes a "token" processed by attention.</p>
                    <div class="usecase-examples"><strong>Used in:</strong> CLIP, SigLIP, DINOv2</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ”€</div>
                        <div class="usecase-title">Cross-Attention</div>
                    </div>
                    <p class="usecase-desc">Allows text tokens to attend to image features and vice versa. Enables fine-grained image-text alignment.</p>
                    <div class="usecase-examples"><strong>Used in:</strong> Flamingo, BLIP-2, InstructBLIP</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ§©</div>
                        <div class="usecase-title">Q-Former</div>
                    </div>
                    <p class="usecase-desc">Learned queries that extract fixed-size visual features regardless of image resolution. Efficient bridge between vision and language.</p>
                    <div class="usecase-examples"><strong>Used in:</strong> BLIP-2, InstructBLIP</div>
                </div>
            </div>

            <div class="compare-table" style="margin-top: 24px;">
                <thead>
                    <tr>
                        <th>Architecture</th>
                        <th>Approach</th>
                        <th>Training Cost</th>
                        <th>Quality</th>
                        <th>Latency</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="model-name-cell">Bolt-On (LLaVA-style)</td>
                        <td>Vision encoder + Projection + LLM</td>
                        <td class="check">âœ“ Lower</td>
                        <td>Good</td>
                        <td>Medium</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Cross-Attention (Flamingo)</td>
                        <td>Interleaved cross-attention layers</td>
                        <td>Medium</td>
                        <td>Better</td>
                        <td>Medium</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Native (GPT-4o/Gemini)</td>
                        <td>Unified multimodal training</td>
                        <td class="none">âœ— Highest</td>
                        <td class="best">Best</td>
                        <td class="best">Fastest</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- THE EVOLUTION -->
        <section class="module" id="evolution">
            <div class="module-header">
                <div class="module-icon">ğŸ“ˆ</div>
                <div class="module-info">
                    <h2>The Evolution of Multimodal AI</h2>
                    <p>From separate models to unified systems</p>
                </div>
            </div>

            <div class="timeline">
                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-date">2021</div>
                    <div class="timeline-title">CLIP & DALL-E</div>
                    <div class="timeline-desc">OpenAI's CLIP learns joint image-text representations. DALL-E generates images from text. The multimodal revolution begins.</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-date">2022</div>
                    <div class="timeline-title">Stable Diffusion & Midjourney</div>
                    <div class="timeline-desc">Open-source image generation explodes. Midjourney captures creative community. DALL-E 2 raises the bar.</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-date">2023</div>
                    <div class="timeline-title">GPT-4V & Claude 3 Vision</div>
                    <div class="timeline-desc">LLMs gain vision. GPT-4 with vision launches. Claude 3 follows. Document understanding becomes viable.</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-date">2024 Q1</div>
                    <div class="timeline-title">Sora & Video Generation</div>
                    <div class="timeline-desc">OpenAI's Sora demonstrates minute-long realistic video. Runway Gen-3, Pika advance video generation.</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-date">2024 Q2</div>
                    <div class="timeline-title">GPT-4o Native Multimodal</div>
                    <div class="timeline-desc">First truly native multimodal model from OpenAI. Text, vision, audio in unified architecture.</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-date">2024 Q4</div>
                    <div class="timeline-title">Gemini 2.0 & Realtime APIs</div>
                    <div class="timeline-desc">Google's Gemini 2.0 with agentic multimodal. OpenAI Realtime API enables voice conversations.</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-date">2025</div>
                    <div class="timeline-title">Unified Generation</div>
                    <div class="timeline-desc">Models that both understand AND generate all modalities. The convergence completes.</div>
                </div>
            </div>
        </section>

        <!-- VISION UNDERSTANDING MODELS -->
        <section class="module" id="understanding">
            <div class="module-header">
                <div class="module-icon">ğŸ‘ï¸</div>
                <div class="module-info">
                    <h2>Vision-Language Models</h2>
                    <p>LLMs that can see and understand images</p>
                </div>
            </div>

            <div class="callout info">
                <div class="callout-icon">ğŸ”</div>
                <div class="callout-content">
                    <h4>Vision is the First Frontier</h4>
                    <p>Adding vision to language models was the first major multimodal breakthrough. GPT-4V (Vision) launched in late 2023, followed rapidly by Claude 3, Gemini 1.5, and others. Today, <strong>all frontier language models include vision capabilities</strong>. If your LLM can't see images, it's behind.</p>
                </div>
            </div>

            <div class="models-grid">
                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">GPT-4o</div>
                            <div class="model-company">OpenAI</div>
                        </div>
                        <span class="model-badge native">Native Multimodal</span>
                    </div>
                    <p class="model-desc">Native multimodal model trained on text, images, and audio together. Excellent vision understanding with fast response times. Powers ChatGPT vision features.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">128K context</span>
                        <span class="model-spec">Image input</span>
                        <span class="model-spec">Audio I/O</span>
                    </div>
                    <div class="model-capabilities">
                        <span class="capability-tag strong">Document OCR</span>
                        <span class="capability-tag strong">Chart Analysis</span>
                        <span class="capability-tag">Scene Understanding</span>
                        <span class="capability-tag">Math/Diagrams</span>
                    </div>
                </div>

                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">Claude 3.5 Sonnet</div>
                            <div class="model-company">Anthropic</div>
                        </div>
                        <span class="model-badge vision">Vision</span>
                    </div>
                    <p class="model-desc">Excellent vision capabilities with strong chart/diagram understanding. Powers computer use and document analysis. Best-in-class for structured data extraction.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">200K context</span>
                        <span class="model-spec">Image input</span>
                        <span class="model-spec">PDF native</span>
                    </div>
                    <div class="model-capabilities">
                        <span class="capability-tag strong">Charts/Tables</span>
                        <span class="capability-tag strong">Screenshots</span>
                        <span class="capability-tag">Computer Use</span>
                        <span class="capability-tag">Handwriting</span>
                    </div>
                </div>

                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">Gemini 2.0 Flash</div>
                            <div class="model-company">Google</div>
                        </div>
                        <span class="model-badge native">Native Multimodal</span>
                    </div>
                    <p class="model-desc">Native multimodal with massive 1M+ token context. Process hours of video, hundreds of images. Best for long-context multimodal tasks.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">1M+ context</span>
                        <span class="model-spec">Video native</span>
                        <span class="model-spec">Audio native</span>
                    </div>
                    <div class="model-capabilities">
                        <span class="capability-tag strong">Video Understanding</span>
                        <span class="capability-tag strong">Long Context</span>
                        <span class="capability-tag">Audio</span>
                        <span class="capability-tag">Multipage Docs</span>
                    </div>
                </div>

                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">Llama 3.2 Vision</div>
                            <div class="model-company">Meta</div>
                        </div>
                        <span class="model-badge vision">Vision</span>
                    </div>
                    <p class="model-desc">Open-source vision-language models in 11B and 90B sizes. Commercially usable. Strong baseline for on-premise vision deployments.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">Open Source</span>
                        <span class="model-spec">11B / 90B</span>
                        <span class="model-spec">Self-host</span>
                    </div>
                    <div class="model-capabilities">
                        <span class="capability-tag strong">Open Weights</span>
                        <span class="capability-tag">Commercial OK</span>
                        <span class="capability-tag">Fine-tunable</span>
                        <span class="capability-tag">Edge Deploy</span>
                    </div>
                </div>
            </div>
        </section>

        <!-- VISION CAPABILITIES DEEP DIVE -->
        <section class="module" id="vision">
            <div class="module-header">
                <div class="module-icon">ğŸ“·</div>
                <div class="module-info">
                    <h2>Vision Capabilities Deep Dive</h2>
                    <p>What vision-language models can actually do</p>
                </div>
            </div>

            <div class="usecase-grid">
                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“„</div>
                        <div class="usecase-title">Document OCR & Extraction</div>
                    </div>
                    <p class="usecase-desc">Read text from images, PDFs, scanned documents. Extract structured data from invoices, receipts, forms. Handle handwriting, stamps, logos.</p>
                    <div class="usecase-examples"><strong>Accuracy:</strong> 95%+ on printed text, 80%+ on handwriting</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“Š</div>
                        <div class="usecase-title">Chart & Graph Analysis</div>
                    </div>
                    <p class="usecase-desc">Understand bar charts, line graphs, pie charts, scatter plots. Extract data points, identify trends, answer questions about visualizations.</p>
                    <div class="usecase-examples"><strong>Best for:</strong> Claude 3.5, GPT-4o, Gemini</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“</div>
                        <div class="usecase-title">Diagram Understanding</div>
                    </div>
                    <p class="usecase-desc">Interpret flowcharts, architecture diagrams, UML, circuit diagrams. Understand relationships and explain complex visual systems.</p>
                    <div class="usecase-examples"><strong>Use cases:</strong> Technical documentation, education</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ–¥ï¸</div>
                        <div class="usecase-title">Screenshot Analysis</div>
                    </div>
                    <p class="usecase-desc">Understand UI screenshots, identify elements, read text, understand layout. Powers computer use and automated testing.</p>
                    <div class="usecase-examples"><strong>Enables:</strong> Computer use, UI testing, accessibility</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸï¸</div>
                        <div class="usecase-title">Scene Understanding</div>
                    </div>
                    <p class="usecase-desc">Describe photos, identify objects, understand spatial relationships, detect activities. General visual comprehension.</p>
                    <div class="usecase-examples"><strong>Applications:</strong> Accessibility, content moderation, search</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ”¢</div>
                        <div class="usecase-title">Math & Science</div>
                    </div>
                    <p class="usecase-desc">Read mathematical notation, understand scientific diagrams, interpret formulas. Solve problems shown in images.</p>
                    <div class="usecase-examples"><strong>Best for:</strong> GPT-4o, Claude, Gemini with math training</div>
                </div>
            </div>

            <div class="callout warning">
                <div class="callout-icon">âš ï¸</div>
                <div class="callout-content">
                    <h4>Vision Limitations</h4>
                    <p>Vision models can hallucinate details, misread text (especially small or stylized), struggle with spatial reasoning, and fail on very high-resolution images. <strong>Always validate critical extractions</strong>. Don't trust vision OCR for legal or financial documents without human review.</p>
                </div>
            </div>
        </section>

        <!-- AUDIO & SPEECH -->
        <section class="module" id="audio">
            <div class="module-header">
                <div class="module-icon">ğŸ¤</div>
                <div class="module-info">
                    <h2>Audio & Speech</h2>
                    <p>Understanding and generating spoken content</p>
                </div>
            </div>

            <div class="overview-content">
                <div>
                    <h3>Speech-to-Text (ASR)</h3>
                    <p>Automatic Speech Recognition has reached human-level accuracy for many languages. <strong>OpenAI's Whisper</strong> democratized high-quality transcription as an open-source model.</p>
                    <ul>
                        <li><strong>Whisper</strong> â€” Open source, 99 languages, open weights</li>
                        <li><strong>Deepgram</strong> â€” API service, real-time, enterprise features</li>
                        <li><strong>AssemblyAI</strong> â€” High accuracy, speaker diarization</li>
                        <li><strong>Google Speech-to-Text</strong> â€” 125 languages, streaming</li>
                        <li><strong>Azure Speech</strong> â€” Enterprise, custom models</li>
                    </ul>
                </div>
                <div>
                    <h3>Text-to-Speech (TTS)</h3>
                    <p>Voice synthesis has become remarkably natural. The best models are nearly indistinguishable from human speech.</p>
                    <ul>
                        <li><strong>ElevenLabs</strong> â€” Best quality, voice cloning, emotions</li>
                        <li><strong>OpenAI TTS</strong> â€” High quality, API access, fast</li>
                        <li><strong>Play.ht</strong> â€” Ultra-realistic, many voices</li>
                        <li><strong>LMNT</strong> â€” Low latency, conversational</li>
                        <li><strong>Cartesia</strong> â€” Realtime streaming TTS</li>
                    </ul>
                </div>
            </div>

            <div class="models-grid">
                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">Whisper Large v3</div>
                            <div class="model-company">OpenAI</div>
                        </div>
                        <span class="model-badge audio">ASR</span>
                    </div>
                    <p class="model-desc">Open-source speech recognition model. 99 languages. Excellent accuracy even on noisy audio. The standard for transcription.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">Open Source</span>
                        <span class="model-spec">99 Languages</span>
                        <span class="model-spec">1.5B params</span>
                    </div>
                </div>

                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">ElevenLabs</div>
                            <div class="model-company">ElevenLabs</div>
                        </div>
                        <span class="model-badge audio">TTS</span>
                    </div>
                    <p class="model-desc">Industry-leading voice synthesis. Voice cloning from minutes of audio. Emotional control. Used in audiobooks, games, podcasts.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">Best Quality</span>
                        <span class="model-spec">Voice Cloning</span>
                        <span class="model-spec">29 Languages</span>
                    </div>
                </div>
            </div>
        </section>

        <!-- VIDEO UNDERSTANDING -->
        <section class="module" id="video">
            <div class="module-header">
                <div class="module-icon">ğŸ¬</div>
                <div class="module-info">
                    <h2>Video Understanding</h2>
                    <p>AI that can watch and comprehend video content</p>
                </div>
            </div>

            <div class="callout info">
                <div class="callout-icon">ğŸ¥</div>
                <div class="callout-content">
                    <h4>The Next Frontier</h4>
                    <p>Video understanding combines vision, audio, and temporal reasoningâ€”the most complex multimodal task. <strong>Gemini 1.5/2.0</strong> leads with native video input, processing up to hours of footage. Other models handle video by sampling frames, losing temporal continuity.</p>
                </div>
            </div>

            <div class="overview-content">
                <div>
                    <h3>Current Capabilities</h3>
                    <ul>
                        <li><strong>Content description</strong> â€” Summarize what happens in a video</li>
                        <li><strong>Temporal Q&A</strong> â€” Answer questions about specific moments</li>
                        <li><strong>Action recognition</strong> â€” Identify activities and events</li>
                        <li><strong>Scene segmentation</strong> â€” Break video into logical segments</li>
                        <li><strong>Object tracking</strong> â€” Follow objects through the video</li>
                        <li><strong>Audio-visual alignment</strong> â€” Connect speech with visual content</li>
                    </ul>
                </div>
                <div>
                    <h3>Leading Models</h3>
                    <ul>
                        <li><strong>Gemini 2.0 Flash</strong> â€” Native video, 1M+ context, hours of video</li>
                        <li><strong>Gemini 1.5 Pro</strong> â€” Up to 1 hour of video analysis</li>
                        <li><strong>GPT-4o</strong> â€” Video via frame extraction</li>
                        <li><strong>Claude 3.5</strong> â€” Video via frame extraction</li>
                        <li><strong>Twelve Labs</strong> â€” Specialized video understanding API</li>
                        <li><strong>VideoLLaVA</strong> â€” Open source video-language model</li>
                    </ul>
                </div>
            </div>

            <div class="usecase-grid">
                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“º</div>
                        <div class="usecase-title">Content Moderation</div>
                    </div>
                    <p class="usecase-desc">Automatically review user-generated video for policy violations, inappropriate content, or copyright issues at scale.</p>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“¹</div>
                        <div class="usecase-title">Meeting Summarization</div>
                    </div>
                    <p class="usecase-desc">Process recorded meetings to extract key points, action items, and decisions. Understand who said what and when.</p>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“</div>
                        <div class="usecase-title">Educational Content</div>
                    </div>
                    <p class="usecase-desc">Analyze lectures, create chapter markers, generate quizzes from video content, enable semantic search within videos.</p>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ›¡ï¸</div>
                        <div class="usecase-title">Security & Surveillance</div>
                    </div>
                    <p class="usecase-desc">Monitor video feeds for anomalies, detect specific events, enable natural language queries over footage.</p>
                </div>
            </div>
        </section>

        <!-- IMAGE GENERATION -->
        <section class="module" id="image-gen">
            <div class="module-header">
                <div class="module-icon">ğŸ–¼ï¸</div>
                <div class="module-info">
                    <h2>Image Generation</h2>
                    <p>Creating images from text descriptions</p>
                </div>
            </div>

            <div class="callout pink">
                <div class="callout-icon">ğŸ¨</div>
                <div class="callout-content">
                    <h4>The Creative Revolution</h4>
                    <p>Image generation went from research curiosity to mainstream tool in just 2 years. DALL-E 2 (2022) sparked the revolution; Midjourney captured the creative community; Stable Diffusion democratized the technology. Today, AI-generated images are everywhereâ€”marketing, products, entertainment. <strong>The quality is now photorealistic.</strong></p>
                </div>
            </div>

            <div class="gen-grid">
                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">DALL-E 3</div>
                            <div class="gen-company">OpenAI</div>
                        </div>
                        <span class="gen-badge image">Image</span>
                    </div>
                    <p class="gen-desc">Excellent prompt following, text rendering, integrated with ChatGPT. Best for precise, instruction-following generation.</p>
                    <div class="gen-features">
                        <span class="gen-feature">Text in Images</span>
                        <span class="gen-feature">Prompt Adherence</span>
                        <span class="gen-feature">Safe by Default</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Midjourney v6.1</div>
                            <div class="gen-company">Midjourney</div>
                        </div>
                        <span class="gen-badge image">Image</span>
                    </div>
                    <p class="gen-desc">Stunning aesthetic quality. Best for artistic, creative, and visually striking images. Strong community and style development.</p>
                    <div class="gen-features">
                        <span class="gen-feature">Artistic Quality</span>
                        <span class="gen-feature">Aesthetic Control</span>
                        <span class="gen-feature">Style References</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Stable Diffusion 3.5</div>
                            <div class="gen-company">Stability AI</div>
                        </div>
                        <span class="gen-badge image">Image</span>
                    </div>
                    <p class="gen-desc">Open weights, highly customizable, massive ecosystem. Best for fine-tuning, control, and local deployment.</p>
                    <div class="gen-features">
                        <span class="gen-feature">Open Source</span>
                        <span class="gen-feature">ControlNet</span>
                        <span class="gen-feature">Fine-tuning</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Ideogram 2.0</div>
                            <div class="gen-company">Ideogram</div>
                        </div>
                        <span class="gen-badge image">Image</span>
                    </div>
                    <p class="gen-desc">Best-in-class text rendering. Creates images with readable, accurate text. Great for marketing, logos, posters.</p>
                    <div class="gen-features">
                        <span class="gen-feature">Text Rendering</span>
                        <span class="gen-feature">Typography</span>
                        <span class="gen-feature">Logos</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Flux 1.1 Pro</div>
                            <div class="gen-company">Black Forest Labs</div>
                        </div>
                        <span class="gen-badge image">Image</span>
                    </div>
                    <p class="gen-desc">From Stable Diffusion creators. Excellent quality, fast inference. Pro and Dev versions available. New standard for quality.</p>
                    <div class="gen-features">
                        <span class="gen-feature">High Quality</span>
                        <span class="gen-feature">Fast</span>
                        <span class="gen-feature">Open Weights</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Imagen 3</div>
                            <div class="gen-company">Google</div>
                        </div>
                        <span class="gen-badge image">Image</span>
                    </div>
                    <p class="gen-desc">Google's latest image generator. Photorealistic quality, strong prompt understanding. Available via Vertex AI.</p>
                    <div class="gen-features">
                        <span class="gen-feature">Photorealistic</span>
                        <span class="gen-feature">Google Cloud</span>
                        <span class="gen-feature">Enterprise</span>
                    </div>
                </div>
            </div>

            <div class="compare-table">
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Best For</th>
                        <th>Text Rendering</th>
                        <th>Photorealism</th>
                        <th>Artistic</th>
                        <th>Open</th>
                        <th>Price/Image</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="model-name-cell">DALL-E 3</td>
                        <td>Precise prompts, integration</td>
                        <td class="check">âœ“âœ“</td>
                        <td class="check">âœ“âœ“</td>
                        <td class="check">âœ“</td>
                        <td class="none">âœ—</td>
                        <td>$0.04-0.12</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Midjourney v6.1</td>
                        <td>Artistic, creative work</td>
                        <td class="partial">~</td>
                        <td class="check">âœ“âœ“</td>
                        <td class="check best">âœ“âœ“âœ“</td>
                        <td class="none">âœ—</td>
                        <td>$0.01-0.10</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Stable Diffusion 3.5</td>
                        <td>Customization, control</td>
                        <td class="check">âœ“</td>
                        <td class="check">âœ“âœ“</td>
                        <td class="check">âœ“âœ“</td>
                        <td class="check best">âœ“âœ“âœ“</td>
                        <td>Free (self-host)</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Ideogram 2.0</td>
                        <td>Text, typography, logos</td>
                        <td class="check best">âœ“âœ“âœ“</td>
                        <td class="check">âœ“âœ“</td>
                        <td class="check">âœ“</td>
                        <td class="none">âœ—</td>
                        <td>$0.01-0.08</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Flux 1.1 Pro</td>
                        <td>Quality + speed balance</td>
                        <td class="check">âœ“âœ“</td>
                        <td class="check">âœ“âœ“âœ“</td>
                        <td class="check">âœ“âœ“</td>
                        <td class="check">âœ“</td>
                        <td>$0.04</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Imagen 3</td>
                        <td>Enterprise, GCP integration</td>
                        <td class="check">âœ“âœ“</td>
                        <td class="check">âœ“âœ“âœ“</td>
                        <td class="check">âœ“âœ“</td>
                        <td class="none">âœ—</td>
                        <td>$0.02-0.04</td>
                    </tr>
                </tbody>
            </table>

            <div class="overview-content" style="margin-top: 24px;">
                <div>
                    <h3>Image Generation Techniques</h3>
                    <ul>
                        <li><strong>Text-to-Image</strong> â€” Generate from text prompt</li>
                        <li><strong>Image-to-Image</strong> â€” Transform existing image based on prompt</li>
                        <li><strong>Inpainting</strong> â€” Edit specific regions while preserving rest</li>
                        <li><strong>Outpainting</strong> â€” Extend image beyond original boundaries</li>
                        <li><strong>ControlNet</strong> â€” Guide generation with poses, edges, depth maps</li>
                        <li><strong>Style Transfer</strong> â€” Apply artistic styles to images</li>
                    </ul>
                </div>
                <div>
                    <h3>Choosing the Right Tool</h3>
                    <ul>
                        <li><strong>Marketing graphics</strong> â†’ Ideogram (text), DALL-E (precision)</li>
                        <li><strong>Artistic/creative</strong> â†’ Midjourney (aesthetic), Flux (quality)</li>
                        <li><strong>Product mockups</strong> â†’ DALL-E, Stable Diffusion with ControlNet</li>
                        <li><strong>Consistent characters</strong> â†’ Midjourney with --cref, custom LoRAs</li>
                        <li><strong>Enterprise/API</strong> â†’ DALL-E, Imagen 3, Flux API</li>
                        <li><strong>Self-hosting</strong> â†’ Stable Diffusion, Flux Dev</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- VIDEO GENERATION -->
        <section class="module" id="video-gen">
            <div class="module-header">
                <div class="module-icon">ğŸ“¹</div>
                <div class="module-info">
                    <h2>Video Generation</h2>
                    <p>Creating video from text or images</p>
                </div>
            </div>

            <div class="callout warning">
                <div class="callout-icon">ğŸ¬</div>
                <div class="callout-content">
                    <h4>The 2024 Breakthrough</h4>
                    <p>Video generation went from choppy clips to cinematic quality in a single year. <strong>OpenAI's Sora</strong> (Feb 2024) demonstrated minute-long, photorealistic videos. While not publicly released, it showed what's possible. Runway Gen-3, Pika, and others are now delivering production-quality video generation.</p>
                </div>
            </div>

            <div class="gen-grid">
                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Sora</div>
                            <div class="gen-company">OpenAI</div>
                        </div>
                        <span class="gen-badge video">Video</span>
                    </div>
                    <p class="gen-desc">State-of-the-art video generation. Up to 1 minute, photorealistic, complex scenes. Limited access, red teaming phase.</p>
                    <div class="gen-features">
                        <span class="gen-feature">1 min length</span>
                        <span class="gen-feature">Photorealistic</span>
                        <span class="gen-feature">Limited Access</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Runway Gen-3 Alpha</div>
                            <div class="gen-company">Runway</div>
                        </div>
                        <span class="gen-badge video">Video</span>
                    </div>
                    <p class="gen-desc">Production-ready video generation. 10-second clips, good quality, fast turnaround. Industry standard for creative work.</p>
                    <div class="gen-features">
                        <span class="gen-feature">10 sec clips</span>
                        <span class="gen-feature">Fast</span>
                        <span class="gen-feature">Available Now</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Pika 1.0</div>
                            <div class="gen-company">Pika Labs</div>
                        </div>
                        <span class="gen-badge video">Video</span>
                    </div>
                    <p class="gen-desc">Fast, accessible video generation. Good for quick iterations and social content. Free tier available.</p>
                    <div class="gen-features">
                        <span class="gen-feature">Fast Gen</span>
                        <span class="gen-feature">Free Tier</span>
                        <span class="gen-feature">Social Content</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Kling</div>
                            <div class="gen-company">Kuaishou</div>
                        </div>
                        <span class="gen-badge video">Video</span>
                    </div>
                    <p class="gen-desc">Chinese video model rivaling Sora quality. 2 minute videos, strong physics understanding. Limited international access.</p>
                    <div class="gen-features">
                        <span class="gen-feature">2 min length</span>
                        <span class="gen-feature">High Quality</span>
                        <span class="gen-feature">China-focused</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Veo 2</div>
                            <div class="gen-company">Google DeepMind</div>
                        </div>
                        <span class="gen-badge video">Video</span>
                    </div>
                    <p class="gen-desc">Google's video generation model. High quality, good physics. Available through VideoFX and Vertex AI.</p>
                    <div class="gen-features">
                        <span class="gen-feature">High Quality</span>
                        <span class="gen-feature">Physics</span>
                        <span class="gen-feature">Google Cloud</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Stable Video</div>
                            <div class="gen-company">Stability AI</div>
                        </div>
                        <span class="gen-badge video">Video</span>
                    </div>
                    <p class="gen-desc">Open weights video generation. Image-to-video, shorter clips. Good for developers and customization.</p>
                    <div class="gen-features">
                        <span class="gen-feature">Open Weights</span>
                        <span class="gen-feature">Img2Vid</span>
                        <span class="gen-feature">Customizable</span>
                    </div>
                </div>
            </div>
        </section>

        <!-- AUDIO GENERATION -->
        <section class="module" id="audio-gen">
            <div class="module-header">
                <div class="module-icon">ğŸ”Š</div>
                <div class="module-info">
                    <h2>Audio Generation</h2>
                    <p>Music, sound effects, and voice synthesis</p>
                </div>
            </div>

            <div class="gen-grid">
                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Suno v3.5</div>
                            <div class="gen-company">Suno</div>
                        </div>
                        <span class="gen-badge music">Music</span>
                    </div>
                    <p class="gen-desc">Full songs from text prompts. Lyrics, vocals, instrumentals. Viral hits generated daily. Best all-around music generation.</p>
                    <div class="gen-features">
                        <span class="gen-feature">Full Songs</span>
                        <span class="gen-feature">Vocals</span>
                        <span class="gen-feature">Any Genre</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Udio</div>
                            <div class="gen-company">Udio</div>
                        </div>
                        <span class="gen-badge music">Music</span>
                    </div>
                    <p class="gen-desc">High-quality music generation. Strong vocal quality, good genre diversity. Suno's main competitor.</p>
                    <div class="gen-features">
                        <span class="gen-feature">High Quality</span>
                        <span class="gen-feature">Great Vocals</span>
                        <span class="gen-feature">Genre Control</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">ElevenLabs</div>
                            <div class="gen-company">ElevenLabs</div>
                        </div>
                        <span class="gen-badge voice">Voice</span>
                    </div>
                    <p class="gen-desc">Best-in-class voice synthesis. Clone voices from samples. Emotional control. Industry standard for TTS.</p>
                    <div class="gen-features">
                        <span class="gen-feature">Voice Clone</span>
                        <span class="gen-feature">Emotions</span>
                        <span class="gen-feature">Multi-language</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">Stable Audio</div>
                            <div class="gen-company">Stability AI</div>
                        </div>
                        <span class="gen-badge audio">Audio</span>
                    </div>
                    <p class="gen-desc">Sound effects and music from text. Open approach, good for loops and ambient. Production-ready audio assets.</p>
                    <div class="gen-features">
                        <span class="gen-feature">Sound Effects</span>
                        <span class="gen-feature">Loops</span>
                        <span class="gen-feature">Open Model</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">MusicGen</div>
                            <div class="gen-company">Meta</div>
                        </div>
                        <span class="gen-badge music">Music</span>
                    </div>
                    <p class="gen-desc">Open-source music generation. Good quality instrumentals. Research-friendly, customizable.</p>
                    <div class="gen-features">
                        <span class="gen-feature">Open Source</span>
                        <span class="gen-feature">Instrumentals</span>
                        <span class="gen-feature">Controllable</span>
                    </div>
                </div>

                <div class="gen-card">
                    <div class="gen-header">
                        <div>
                            <div class="gen-name">OpenAI TTS</div>
                            <div class="gen-company">OpenAI</div>
                        </div>
                        <span class="gen-badge voice">Voice</span>
                    </div>
                    <p class="gen-desc">High-quality text-to-speech via API. Multiple voices, fast generation. Easy integration with OpenAI stack.</p>
                    <div class="gen-features">
                        <span class="gen-feature">API Access</span>
                        <span class="gen-feature">Fast</span>
                        <span class="gen-feature">Multi-voice</span>
                    </div>
                </div>
            </div>
        </section>

        <!-- NATIVE MULTIMODAL MODELS -->
        <section class="module" id="native">
            <div class="module-header">
                <div class="module-icon">ğŸŒ</div>
                <div class="module-info">
                    <h2>Native Multimodal Models</h2>
                    <p>Models trained on all modalities from the ground up</p>
                </div>
            </div>

            <div class="callout success">
                <div class="callout-icon">ğŸ”®</div>
                <div class="callout-content">
                    <h4>The Future is Native</h4>
                    <p>Early multimodal models bolted vision onto text models. New architectures are <strong>natively multimodal</strong>â€”trained on text, images, audio, and video together from the start. This creates deeper cross-modal understanding and enables generation across modalities. GPT-4o and Gemini 2.0 represent this new paradigm.</p>
                </div>
            </div>

            <div class="models-grid">
                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">GPT-4o ("Omni")</div>
                            <div class="model-company">OpenAI</div>
                        </div>
                        <span class="model-badge native">Native Multimodal</span>
                    </div>
                    <p class="model-desc">First native multimodal model from OpenAI. Processes text, images, and audio in a unified architecture. Powers ChatGPT voice mode and vision features.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">Text + Image + Audio</span>
                        <span class="model-spec">128K context</span>
                        <span class="model-spec">Realtime capable</span>
                    </div>
                    <div class="model-capabilities">
                        <span class="capability-tag strong">Voice Mode</span>
                        <span class="capability-tag strong">Vision</span>
                        <span class="capability-tag">Realtime</span>
                        <span class="capability-tag">Unified</span>
                    </div>
                </div>

                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">Gemini 2.0</div>
                            <div class="model-company">Google DeepMind</div>
                        </div>
                        <span class="model-badge native">Native Multimodal</span>
                    </div>
                    <p class="model-desc">Google's most capable multimodal model. Native audio, video, and image understanding. Massive context for long-form content. Powers agentic features.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">Text + Image + Audio + Video</span>
                        <span class="model-spec">1M+ context</span>
                        <span class="model-spec">Agentic</span>
                    </div>
                    <div class="model-capabilities">
                        <span class="capability-tag strong">Long Video</span>
                        <span class="capability-tag strong">Native Audio</span>
                        <span class="capability-tag">Tool Use</span>
                        <span class="capability-tag">Grounding</span>
                    </div>
                </div>
            </div>

            <div class="overview-content">
                <div>
                    <h3>Why Native Matters</h3>
                    <ul>
                        <li><strong>Deeper understanding</strong> â€” Cross-modal connections learned during training</li>
                        <li><strong>Better coherence</strong> â€” All modalities in single forward pass</li>
                        <li><strong>Lower latency</strong> â€” No separate encoding/decoding steps</li>
                        <li><strong>Emergent abilities</strong> â€” New capabilities from joint training</li>
                        <li><strong>Simpler architecture</strong> â€” One model, not multiple stitched together</li>
                    </ul>
                </div>
                <div>
                    <h3>Coming Soon</h3>
                    <ul>
                        <li><strong>GPT-5</strong> â€” Expected to be fully native multimodal</li>
                        <li><strong>Claude 4</strong> â€” Likely native multimodal capabilities</li>
                        <li><strong>Open source native</strong> â€” Chameleon, LLaVA-Next evolution</li>
                        <li><strong>Video generation</strong> â€” Integration into chat models</li>
                        <li><strong>3D understanding</strong> â€” Spatial reasoning capabilities</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- REALTIME APIs -->
        <section class="module" id="realtime">
            <div class="module-header">
                <div class="module-icon">âš¡</div>
                <div class="module-info">
                    <h2>Realtime Multimodal APIs</h2>
                    <p>Low-latency voice and video interaction</p>
                </div>
            </div>

            <div class="callout info">
                <div class="callout-icon">ğŸ™ï¸</div>
                <div class="callout-content">
                    <h4>The Voice Interface Revolution</h4>
                    <p>Realtime APIs enable <strong>natural voice conversations</strong> with AIâ€”no more type, wait, read. Speak naturally, get spoken responses, interrupt mid-sentence. This is how humans communicate. OpenAI's Realtime API and Gemini Live are making this production-ready.</p>
                </div>
            </div>

            <div class="models-grid">
                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">OpenAI Realtime API</div>
                            <div class="model-company">OpenAI</div>
                        </div>
                        <span class="model-badge native">Realtime</span>
                    </div>
                    <p class="model-desc">WebSocket-based voice conversations with GPT-4o. Sub-second latency, natural interruption, function calling. Powers voice assistants and call centers.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">~300ms latency</span>
                        <span class="model-spec">WebSocket</span>
                        <span class="model-spec">Function calling</span>
                    </div>
                    <div class="model-capabilities">
                        <span class="capability-tag strong">Voice I/O</span>
                        <span class="capability-tag strong">Interruption</span>
                        <span class="capability-tag">Tool Use</span>
                        <span class="capability-tag">Emotions</span>
                    </div>
                </div>

                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">Gemini Live</div>
                            <div class="model-company">Google</div>
                        </div>
                        <span class="model-badge native">Realtime</span>
                    </div>
                    <p class="model-desc">Realtime voice and video conversation with Gemini. Camera integration for visual context. Available on Android, expanding to more platforms.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">Voice + Camera</span>
                        <span class="model-spec">Mobile Native</span>
                        <span class="model-spec">Visual Context</span>
                    </div>
                    <div class="model-capabilities">
                        <span class="capability-tag strong">Voice + Vision</span>
                        <span class="capability-tag strong">Mobile</span>
                        <span class="capability-tag">Always On</span>
                        <span class="capability-tag">Context</span>
                    </div>
                </div>
            </div>

            <div class="overview-content">
                <div>
                    <h3>Technical Requirements</h3>
                    <ul>
                        <li><strong>WebSocket connection</strong> â€” Persistent bidirectional communication</li>
                        <li><strong>Audio streaming</strong> â€” PCM16, 24kHz typical</li>
                        <li><strong>Voice Activity Detection</strong> â€” Know when user is speaking</li>
                        <li><strong>Interruption handling</strong> â€” Cancel generation mid-stream</li>
                        <li><strong>Client-side audio</strong> â€” Microphone access, speaker output</li>
                    </ul>
                </div>
                <div>
                    <h3>Use Cases</h3>
                    <ul>
                        <li><strong>Voice assistants</strong> â€” Natural conversational AI</li>
                        <li><strong>Call centers</strong> â€” AI-powered customer support</li>
                        <li><strong>Tutoring</strong> â€” Interactive learning conversations</li>
                        <li><strong>Accessibility</strong> â€” Voice interface for all applications</li>
                        <li><strong>Gaming</strong> â€” NPCs with natural conversation</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- MODEL COMPARISON -->
        <section class="module" id="comparison">
            <div class="module-header">
                <div class="module-icon">ğŸ“Š</div>
                <div class="module-info">
                    <h2>Multimodal Model Comparison</h2>
                    <p>Capabilities across major providers</p>
                </div>
            </div>

            <table class="compare-table">
                <thead>
                    <tr>
                        <th>Capability</th>
                        <th>GPT-4o</th>
                        <th>Claude 3.5</th>
                        <th>Gemini 2.0</th>
                        <th>Llama 3.2</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="model-name-cell">Image Input</td>
                        <td class="check">âœ“âœ“âœ“</td>
                        <td class="check">âœ“âœ“âœ“</td>
                        <td class="check">âœ“âœ“âœ“</td>
                        <td class="check">âœ“âœ“</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Audio Input</td>
                        <td class="check best">âœ“âœ“âœ“</td>
                        <td class="none">âœ—</td>
                        <td class="check">âœ“âœ“âœ“</td>
                        <td class="none">âœ—</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Video Input</td>
                        <td class="partial">~ (frames)</td>
                        <td class="partial">~ (frames)</td>
                        <td class="check best">âœ“âœ“âœ“</td>
                        <td class="none">âœ—</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Document OCR</td>
                        <td class="check">âœ“âœ“âœ“</td>
                        <td class="check best">âœ“âœ“âœ“</td>
                        <td class="check">âœ“âœ“âœ“</td>
                        <td class="check">âœ“âœ“</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Chart Analysis</td>
                        <td class="check">âœ“âœ“âœ“</td>
                        <td class="check best">âœ“âœ“âœ“</td>
                        <td class="check">âœ“âœ“âœ“</td>
                        <td class="check">âœ“âœ“</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Realtime Voice</td>
                        <td class="check best">âœ“âœ“âœ“</td>
                        <td class="none">âœ—</td>
                        <td class="check">âœ“âœ“âœ“</td>
                        <td class="none">âœ—</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Max Context</td>
                        <td>128K</td>
                        <td>200K</td>
                        <td class="best">1M+</td>
                        <td>128K</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Open Weights</td>
                        <td class="none">âœ—</td>
                        <td class="none">âœ—</td>
                        <td class="none">âœ—</td>
                        <td class="check best">âœ“âœ“âœ“</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- USE CASES -->
        <section class="module" id="usecases">
            <div class="module-header">
                <div class="module-icon">ğŸ’¼</div>
                <div class="module-info">
                    <h2>Enterprise Use Cases</h2>
                    <p>Where multimodal AI delivers value</p>
                </div>
            </div>

            <div class="usecase-grid">
                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“„</div>
                        <div class="usecase-title">Document Processing</div>
                    </div>
                    <p class="usecase-desc">Extract data from invoices, contracts, forms. Handle mixed text, tables, images, signatures. Process any document type without templates.</p>
                    <div class="usecase-examples"><strong>ROI:</strong> 80% reduction in manual data entry</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ›’</div>
                        <div class="usecase-title">E-commerce</div>
                    </div>
                    <p class="usecase-desc">Visual search ("find similar products"), virtual try-on, automated product photography, catalog enrichment from images.</p>
                    <div class="usecase-examples"><strong>ROI:</strong> 30% improvement in product discovery</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ¥</div>
                        <div class="usecase-title">Healthcare</div>
                    </div>
                    <p class="usecase-desc">Medical image analysis, radiology assistance, pathology screening, clinical documentation from dictation.</p>
                    <div class="usecase-examples"><strong>ROI:</strong> 40% faster diagnosis workflows</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ­</div>
                        <div class="usecase-title">Manufacturing</div>
                    </div>
                    <p class="usecase-desc">Visual quality inspection, defect detection, assembly verification, maintenance documentation from photos.</p>
                    <div class="usecase-examples"><strong>ROI:</strong> 90% accuracy in defect detection</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“¢</div>
                        <div class="usecase-title">Marketing & Creative</div>
                    </div>
                    <p class="usecase-desc">Generate product images, create ad variations, produce social content, automate video editing, podcast production.</p>
                    <div class="usecase-examples"><strong>ROI:</strong> 10x content output at same cost</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“</div>
                        <div class="usecase-title">Customer Support</div>
                    </div>
                    <p class="usecase-desc">Voice-based support agents, visual troubleshooting ("show me the error"), image-based product identification.</p>
                    <div class="usecase-examples"><strong>ROI:</strong> 60% faster resolution with images</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“</div>
                        <div class="usecase-title">Education</div>
                    </div>
                    <p class="usecase-desc">Visual explanations of concepts, diagram generation, voice tutoring, video lecture summarization, interactive learning.</p>
                    <div class="usecase-examples"><strong>ROI:</strong> Personalized learning at scale</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">â™¿</div>
                        <div class="usecase-title">Accessibility</div>
                    </div>
                    <p class="usecase-desc">Auto-generate alt text, describe images for visually impaired, transcribe audio, voice interfaces for motor impairments.</p>
                    <div class="usecase-examples"><strong>Impact:</strong> Democratize access to content</div>
                </div>
            </div>
        </section>

        <!-- IMPLEMENTATION GUIDE -->
        <section class="module" id="implementation">
            <div class="module-header">
                <div class="module-icon">ğŸ› ï¸</div>
                <div class="module-info">
                    <h2>Implementation Guide</h2>
                    <p>How to build with multimodal APIs</p>
                </div>
            </div>

            <div class="overview-content">
                <div>
                    <h3>Image Input Basics</h3>
                    <p>Most APIs accept images as base64-encoded strings or URLs. Send alongside your text prompt. Be mindful of:</p>
                    <ul>
                        <li><strong>Image size limits</strong> â€” Usually 20MB max, resize if needed</li>
                        <li><strong>Token costs</strong> â€” Images consume tokens (varies by model)</li>
                        <li><strong>Resolution</strong> â€” Higher res = more tokens, better detail</li>
                        <li><strong>Multiple images</strong> â€” Most support multiple per request</li>
                    </ul>
                </div>
                <div>
                    <h3>Audio Integration</h3>
                    <p>For audio input/output, consider:</p>
                    <ul>
                        <li><strong>Transcription first</strong> â€” Whisper for ASR, then text LLM</li>
                        <li><strong>Native audio</strong> â€” GPT-4o/Gemini for end-to-end</li>
                        <li><strong>TTS output</strong> â€” ElevenLabs/OpenAI TTS for speech</li>
                        <li><strong>Realtime</strong> â€” WebSocket APIs for conversation</li>
                    </ul>
                </div>
            </div>

            <div class="agent-code" style="margin-top: 24px;">
                <div class="code-header">multimodal_examples.py - Working with Multimodal APIs</div>
                <div class="code-content">
<pre><span class="code-comment"># Example: Vision with Claude</span>
<span class="code-keyword">import</span> anthropic
<span class="code-keyword">import</span> base64

client = anthropic.Anthropic()

<span class="code-comment"># Encode image to base64</span>
<span class="code-keyword">with</span> open(<span class="code-string">"document.png"</span>, <span class="code-string">"rb"</span>) <span class="code-keyword">as</span> f:
    image_data = base64.standard_b64encode(f.read()).decode(<span class="code-string">"utf-8"</span>)

response = client.messages.create(
    model=<span class="code-string">"claude-sonnet-4-20250514"</span>,
    max_tokens=<span class="code-keyword">1024</span>,
    messages=[{
        <span class="code-string">"role"</span>: <span class="code-string">"user"</span>,
        <span class="code-string">"content"</span>: [
            {<span class="code-string">"type"</span>: <span class="code-string">"image"</span>, <span class="code-string">"source"</span>: {
                <span class="code-string">"type"</span>: <span class="code-string">"base64"</span>,
                <span class="code-string">"media_type"</span>: <span class="code-string">"image/png"</span>,
                <span class="code-string">"data"</span>: image_data
            }},
            {<span class="code-string">"type"</span>: <span class="code-string">"text"</span>, <span class="code-string">"text"</span>: <span class="code-string">"Extract all data from this invoice"</span>}
        ]
    }]
)

<span class="code-comment"># Example: Vision with GPT-4o</span>
<span class="code-keyword">from</span> openai <span class="code-keyword">import</span> OpenAI

client = OpenAI()

response = client.chat.completions.create(
    model=<span class="code-string">"gpt-4o"</span>,
    messages=[{
        <span class="code-string">"role"</span>: <span class="code-string">"user"</span>,
        <span class="code-string">"content"</span>: [
            {<span class="code-string">"type"</span>: <span class="code-string">"text"</span>, <span class="code-string">"text"</span>: <span class="code-string">"What's in this image?"</span>},
            {<span class="code-string">"type"</span>: <span class="code-string">"image_url"</span>, <span class="code-string">"image_url"</span>: {
                <span class="code-string">"url"</span>: <span class="code-string">"https://example.com/image.jpg"</span>
            }}
        ]
    }]
)

<span class="code-comment"># Example: Audio transcription with Whisper</span>
audio_file = open(<span class="code-string">"recording.mp3"</span>, <span class="code-string">"rb"</span>)
transcript = client.audio.transcriptions.create(
    model=<span class="code-string">"whisper-1"</span>,
    file=audio_file
)

<span class="code-comment"># Example: Text-to-Speech</span>
speech = client.audio.speech.create(
    model=<span class="code-string">"tts-1"</span>,
    voice=<span class="code-string">"alloy"</span>,
    input=<span class="code-string">"Hello! This is AI-generated speech."</span>
)
speech.stream_to_file(<span class="code-string">"output.mp3"</span>)</pre>
                </div>
            </div>
        </section>

        <!-- BEST PRACTICES -->
        <section class="module" id="practices">
            <div class="module-header">
                <div class="module-icon">âœ…</div>
                <div class="module-info">
                    <h2>Best Practices</h2>
                    <p>Guidelines for multimodal AI implementation</p>
                </div>
            </div>

            <div class="practices-grid">
                <div class="practice-card">
                    <div class="practice-num">1</div>
                    <div class="practice-content">
                        <h4>Optimize Image Resolution</h4>
                        <p>Higher resolution = more tokens = higher cost. Resize images to the minimum needed for your task. 1024px is often sufficient.</p>
                    </div>
                </div>
                <div class="practice-card">
                    <div class="practice-num">2</div>
                    <div class="practice-content">
                        <h4>Use Appropriate Models</h4>
                        <p>Don't use vision for text-only tasks. Don't use expensive models for simple image descriptions. Match capability to requirement.</p>
                    </div>
                </div>
                <div class="practice-card">
                    <div class="practice-num">3</div>
                    <div class="practice-content">
                        <h4>Validate Visual Extractions</h4>
                        <p>Vision models hallucinate. For critical data (financial, legal), always validate extracted information against source.</p>
                    </div>
                </div>
                <div class="practice-card">
                    <div class="practice-num">4</div>
                    <div class="practice-content">
                        <h4>Handle Multiple Modalities Gracefully</h4>
                        <p>Build systems that work when images fail to load, audio is unavailable, or only text is present. Graceful degradation.</p>
                    </div>
                </div>
                <div class="practice-card">
                    <div class="practice-num">5</div>
                    <div class="practice-content">
                        <h4>Consider Privacy</h4>
                        <p>Images and audio may contain PII. Ensure you have rights to process. Consider on-premise models for sensitive content.</p>
                    </div>
                </div>
                <div class="practice-card">
                    <div class="practice-num">6</div>
                    <div class="practice-content">
                        <h4>Cache Generated Assets</h4>
                        <p>Image/video generation is expensive. Cache outputs, use deterministic seeds when possible, avoid regenerating identical content.</p>
                    </div>
                </div>
                <div class="practice-card">
                    <div class="practice-num">7</div>
                    <div class="practice-content">
                        <h4>Prompt Engineering for Vision</h4>
                        <p>Be specific about what to look for. "Extract the total amount" beats "What's in this image?" for invoices.</p>
                    </div>
                </div>
                <div class="practice-card">
                    <div class="practice-num">8</div>
                    <div class="practice-content">
                        <h4>Test Across Variations</h4>
                        <p>Test with different image qualities, lighting, orientations. Vision models are sensitive to visual variations.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- MULTIMODAL COSTS -->
        <section class="module" id="costs">
            <div class="module-header">
                <div class="module-icon">ğŸ’°</div>
                <div class="module-info">
                    <h2>Multimodal Costs & Token Economics</h2>
                    <p>Understanding the cost structure</p>
                </div>
            </div>

            <div class="callout warning">
                <div class="callout-icon">ğŸ’¸</div>
                <div class="callout-content">
                    <h4>Images Consume Tokens</h4>
                    <p>Vision models process images by converting them to tokens. A single high-resolution image can consume <strong>1,000-10,000+ tokens</strong>â€”equivalent to thousands of words of text. At scale, image processing can dominate your AI costs. Understanding token economics is critical.</p>
                </div>
            </div>

            <div class="compare-table">
                <thead>
                    <tr>
                        <th>Provider/Model</th>
                        <th>Image Input Cost</th>
                        <th>Token Calculation</th>
                        <th>Notes</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="model-name-cell">GPT-4o</td>
                        <td>$2.50/1M input tokens</td>
                        <td>85 tokens base + 170/512px tile</td>
                        <td>~765 tokens for 1024x1024</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Claude 3.5 Sonnet</td>
                        <td>$3.00/1M input tokens</td>
                        <td>~1,600 tokens per image (typical)</td>
                        <td>Varies with resolution</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Gemini 1.5 Pro</td>
                        <td>$1.25/1M input tokens</td>
                        <td>258 tokens per image</td>
                        <td>Most cost-effective for vision</td>
                    </tr>
                    <tr>
                        <td class="model-name-cell">Gemini 2.0 Flash</td>
                        <td>$0.10/1M input tokens</td>
                        <td>258 tokens per image</td>
                        <td>Best value for high volume</td>
                    </tr>
                </tbody>
            </table>

            <div class="overview-content" style="margin-top: 24px;">
                <div>
                    <h3>Cost Optimization Strategies</h3>
                    <ul>
                        <li><strong>Resize before sending</strong> â€” Most tasks don't need 4K images</li>
                        <li><strong>Use low-detail mode</strong> â€” OpenAI offers "low" detail for simple tasks</li>
                        <li><strong>Batch similar images</strong> â€” Process together to amortize overhead</li>
                        <li><strong>Choose the right model</strong> â€” Gemini Flash for volume, GPT-4o for precision</li>
                        <li><strong>Cache vision results</strong> â€” Don't reprocess the same image twice</li>
                        <li><strong>Pre-process with OCR</strong> â€” For text-heavy docs, traditional OCR may be cheaper</li>
                    </ul>
                </div>
                <div>
                    <h3>Generation Costs</h3>
                    <ul>
                        <li><strong>DALL-E 3</strong> â€” $0.04 (1024x1024) to $0.12 (HD)</li>
                        <li><strong>Midjourney</strong> â€” $0.01-0.10 per image (subscription)</li>
                        <li><strong>Stable Diffusion</strong> â€” Free if self-hosted (~$0.01/image on Replicate)</li>
                        <li><strong>Flux Pro</strong> â€” $0.04 per image via API</li>
                        <li><strong>Video (Runway)</strong> â€” $0.05 per second generated</li>
                        <li><strong>Voice (ElevenLabs)</strong> â€” $0.30/1K characters</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- OPEN SOURCE MULTIMODAL -->
        <section class="module" id="opensource">
            <div class="module-header">
                <div class="module-icon">ğŸ”“</div>
                <div class="module-info">
                    <h2>Open Source Multimodal</h2>
                    <p>Self-hostable multimodal models</p>
                </div>
            </div>

            <div class="models-grid">
                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">Llama 3.2 Vision</div>
                            <div class="model-company">Meta</div>
                        </div>
                        <span class="model-badge vision">Vision</span>
                    </div>
                    <p class="model-desc">Open-source vision-language models. 11B and 90B variants. Commercial use allowed. Strong baseline for self-hosting.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">11B / 90B</span>
                        <span class="model-spec">Commercial OK</span>
                        <span class="model-spec">Fine-tunable</span>
                    </div>
                </div>

                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">LLaVA-Next</div>
                            <div class="model-company">LLaVA Team</div>
                        </div>
                        <span class="model-badge vision">Vision</span>
                    </div>
                    <p class="model-desc">Popular open VLM family. Multiple size options. Strong community, many fine-tunes available. Research-friendly license.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">7B-70B</span>
                        <span class="model-spec">Dynamic Resolution</span>
                        <span class="model-spec">Many Variants</span>
                    </div>
                </div>

                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">Qwen2-VL</div>
                            <div class="model-company">Alibaba</div>
                        </div>
                        <span class="model-badge vision">Vision</span>
                    </div>
                    <p class="model-desc">Alibaba's vision-language model. Strong multilingual support. Video understanding. Competitive with proprietary models.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">2B-72B</span>
                        <span class="model-spec">Video</span>
                        <span class="model-spec">Multilingual</span>
                    </div>
                </div>

                <div class="model-card">
                    <div class="model-header">
                        <div>
                            <div class="model-name">InternVL2</div>
                            <div class="model-company">OpenGVLab</div>
                        </div>
                        <span class="model-badge vision">Vision</span>
                    </div>
                    <p class="model-desc">Open multimodal model rivaling GPT-4V. Strong OCR and document understanding. Active development.</p>
                    <div class="model-specs">
                        <span class="model-spec highlight">1B-76B</span>
                        <span class="model-spec">Strong OCR</span>
                        <span class="model-spec">Dynamic Res</span>
                    </div>
                </div>
            </div>

            <div class="overview-content" style="margin-top: 24px;">
                <div>
                    <h3>When to Self-Host</h3>
                    <ul>
                        <li><strong>Data privacy</strong> â€” Sensitive images that can't leave your infrastructure</li>
                        <li><strong>High volume</strong> â€” Processing millions of images where API costs dominate</li>
                        <li><strong>Low latency</strong> â€” Avoid network round-trips for real-time applications</li>
                        <li><strong>Customization</strong> â€” Fine-tuning for domain-specific vision tasks</li>
                        <li><strong>Offline operation</strong> â€” Edge deployments without internet access</li>
                    </ul>
                </div>
                <div>
                    <h3>Hardware Requirements</h3>
                    <ul>
                        <li><strong>7B-13B VLMs</strong> â€” Single GPU (24GB+ VRAM), A10G, RTX 4090</li>
                        <li><strong>34B-40B VLMs</strong> â€” 2x A100 40GB or 1x A100 80GB</li>
                        <li><strong>70B+ VLMs</strong> â€” 4-8x A100 80GB or H100</li>
                        <li><strong>Quantized (4-bit)</strong> â€” Reduce VRAM by 4x with some quality loss</li>
                        <li><strong>Stable Diffusion</strong> â€” Single GPU (8GB+), consumer hardware OK</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- PROMPT ENGINEERING FOR VISION -->
        <section class="module" id="prompting">
            <div class="module-header">
                <div class="module-icon">âœï¸</div>
                <div class="module-info">
                    <h2>Prompting for Multimodal</h2>
                    <p>Getting the best results from vision models</p>
                </div>
            </div>

            <div class="callout info">
                <div class="callout-icon">ğŸ’¡</div>
                <div class="callout-content">
                    <h4>Prompts Matter More for Vision</h4>
                    <p>Vision models can see everything in an image but need guidance on what to focus on. A vague prompt gets a vague answer. <strong>Specific prompts get specific results.</strong> Tell the model exactly what to look for, what format you want, and what to ignore.</p>
                </div>
            </div>

            <div class="usecase-grid">
                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“„</div>
                        <div class="usecase-title">Document Extraction</div>
                    </div>
                    <p class="usecase-desc"><strong>Bad:</strong> "What's in this document?"<br><strong>Good:</strong> "Extract the invoice number, date, vendor name, line items with quantities and prices, and total amount. Return as JSON."</p>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“Š</div>
                        <div class="usecase-title">Chart Analysis</div>
                    </div>
                    <p class="usecase-desc"><strong>Bad:</strong> "Describe this chart."<br><strong>Good:</strong> "This is a bar chart showing quarterly revenue. Extract all data points, identify the highest and lowest quarters, and calculate the year-over-year growth rate."</p>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ–¥ï¸</div>
                        <div class="usecase-title">UI Analysis</div>
                    </div>
                    <p class="usecase-desc"><strong>Bad:</strong> "What do you see?"<br><strong>Good:</strong> "This is a screenshot of a web application. Identify all clickable buttons, their labels, and their approximate positions (top-left, center, etc.)."</p>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ·ï¸</div>
                        <div class="usecase-title">Product Recognition</div>
                    </div>
                    <p class="usecase-desc"><strong>Bad:</strong> "What product is this?"<br><strong>Good:</strong> "Identify this product. Return: brand name, product name, visible text/model numbers, color, condition (new/used), and any visible damage."</p>
                </div>
            </div>

            <div class="overview-content" style="margin-top: 24px;">
                <div>
                    <h3>Vision Prompting Tips</h3>
                    <ul>
                        <li><strong>Be specific about output format</strong> â€” JSON, markdown table, bullet points</li>
                        <li><strong>Tell it what to focus on</strong> â€” "Pay attention to the header area"</li>
                        <li><strong>Provide context</strong> â€” "This is a medical form" vs just sending the image</li>
                        <li><strong>Ask for confidence</strong> â€” "Rate your confidence in each extraction"</li>
                        <li><strong>Handle uncertainty</strong> â€” "If text is unclear, mark as [illegible]"</li>
                    </ul>
                </div>
                <div>
                    <h3>Image Generation Prompting</h3>
                    <ul>
                        <li><strong>Subject first</strong> â€” Start with the main subject</li>
                        <li><strong>Style descriptors</strong> â€” "digital art", "photorealistic", "watercolor"</li>
                        <li><strong>Composition</strong> â€” "close-up", "wide shot", "centered"</li>
                        <li><strong>Lighting</strong> â€” "golden hour", "studio lighting", "dramatic shadows"</li>
                        <li><strong>Negative prompts</strong> â€” What to avoid (model-dependent)</li>
                        <li><strong>Reference images</strong> â€” Style references where supported</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- LIMITATIONS & CHALLENGES -->
        <section class="module" id="limitations">
            <div class="module-header">
                <div class="module-icon">âš ï¸</div>
                <div class="module-info">
                    <h2>Limitations & Challenges</h2>
                    <p>Understanding where multimodal AI falls short</p>
                </div>
            </div>

            <div class="callout warning">
                <div class="callout-icon">ğŸ¯</div>
                <div class="callout-content">
                    <h4>Know the Boundaries</h4>
                    <p>Multimodal AI is impressive but far from perfect. Understanding limitations helps you build realistic systems, set appropriate expectations, and implement necessary safeguards. <strong>Never assume perfect accuracy</strong> for any visual or audio task.</p>
                </div>
            </div>

            <div class="usecase-grid">
                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ‘»</div>
                        <div class="usecase-title">Visual Hallucinations</div>
                    </div>
                    <p class="usecase-desc">Models confidently describe things that aren't in images. They may invent text, add details, or miscount objects. Critical extractions require validation.</p>
                    <div class="usecase-examples"><strong>Risk:</strong> Financial documents, legal contracts, medical images</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ”¢</div>
                        <div class="usecase-title">Counting & Spatial Reasoning</div>
                    </div>
                    <p class="usecase-desc">Models struggle to accurately count objects, especially in crowded scenes. Spatial relationships ("left of", "behind") are often wrong.</p>
                    <div class="usecase-examples"><strong>Example:</strong> "How many people in this photo?" often incorrect</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ”¤</div>
                        <div class="usecase-title">Small/Stylized Text</div>
                    </div>
                    <p class="usecase-desc">OCR quality drops significantly for small text, unusual fonts, handwriting, rotated text, or text in complex backgrounds.</p>
                    <div class="usecase-examples"><strong>Workaround:</strong> Crop and zoom relevant regions</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ­</div>
                        <div class="usecase-title">Face/Person Recognition</div>
                    </div>
                    <p class="usecase-desc">Models refuse to identify specific individuals (by design). Describing people can be inconsistent or raise bias concerns.</p>
                    <div class="usecase-examples"><strong>Note:</strong> Privacy/safety guardrails are intentional</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ¬</div>
                        <div class="usecase-title">Video Temporal Gaps</div>
                    </div>
                    <p class="usecase-desc">Frame-sampling approaches miss fast actions. Even native video models struggle with precise temporal reasoning ("What happened at 2:34?").</p>
                    <div class="usecase-examples"><strong>Best model:</strong> Gemini 2.0 for video understanding</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ¨</div>
                        <div class="usecase-title">Generation Consistency</div>
                    </div>
                    <p class="usecase-desc">Maintaining consistent characters, objects, or styles across multiple generations is extremely difficult without specialized techniques.</p>
                    <div class="usecase-examples"><strong>Workaround:</strong> LoRAs, style references, seed control</div>
                </div>
            </div>

            <div class="overview-content" style="margin-top: 24px;">
                <div>
                    <h3>Image Generation Limitations</h3>
                    <ul>
                        <li><strong>Hands/fingers</strong> â€” Still often malformed or wrong count</li>
                        <li><strong>Text in images</strong> â€” Better but still imperfect (Ideogram best)</li>
                        <li><strong>Physics/anatomy</strong> â€” Impossible poses, broken physics</li>
                        <li><strong>Specific people</strong> â€” Can't reliably generate known individuals</li>
                        <li><strong>Exact replication</strong> â€” Can't exactly match reference images</li>
                        <li><strong>Fine control</strong> â€” Difficult to get precise compositions</li>
                    </ul>
                </div>
                <div>
                    <h3>Audio/Video Generation Limits</h3>
                    <ul>
                        <li><strong>Video length</strong> â€” Most limited to 10-60 seconds</li>
                        <li><strong>Temporal coherence</strong> â€” Objects/people may morph or vanish</li>
                        <li><strong>Audio sync</strong> â€” Lip sync in video generation is poor</li>
                        <li><strong>Music vocals</strong> â€” Often uncanny, lyrics unclear</li>
                        <li><strong>Sound effects</strong> â€” Limited realism for complex sounds</li>
                        <li><strong>Real-time generation</strong> â€” Not yet possible for video/audio</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- INTEGRATION PATTERNS -->
        <section class="module" id="integration">
            <div class="module-header">
                <div class="module-icon">ğŸ”—</div>
                <div class="module-info">
                    <h2>Integration Patterns</h2>
                    <p>How to integrate multimodal into your systems</p>
                </div>
            </div>

            <div class="usecase-grid">
                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ“‹</div>
                        <div class="usecase-title">Document Pipeline</div>
                    </div>
                    <p class="usecase-desc">Upload â†’ Pre-process (resize, enhance) â†’ Vision extraction â†’ Structured output â†’ Validation â†’ Database storage. Use queues for async processing at scale.</p>
                    <div class="usecase-examples"><strong>Tools:</strong> Vision API + Pydantic + PostgreSQL</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ”„</div>
                        <div class="usecase-title">Multimodal RAG</div>
                    </div>
                    <p class="usecase-desc">Index images/documents with vision-generated descriptions. Retrieve by semantic similarity. Include images in LLM context for grounded answers.</p>
                    <div class="usecase-examples"><strong>Tools:</strong> CLIP embeddings + Vector DB + VLM</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ™ï¸</div>
                        <div class="usecase-title">Voice Assistant</div>
                    </div>
                    <p class="usecase-desc">Audio input â†’ ASR (Whisper) â†’ LLM processing â†’ TTS output. Or use native realtime APIs for end-to-end voice with lower latency.</p>
                    <div class="usecase-examples"><strong>Tools:</strong> OpenAI Realtime API or Whisper + LLM + ElevenLabs</div>
                </div>

                <div class="usecase-card">
                    <div class="usecase-header">
                        <div class="usecase-icon">ğŸ–¼ï¸</div>
                        <div class="usecase-title">Content Generation</div>
                    </div>
                    <p class="usecase-desc">Text prompt â†’ LLM expands/refines prompt â†’ Image generation â†’ Quality check â†’ Storage/CDN. Use webhooks for async generation completion.</p>
                    <div class="usecase-examples"><strong>Tools:</strong> LLM + DALL-E/Midjourney API + S3/Cloudflare</div>
                </div>
            </div>

            <div class="agent-code" style="margin-top: 24px;">
                <div class="code-header">multimodal_pipeline.py - Production Pipeline Example</div>
                <div class="code-content">
<pre><span class="code-comment"># Production multimodal document processing pipeline</span>
<span class="code-keyword">from</span> anthropic <span class="code-keyword">import</span> Anthropic
<span class="code-keyword">from</span> pydantic <span class="code-keyword">import</span> BaseModel
<span class="code-keyword">import</span> base64
<span class="code-keyword">from</span> PIL <span class="code-keyword">import</span> Image
<span class="code-keyword">import</span> io

<span class="code-keyword">class</span> <span class="code-function">InvoiceData</span>(BaseModel):
    invoice_number: <span class="code-keyword">str</span>
    date: <span class="code-keyword">str</span>
    vendor: <span class="code-keyword">str</span>
    total: <span class="code-keyword">float</span>
    line_items: <span class="code-keyword">list</span>[<span class="code-keyword">dict</span>]
    confidence: <span class="code-keyword">float</span>

<span class="code-keyword">class</span> <span class="code-function">DocumentProcessor</span>:
    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self):
        self.client = Anthropic()
        self.max_size = (<span class="code-keyword">1568</span>, <span class="code-keyword">1568</span>)  <span class="code-comment"># Claude optimal</span>
    
    <span class="code-keyword">def</span> <span class="code-function">preprocess_image</span>(self, image_bytes: <span class="code-keyword">bytes</span>) -> <span class="code-keyword">str</span>:
        <span class="code-string">"""Resize and optimize image for vision API."""</span>
        img = Image.open(io.BytesIO(image_bytes))
        img.thumbnail(self.max_size, Image.Resampling.LANCZOS)
        
        buffer = io.BytesIO()
        img.save(buffer, format=<span class="code-string">"PNG"</span>, optimize=<span class="code-keyword">True</span>)
        <span class="code-keyword">return</span> base64.standard_b64encode(buffer.getvalue()).decode()
    
    <span class="code-keyword">async def</span> <span class="code-function">extract_invoice</span>(self, image_bytes: <span class="code-keyword">bytes</span>) -> InvoiceData:
        <span class="code-string">"""Extract structured data from invoice image."""</span>
        image_b64 = self.preprocess_image(image_bytes)
        
        response = self.client.messages.create(
            model=<span class="code-string">"claude-sonnet-4-20250514"</span>,
            max_tokens=<span class="code-keyword">1024</span>,
            messages=[{
                <span class="code-string">"role"</span>: <span class="code-string">"user"</span>,
                <span class="code-string">"content"</span>: [
                    {<span class="code-string">"type"</span>: <span class="code-string">"image"</span>, <span class="code-string">"source"</span>: {
                        <span class="code-string">"type"</span>: <span class="code-string">"base64"</span>,
                        <span class="code-string">"media_type"</span>: <span class="code-string">"image/png"</span>,
                        <span class="code-string">"data"</span>: image_b64
                    }},
                    {<span class="code-string">"type"</span>: <span class="code-string">"text"</span>, <span class="code-string">"text"</span>: <span class="code-string">"""
                    Extract invoice data. Return JSON:
                    {
                        "invoice_number": "...",
                        "date": "YYYY-MM-DD",
                        "vendor": "...",
                        "total": 0.00,
                        "line_items": [{"description": "...", "qty": 1, "price": 0.00}],
                        "confidence": 0.0-1.0
                    }
                    If any field is unclear, use null and lower confidence.
                    """</span>}
                ]
            }]
        )
        
        <span class="code-comment"># Parse and validate with Pydantic</span>
        data = json.loads(response.content[<span class="code-keyword">0</span>].text)
        <span class="code-keyword">return</span> InvoiceData(**data)
    
    <span class="code-keyword">async def</span> <span class="code-function">validate_extraction</span>(self, data: InvoiceData) -> <span class="code-keyword">bool</span>:
        <span class="code-string">"""Validate extracted data meets quality thresholds."""</span>
        <span class="code-keyword">if</span> data.confidence < <span class="code-keyword">0.8</span>:
            <span class="code-keyword">return</span> <span class="code-keyword">False</span>  <span class="code-comment"># Flag for human review</span>
        <span class="code-keyword">if</span> <span class="code-keyword">not</span> data.invoice_number <span class="code-keyword">or not</span> data.total:
            <span class="code-keyword">return</span> <span class="code-keyword">False</span>
        <span class="code-keyword">return</span> <span class="code-keyword">True</span></pre>
                </div>
            </div>
        </section>

        <!-- FUTURE -->
        <section class="module" id="future">
            <div class="module-header">
                <div class="module-icon">ğŸ”®</div>
                <div class="module-info">
                    <h2>The Future of Multimodal</h2>
                    <p>Where multimodal AI is headed</p>
                </div>
            </div>

            <div class="timeline">
                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-date">2025</div>
                    <div class="timeline-title">Native Multimodal Becomes Standard</div>
                    <div class="timeline-desc">All frontier models ship with native vision, audio, and video. Bolt-on approaches become legacy.</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-date">2025</div>
                    <div class="timeline-title">Video Generation Matures</div>
                    <div class="timeline-desc">Minute-long, high-quality video generation becomes production-ready. Sora-class capabilities widespread.</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-date">2025-26</div>
                    <div class="timeline-title">Unified Generation</div>
                    <div class="timeline-desc">Single models that understand AND generate all modalities. Chat models that output images, video, audio natively.</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-date">2026</div>
                    <div class="timeline-title">3D & Spatial Intelligence</div>
                    <div class="timeline-desc">Models that understand and generate 3D content. AR/VR content creation, robotics integration.</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-dot"></div>
                    <div class="timeline-date">2026+</div>
                    <div class="timeline-title">Embodied Multimodal</div>
                    <div class="timeline-desc">Multimodal models powering robots and physical systems. AI that sees, hears, speaks, and acts in the real world.</div>
                </div>
            </div>
        </section>

        <!-- AGENT THIS -->
        <section class="module" id="agent">
            <div class="module-header">
                <div class="module-icon">ğŸ¤–</div>
                <div class="module-info">
                    <h2>Agent This</h2>
                    <p>Multimodal content processor agent</p>
                </div>
            </div>

            <div class="agent-grid">
                <div class="agent-info">
                    <h3>ğŸ¨ MultimodalProcessor</h3>
                    <p>An agent that processes any combination of text, images, audio, and video. Automatically routes to appropriate models, handles format conversion, and synthesizes results across modalities.</p>
                    
                    <div class="agent-capabilities">
                        <div class="agent-capability"><span class="capability-icon">ğŸ–¼ï¸</span> Process images and extract structured data</div>
                        <div class="agent-capability"><span class="capability-icon">ğŸ¤</span> Transcribe and analyze audio content</div>
                        <div class="agent-capability"><span class="capability-icon">ğŸ¬</span> Summarize and query video content</div>
                        <div class="agent-capability"><span class="capability-icon">ğŸ“</span> Generate images from descriptions</div>
                        <div class="agent-capability"><span class="capability-icon">ğŸ”Š</span> Convert text to natural speech</div>
                        <div class="agent-capability"><span class="capability-icon">ğŸ”„</span> Cross-modal reasoning and synthesis</div>
                        <div class="agent-capability"><span class="capability-icon">ğŸ“Š</span> Format detection and optimization</div>
                        <div class="agent-capability"><span class="capability-icon">ğŸ’¾</span> Output in requested modalities</div>
                    </div>
                </div>

                <div class="agent-code">
                    <div class="code-header">multimodal_agent.py</div>
                    <div class="code-content">
<pre><span class="code-comment"># Multimodal Processor Agent</span>
<span class="code-keyword">from</span> crewai <span class="code-keyword">import</span> Agent, Task, Crew
<span class="code-keyword">from</span> langchain_anthropic <span class="code-keyword">import</span> ChatAnthropic
<span class="code-keyword">from</span> multimodal_tools <span class="code-keyword">import</span> (
    VisionAnalyzer, AudioTranscriber, VideoProcessor,
    ImageGenerator, SpeechSynthesizer, FormatConverter
)

llm = ChatAnthropic(model=<span class="code-string">"claude-sonnet-4-20250514"</span>)

<span class="code-comment"># Initialize multimodal tools</span>
vision = VisionAnalyzer(models=[<span class="code-string">"claude-3.5"</span>, <span class="code-string">"gpt-4o"</span>])
audio = AudioTranscriber(model=<span class="code-string">"whisper-large-v3"</span>)
video = VideoProcessor(model=<span class="code-string">"gemini-2.0"</span>)
image_gen = ImageGenerator(model=<span class="code-string">"dall-e-3"</span>)
tts = SpeechSynthesizer(model=<span class="code-string">"elevenlabs"</span>)
converter = FormatConverter()

multimodal_agent = Agent(
    role=<span class="code-string">"Multimodal Content Processor"</span>,
    goal=<span class="code-string">"""Process any combination of text, images, 
    audio, and video to extract insights and 
    generate requested outputs"""</span>,
    backstory=<span class="code-string">"""Expert in multimodal AI with deep 
    understanding of vision, audio, and video 
    processing. Knows which models work best 
    for different tasks and can seamlessly 
    combine information across modalities."""</span>,
    llm=llm,
    tools=[vision, audio, video, image_gen, tts, converter],
    verbose=<span class="code-keyword">True</span>
)

<span class="code-keyword">async def</span> <span class="code-function">process_multimodal</span>(
    inputs: <span class="code-keyword">dict</span>,
    output_format: <span class="code-keyword">str</span> = <span class="code-string">"text"</span>
) -> <span class="code-keyword">dict</span>:
    <span class="code-string">"""Process multimodal inputs and generate outputs."""</span>
    
    task = Task(
        description=<span class="code-string">f"""
        Process the provided multimodal content:
        - Text: {inputs.get('text', 'None')}
        - Images: {len(inputs.get('images', []))} provided
        - Audio: {inputs.get('audio', 'None')}
        - Video: {inputs.get('video', 'None')}
        
        1. Analyze each modality present
        2. Extract key information and insights
        3. Cross-reference across modalities
        4. Generate output in format: {output_format}
        """</span>,
        agent=multimodal_agent,
        expected_output=<span class="code-string">"Processed multimodal content"</span>
    )
    
    crew = Crew(agents=[multimodal_agent], tasks=[task])
    result = <span class="code-keyword">await</span> crew.kickoff_async()
    
    <span class="code-keyword">return</span> {
        <span class="code-string">"analysis"</span>: result.analysis,
        <span class="code-string">"outputs"</span>: result.generated_content,
        <span class="code-string">"cross_modal_insights"</span>: result.insights
    }</pre>
                    </div>
                </div>
            </div>
        </section>

        <!-- RELATED -->
        <section class="module">
            <div class="module-header">
                <div class="module-icon">ğŸ“š</div>
                <div class="module-info">
                    <h2>Related Topics</h2>
                    <p>Continue exploring emerging technologies</p>
                </div>
            </div>

            <div class="related-grid">
                <a href="cat19-p1-foundation-models.html" class="related-card">
                    <div class="related-icon">ğŸ§ </div>
                    <div class="related-title">Foundation Models</div>
                    <div class="related-desc">The base models powering multimodal AI</div>
                </a>
                <a href="cat19-p2-agentic-ai.html" class="related-card">
                    <div class="related-icon">ğŸ¤–</div>
                    <div class="related-title">Agentic AI</div>
                    <div class="related-desc">Agents that use multimodal capabilities</div>
                </a>
                <a href="cat19-p5-edge-ai.html" class="related-card">
                    <div class="related-icon">ğŸ“±</div>
                    <div class="related-title">Edge & On-Device AI</div>
                    <div class="related-desc">Multimodal processing on local devices</div>
                </a>
            </div>
        </section>

    </div>
</div>

<footer>
    <div class="footer-nav">
        <a href="cat19-p2-agentic-ai.html" class="footer-link">
            <span>â†</span>
            <div>
                <div class="footer-link-label">Previous</div>
                <div class="footer-link-title">19.2 Agentic AI</div>
            </div>
        </a>
        <div class="footer-brand"><span>STRATEGY</span>HUB â€¢ Page 19.3</div>
        <a href="cat19-p4-ai-coding.html" class="footer-link">
            <div>
                <div class="footer-link-label">Next</div>
                <div class="footer-link-title">19.4 AI Coding Tools</div>
            </div>
            <span>â†’</span>
        </a>
    </div>
</footer>

</body>
</html>
